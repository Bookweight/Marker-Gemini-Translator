import os
import yaml
import logging
from dotenv import load_dotenv
from pathlib import Path

from src.client import S2Client
from src.ranker import PaperRanker
from src.writer import ObsidianWriter
from src.downloader import PaperDownloader

# è¨­å®š Logging (åŒæ™‚è¼¸å‡ºåˆ° Console å’Œ File)
def setup_logging():
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_dir / "app.log", encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

def load_config(path="config.yaml"):
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def main():
    setup_logging()
    logger = logging.getLogger("Main")
    
    try:
        # 1. åˆå§‹åŒ–
        logger.info("ç³»çµ±å•Ÿå‹•...")
        load_dotenv()
        config = load_config()
        api_key = os.getenv("S2_API_KEY")
        
        client = S2Client(api_key, config)
        # Refactored: Ranker now handles Profile and Harvesting
        ranker = PaperRanker(config, client)
        writer = ObsidianWriter(config)
        downloader = PaperDownloader(config, writer=writer)

        # 1.5 åŸ·è¡Œæ”¶å‰²
        try:
            ranker.harvest_feedback(lookback_days=7)
        except Exception as e:
            logger.error(f"æ”¶å‰²è©•åˆ†å¤±æ•—ï¼Œå°‡ä½¿ç”¨èˆŠæœ‰ Profile ç¹¼çºŒ: {e}")
        
        # 2. ç²å–å€™é¸
        keywords = config['search']['keywords']
        if isinstance(keywords, str):
            keywords = [keywords]
            
        years = config['search']['year_range']
        
        all_candidates = {} # ä½¿ç”¨å­—å…¸ä¾ paperId å»é‡
        
        logger.info(f"ğŸ” å•Ÿå‹•å¤šé ˜åŸŸæœå°‹: åŒ…å« {len(keywords)} å€‹ä¸»é¡Œ")
        
        for topic in keywords:
            logger.info(f"  - æ­£åœ¨æœå°‹é ˜åŸŸ: {topic}...")
            # Note: client usage remains same
            papers = client.search_papers(topic, years, limit=15)
            
            for p in papers:
                all_candidates[p['paperId']] = p
                
        # è½‰å›åˆ—è¡¨
        candidates = list(all_candidates.values())
        logger.info(f"âœ… å¤šé ˜åŸŸæœå°‹å®Œæˆï¼Œåˆä½µå¾Œå…± {len(candidates)} ç¯‡å€™é¸è«–æ–‡")
        
        # 3. éæ¿¾èˆ‡æ’åº
        whitelist = set(config['filters']['whitelist_fields'])
        # Access history from ranker's profile manager
        history_set = set(ranker.profile_manager.profile.get('history_ids', []))
        
        logger.info(f"ç›®å‰æ­·å²è³‡æ–™åº«å·²æœ‰ {len(history_set)} ç¯‡è«–æ–‡ (å°‡è¢«æ’é™¤)")
        valid_ids = []
        for p in candidates:
            p_id = p['paperId']
            if p_id in history_set:
                continue
            fields = set(p.get('fieldsOfStudy') or [])
            if not fields.isdisjoint(whitelist):
                valid_ids.append(p['paperId'])
                
        logger.info(f"ç¶“éç™½åå–®éæ¿¾ï¼Œæº–å‚™æŠ“å– {len(valid_ids)} ç¯‡è«–æ–‡è©³æƒ…")
        
        detailed_papers = client.get_batch_details(valid_ids)
        
        # Refactored: rank_candidates uses internal profile, no need to pass user_vector
        top_papers = ranker.rank_candidates(detailed_papers, top_k=5)
        
        # 4. å¯«å…¥ä»‹é¢
        if top_papers:
            if writer.write_recommendations(top_papers):
                # æˆåŠŸå¯«å…¥å¾Œï¼Œæ›´æ–°ä»Šæ—¥æ­·ç¨‹ (é¿å…é‡è¤‡æ¨è–¦)
                recommended_ids = [p['paperId'] for p in top_papers]
                ranker.profile_manager.add_recommendations(recommended_ids)


            logger.info("é€²å…¥æª”æ¡ˆæª¢æŸ¥æµç¨‹ï¼šç¢ºèª PDF èˆ‡ç¿»è­¯æ˜¯å¦é½Šå…¨...")
            downloader.process_papers(top_papers)
            
            logger.info("æœ¬æ¬¡åŸ·è¡ŒçµæŸã€‚")
        else:
            logger.warning("ä»Šæ—¥æœªèƒ½é¸å‡ºä»»ä½•è«–æ–‡ã€‚")
            
    except Exception as e:
        logger.error(f"ç³»çµ±å´©æ½°: {e}", exc_info=True)

if __name__ == "__main__":
    main()