import os
import yaml
import logging
from dotenv import load_dotenv
from pathlib import Path

from src.client import S2Client
from src.ranker import PaperRanker
from src.writer import ObsidianWriter
from src.harvester import NoteHarvester, ProfileManager
from src.downloader import PaperDownloader

# è¨­å®š Logging (åŒæ™‚è¼¸å‡ºåˆ° Console å’Œ File)
def setup_logging():
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_dir / "app.log", encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

def load_config(path="config.yaml"):
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def main():
    setup_logging()
    logger = logging.getLogger("Main")
    
    try:
        # 1. åˆå§‹åŒ–
        logger.info("ğŸš€ ç³»çµ±å•Ÿå‹•...")
        load_dotenv()
        config = load_config()
        api_key = os.getenv("S2_API_KEY")
        
        client = S2Client(api_key, config)
        profile_manager = ProfileManager()
        harvester = NoteHarvester(config, client, profile_manager)
        ranker = PaperRanker(config)
        writer = ObsidianWriter(config)
        downloader = PaperDownloader(config)

        # 1.5 åŸ·è¡Œæ”¶å‰² (ç¶­æŒä¸è®Š)
        try:
            harvester.harvest(lookback_days=7)
        except Exception as e:
            logger.error(f"æ”¶å‰²è©•åˆ†å¤±æ•—ï¼Œå°‡ä½¿ç”¨èˆŠæœ‰ Profile ç¹¼çºŒ: {e}")
        
        # 2. ç²å–å€™é¸ (ç¶­æŒä¸è®Š)
        query = config['search']['keywords']
        years = config['search']['year_range']
        candidates = client.search_papers(query, years)
        
        # 3. éæ¿¾èˆ‡æ’åº (ç¶­æŒä¸è®Š)
        whitelist = set(config['filters']['whitelist_fields'])
        valid_ids = []
        for p in candidates:
            fields = set(p.get('fieldsOfStudy') or [])
            if not fields.isdisjoint(whitelist):
                valid_ids.append(p['paperId'])
                
        logger.info(f"ç¶“éç™½åå–®éæ¿¾ï¼Œæº–å‚™æŠ“å– {len(valid_ids)} ç¯‡è«–æ–‡è©³æƒ…")
        
        user_vec = profile_manager.profile['user_vector']
        detailed_papers = client.get_batch_details(valid_ids)
        
        top_papers = ranker.rank_candidates(detailed_papers, top_k=5, user_vector=user_vec)
        
        # 4. å¯«å…¥ä»‹é¢ (ä¿®æ”¹æ ¸å¿ƒé‚è¼¯)
        if top_papers:
            # å˜—è©¦å¯«å…¥ç­†è¨˜ (å¦‚æœå·²å­˜åœ¨ï¼Œwriter æœƒè‡ªå‹•è·³éä¸¦å›å‚³ Falseï¼Œä½†é€™ä¸é‡è¦)
            writer.write_recommendations(top_papers)
            
            # [ä¿®æ”¹é»] ä¸è«–ç­†è¨˜æ˜¯å¦æ˜¯æ–°å»ºç«‹çš„ï¼Œéƒ½å¼·åˆ¶åŸ·è¡Œä¸‹è¼‰æª¢æŸ¥
            # Downloader å…§éƒ¨æœ¬èº«å°±æœ‰æª¢æŸ¥ "æª”æ¡ˆæ˜¯å¦å­˜åœ¨" çš„é‚è¼¯ï¼Œæ‰€ä»¥é€™è£¡ç›´æ¥å‘¼å«æ˜¯å®‰å…¨çš„
            logger.info("ğŸš€ é€²å…¥æª”æ¡ˆæª¢æŸ¥æµç¨‹ï¼šç¢ºèª PDF èˆ‡ç¿»è­¯æ˜¯å¦é½Šå…¨...")
            downloader.process_papers(top_papers)
            
            logger.info("ğŸ‰ æœ¬æ¬¡åŸ·è¡ŒçµæŸã€‚")
        else:
            logger.warning("âš ï¸ ä»Šæ—¥æœªèƒ½é¸å‡ºä»»ä½•è«–æ–‡ã€‚")
            
    except Exception as e:
        logger.error(f"ğŸ’¥ ç³»çµ±å´©æ½°: {e}", exc_info=True)

if __name__ == "__main__":
    main()