import logging
import requests
import time
import re
from pathlib import Path
from pypdf import PdfReader, PdfWriter
from typing import List, Dict, Any
from src.translator import PaperTranslator

class PaperDownloader:
    def __init__(self, config: Dict[str, Any]):
        self.logger = logging.getLogger(__name__)
        self.config = config
        self.vault_path = Path(config['obsidian']['vault_path'])
        unclassified_rel = config['obsidian'].get('unclassified_folder', 'Papers/unclassified')
        self.default_dir = self.vault_path / unclassified_rel
        self.default_dir.mkdir(parents=True, exist_ok=True)
        self.papers_root = self.default_dir.parent
        self.folder_map = {
            "Time Series": ["Time Series", "Forecasting", "Anomaly Detection"],
            "Computer Vision": ["Computer Vision", "Image", "Object Detection", "Segmentation"],
            "Natural Language Processing": ["NLP", "Language Model", "Text", "LLM"],
            "Graph Neural Networks": ["Graph", "GNN", "Link Prediction"],
            "Recommendation System": ["Recommendation", "Recommender"],
            "Deep Learning": ["Deep Learning", "Neural Network"],
            # æ‚¨ç¾æœ‰çš„å…¶ä»–è³‡æ–™å¤¾
            "LLM": ["Large Language Model", "LLM"],
            "Knowledge Graph": ["Knowledge Graph"],
            "Sentiment Analysis": ["Sentiment"],
            "Point Cloud Registration": ["Point Cloud"],
            "Hearing Loss Simulation": ["Hearing", "Audio"],
            "Database": ["Database"]
        }
        # Initialize Translator
        self.translator = PaperTranslator(config)
        
    def _determine_save_dir(self, paper: Dict[str, Any]) -> Path:
        """æ ¹æ“šè«–æ–‡ Metadata æ±ºå®šå­˜æª”è³‡æ–™å¤¾"""
        
        # 1. æº–å‚™æª¢æŸ¥çš„æ–‡å­— (æ¨™é¡Œ + é ˜åŸŸæ¨™ç±¤)
        title = paper.get('title', '').lower()
        fields = [f.lower() for f in (paper.get('fieldsOfStudy') or [])]
        # æœ‰äº›è«–æ–‡åªæœ‰ s2FieldsOfStudy
        if not fields:
            s2_fields = paper.get('s2FieldsOfStudy') or []
            fields = [f['category'].lower() for f in s2_fields]
        
        text_to_check = title + " " + " ".join(fields)

        # 2. æ¯”å°é—œéµå­—
        for folder_name, keywords in self.folder_map.items():
            for kw in keywords:
                if kw.lower() in text_to_check:
                    target_dir = self.papers_root / folder_name
                    # å¦‚æœè³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œè‡ªå‹•å»ºç«‹ (æˆ–è¦–éœ€æ±‚æ±ºå®šæ˜¯å¦å»ºç«‹)
                    if not target_dir.exists():
                        # é€™è£¡é¸æ“‡è‡ªå‹•å»ºç«‹ï¼Œç¢ºä¿åˆ†é¡æˆåŠŸ
                        target_dir.mkdir(parents=True, exist_ok=True)
                    return target_dir
        
        # 3. æ²’å°æ‡‰åˆ° -> å›å‚³ unclassified
        return self.default_dir
    
    def _compress_pdf(self, input_path: Path) -> Path:
        """
        [æ–°å¢] ç°¡å–®çš„ PDF å£“ç¸®/é‡å¯«é‚è¼¯
        å¦‚æœæª”æ¡ˆè¶…é 20MBï¼Œå˜—è©¦é‡å¯«ä»¥æ¸›å°‘ metadata æˆ–ç„¡ç”¨æ•¸æ“šã€‚
        æ³¨æ„ï¼špypdf çš„å£“ç¸®èƒ½åŠ›æœ‰é™ï¼Œè‹¥è¦å¼·åŠ›å£“ç¸®åœ–ç‰‡éœ€è¦ç”¨ ghostscriptï¼Œ
        ä½†é€™èƒ½è§£æ±ºéƒ¨åˆ†æ ¼å¼è‡ƒè…«çš„å•é¡Œã€‚
        """
        file_size_mb = input_path.stat().st_size / (1024 * 1024)
        if file_size_mb < 20:
            return input_path
            
        self.logger.warning(f"âš ï¸ æª”æ¡ˆéå¤§ ({file_size_mb:.2f} MB)ï¼Œå˜—è©¦ç¸®æ¸›: {input_path.name}")
        
        try:
            reader = PdfReader(input_path)
            writer = PdfWriter()
            
            for page in reader.pages:
                writer.add_page(page)
                
            # åŠ å…¥å£“ç¸®åƒæ•¸
            for page in writer.pages:
                page.compress_content_streams()  # å£“ç¸®å…§å®¹æµ
                
            temp_output = input_path.with_suffix('.compressed.pdf')
            with open(temp_output, "wb") as f:
                writer.write(f)
                
            # æª¢æŸ¥æ˜¯å¦çœŸçš„è®Šå°äº†
            new_size = temp_output.stat().st_size / (1024 * 1024)
            if new_size < 20:
                self.logger.info(f"âœ… ç¸®æ¸›æˆåŠŸ: {new_size:.2f} MB")
                input_path.unlink() # åˆªé™¤åŸæª”
                temp_output.rename(input_path) # å–ä»£åŸæª”
                return input_path
            else:
                self.logger.warning(f"âŒ ç¸®æ¸›å¾Œä»éå¤§ ({new_size:.2f} MB)ï¼Œå¯èƒ½å°è‡´ç¿»è­¯å¤±æ•—ã€‚")
                temp_output.unlink()
                return input_path
                
        except Exception as e:
            self.logger.error(f"å£“ç¸®éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            return input_path

    def _download_pdf(self, url: str, title: str, target_dir: Path) -> Path: # [ä¿®æ”¹] å¢åŠ  target_dir åƒæ•¸
        """ä¸‹è¼‰å–®ç¯‡ PDF"""
        safe_title = re.sub(r'[\\/*?:"<>|]', "", title)[:100].strip()
        filename = f"{safe_title}.pdf"
        file_path = target_dir / filename # [ä¿®æ”¹] ä½¿ç”¨å‚³å…¥çš„è³‡æ–™å¤¾

        if file_path.exists():
            if file_path.stat().st_size < 2048:
                self.logger.warning(f"ç™¼ç¾ç„¡æ•ˆèˆŠæª” ({filename})ï¼Œåˆªé™¤é‡è©¦...")
                file_path.unlink()
            else:
                self.logger.info(f"â­PDF å·²å­˜åœ¨ ({target_dir.name})ï¼Œè·³éä¸‹è¼‰")
                return file_path
            
        self.logger.info(f"â¬‡ï¸ ä¸‹è¼‰ä¸­ ({target_dir.name}): {filename}")
        
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            }
            response = requests.get(url, headers=headers, stream=True, timeout=60)
            response.raise_for_status()
            
            content_type = response.headers.get('Content-Type', '').lower()
            if 'application/pdf' not in content_type and 'binary/octet-stream' not in content_type:
                self.logger.warning(f"ä¸‹è¼‰å…§å®¹é PDF: {url}")
                return None

            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            if file_path.stat().st_size < 2048:
                file_path.unlink()
                return None

            self.logger.info(f"ä¸‹è¼‰å®Œæˆ")
            
            if file_path and file_path.exists():
                self._compress_pdf(file_path)
            return file_path

        except Exception as e:
            self.logger.warning(f"ä¸‹è¼‰å¤±æ•—: {e}")
            if file_path.exists():
                file_path.unlink()
            return None

    def _run_translation_script(self, pdf_path: Path, paper_metadata: Dict):
        """Invoke Python Translator directly"""
        zh_md_path = pdf_path.with_suffix('.zh.md')
        if zh_md_path.exists():
            self.logger.info(f"â­ Translation exists, skipping: {zh_md_path.name}")
            return
            
        # [Fix] Organize Folder First (Restore Structure)
        try:
            pdf_path = self.translator.organize_paper_folder(pdf_path)
            # Update output path based on new location
            zh_md_path = pdf_path.with_suffix('.zh.md')
        except Exception as e:
            self.logger.warning(f"Organization step skipped: {e}")

        self.logger.info(f"ğŸ§  Starting Native Python Translation: {pdf_path.name}...")
        try:
            self.translator.translate_paper(pdf_path, zh_md_path)
        except Exception as e:
            self.logger.error(f"Translation Crash: {e}", exc_info=True)

    def process_papers(self, papers: List[Dict[str, Any]]):
        """ä¸»æµç¨‹"""
        for paper in papers:
            title = paper.get('title', 'Untitled')
            target_dir = self._determine_save_dir(paper)
            pdf_path = None
            
            # 2. ä¸‹è¼‰é‚è¼¯ (å‚³å…¥ target_dir)
            pdf_info = paper.get('openAccessPdf')
            if pdf_info and pdf_info.get('url'):
                pdf_path = self._download_pdf(pdf_info['url'], title, target_dir)
            
            if not pdf_path:
                external_ids = paper.get('externalIds') or {}
                arxiv_id = external_ids.get('ArXiv')
                if arxiv_id:
                    arxiv_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                    pdf_path = self._download_pdf(arxiv_url, title, target_dir)

            # 3. ç¿»è­¯é‚è¼¯ (ä¿®æ­£åƒæ•¸å‚³é)
            if pdf_path:
                # [FIXED] é€™è£¡è£œä¸Š paper åƒæ•¸
                self._run_translation_script(pdf_path, paper)