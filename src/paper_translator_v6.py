import sys
import os
import re
import time
import subprocess
import shutil
from pathlib import Path
import fitz  # PyMuPDF

# --- è¨­å®š ---
GEMINI_CMD = "gemini"
MAX_RETRIES = 3
BATCH_SIZE_LIMIT = 10000 

print("ğŸš€ Loading Marker AI models (V26 Boundary Detector)...", file=sys.stderr)

try:
    from marker.converters.pdf import PdfConverter
    from marker.models import create_model_dict
    from marker.output import text_from_rendered
    from marker.config.parser import ConfigParser
except ImportError:
    print("âŒ Critical Modules missing.", file=sys.stderr)
    sys.exit(1)

# Marker è¨­å®š
config_dict = {
    "output_format": "markdown",
    "disable_image_extraction": False,
    "disable_table_extraction": False,
    "paginate_output": False
}
config_parser = ConfigParser(config_dict)

converter = PdfConverter(
    config=config_parser.generate_config_dict(),
    artifact_dict=create_model_dict(),
    processor_list=config_parser.get_processors(),
    renderer=config_parser.get_renderer()
)

def clean_filename(name):
    return re.sub(r'[^a-zA-Z0-9_.-]', '_', name)

def detect_content_boundaries(page):
    """
    [V26] åµæ¸¬é é¢ä¸Šçš„ã€Œåˆ†éš”æ©«ç·šã€ï¼Œå®šç¾©æœ‰æ•ˆå…§å®¹å€åŸŸã€‚
    å›å‚³: (top_limit, bottom_limit)
    """
    # é è¨­é‚Šç•Œ (å¦‚æœæ²’æŠ“åˆ°ç·šï¼Œå°±ç”¨ä¿å®ˆå€¼)
    page_height = page.rect.height
    top_limit = 0
    bottom_limit = page_height

    # å–å¾—æ‰€æœ‰ç¹ªåœ–è·¯å¾‘ (Drawings)
    paths = page.get_drawings()
    
    horizontal_lines = []
    
    for p in paths:
        rect = p["rect"]
        # åˆ¤æ–·æ˜¯å¦ç‚ºæ©«ç·šï¼šå¯¬åº¦å¤ å¯¬ï¼Œé«˜åº¦æ¥µå°
        # å¯¬åº¦è‡³å°‘è¦æ˜¯é é¢å¯¬åº¦çš„ 40% æ‰ç®—åˆ†éš”ç·š
        if rect.width > page.rect.width * 0.4 and rect.height < 5:
            horizontal_lines.append(rect.y0)
            
    if horizontal_lines:
        horizontal_lines.sort()
        
        # ç­–ç•¥ï¼š
        # 1. æœ€ä¸Šé¢çš„ç·šé€šå¸¸æ˜¯ Header Separator (ä½†è¦é¿å…æŠ“åˆ°è¡¨æ ¼å…§çš„ç·š)
        #    æˆ‘å€‘å‡è¨­ Header ç·šé€šå¸¸ä½æ–¼é é¢é ‚éƒ¨ 20% å€åŸŸå…§
        header_candidates = [y for y in horizontal_lines if y < page_height * 0.2]
        if header_candidates:
            # å–æœ€ä¸‹é¢çš„ä¸€æ¢ header line (ä»¥é˜² header å€å¡Šæœ‰å…©æ¢ç·š)
            top_limit = header_candidates[-1]
            
        # 2. æœ€ä¸‹é¢çš„ç·šé€šå¸¸æ˜¯ Footer Separator (å¦‚æœæ˜¯è¨»é‡‹ç·š)
        #    å‡è¨­ Footer ç·šä½æ–¼é é¢åº•éƒ¨ 25% å€åŸŸå…§
        footer_candidates = [y for y in horizontal_lines if y > page_height * 0.75]
        if footer_candidates:
            # å–æœ€ä¸Šé¢çš„ä¸€æ¢ footer line (å› ç‚ºè¨»é‡‹æ˜¯åœ¨ç·šä¸‹æ–¹)
            bottom_limit = footer_candidates[0]
            
    return top_limit, bottom_limit

def extract_images_with_boundary_check(pdf_path, output_dir):
    """
    [V26] æå–åœ–ç‰‡ï¼Œä¸¦æ¨™è¨˜æ˜¯å¦ç‚ºã€Œè¶Šç•Œã€çš„é›œè¨Š
    æ³¨æ„ï¼šæˆ‘å€‘å¿…é ˆæå–æ‰€æœ‰åœ–ç‰‡ä»¥ç¶­æŒèˆ‡ Marker çš„ç´¢å¼•å°é½Šï¼Œä½†æˆ‘å€‘å¯ä»¥æ¨™è¨˜å®ƒç‚º skip
    """
    images_dir = os.path.join(output_dir, "images")
    if not os.path.exists(images_dir):
        os.makedirs(images_dir)
        
    print("âš ï¸ Extracting images with Boundary Check...", file=sys.stderr)
    doc = fitz.open(pdf_path)
    extracted_map = {} 
    
    for page_index in range(len(doc)):
        page = doc[page_index]
        image_list = page.get_images(full=True)
        extracted_map[page_index] = []
        
        # 1. åµæ¸¬è©²é çš„é‚Šç•Œ
        top_limit, bottom_limit = detect_content_boundaries(page)
        
        if image_list:
            for img_index, img in enumerate(image_list):
                xref = img[0]
                is_noise = False
                
                # 2. æª¢æŸ¥åœ–ç‰‡ä½ç½®
                rects = page.get_image_rects(xref)
                if rects:
                    rect = rects[0]
                    # å¦‚æœåœ–ç‰‡ä¸­å¿ƒé»åœ¨é‚Šç•Œå¤–ï¼Œè¦–ç‚ºé›œè¨Š
                    mid_y = (rect.y0 + rect.y1) / 2
                    if mid_y < top_limit or mid_y > bottom_limit:
                        is_noise = True
                
                # å³ä½¿æ˜¯é›œè¨Šï¼Œæˆ‘å€‘ä¹Ÿè¦å­˜ä¸‹ä¾† (ç‚ºäº†ä½”ä½)ï¼Œä½†åœ¨ Metadata æ¨™è¨˜å®ƒ
                try:
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image_ext = base_image["ext"]
                    image_name = f"p{page_index}_img{img_index}.{image_ext}"
                    image_path = os.path.join(images_dir, image_name)
                    
                    with open(image_path, "wb") as f:
                        f.write(image_bytes)
                    
                    extracted_map[page_index].append({
                        "filename": image_name,
                        "is_noise": is_noise,
                        "debug_info": f"y={mid_y:.1f}, bounds=({top_limit:.1f}, {bottom_limit:.1f})"
                    })
                except Exception:
                    pass
                    
    return extracted_map

def clean_text_noise(text):
    """
    [V26] æ¸…æ´—å¸¸è¦‹çš„é ç·£æ–‡å­—é›œè¨Š
    """
    lines = text.split('\n')
    cleaned_lines = []
    
    # å¸¸è¦‹é›œè¨Šæ¨¡å¼
    patterns = [
        r'^\s*\d+\s+Page\s+\d+\s+of\s+\d+', # "20 Page 30 of 47"
        r'^\s*Page\s+\d+\s*$',              # "Page 30"
        r'^\s*arXiv:\d+\.\d+.*$',           # arXiv ID
        r'^\s*https?://doi\.org/.*$',       # DOI Links (è‹¥ç¨ç«‹ä¸€è¡Œ)
        r'.*Â©.*Permission\s+to\s+make.*',   # ç‰ˆæ¬Šå®£å‘Š
        r'^\s*Vol\.\s+\d+,\s+No\.\s+\d+.*$' # æœŸåˆŠå·è™Ÿ
    ]
    
    compiled_patterns = [re.compile(p, re.IGNORECASE) for p in patterns]
    
    for line in lines:
        is_noise = False
        if len(line) < 100: # é›œè¨Šé€šå¸¸ä¸é•·
            for p in compiled_patterns:
                if p.match(line):
                    is_noise = True
                    break
        
        if not is_noise:
            cleaned_lines.append(line)
            
    return '\n'.join(cleaned_lines)

def force_normalize_headers(text):
    """æ¨™é¡Œä¿®å¾© (V23 Logic)"""
    lines = text.split('\n')
    new_lines = []
    h2_pattern = re.compile(r'^[*#]*\s*(\d+\.?\s+[A-Z].*?)[*#]*$') 
    h3_pattern = re.compile(r'^[*#]*\s*(\d+\.\d+\.?\s+.*?)[*#]*$')
    special_headers = ["Abstract", "References", "Introduction", "Conclusion"]

    for line in lines:
        clean_line = line.strip()
        if not clean_line:
            new_lines.append(line)
            continue
        
        content = re.sub(r'^[*#]+\s*', '', clean_line)
        if h3_pattern.match(clean_line):
            match = h3_pattern.match(clean_line)
            content = re.sub(r'^[*#]+\s*', '', match.group(0))
            new_lines.append(f"### {content}")
        elif h2_pattern.match(clean_line) or any(content.startswith(h) for h in special_headers):
            match = h2_pattern.match(clean_line)
            if match: content = re.sub(r'^[*#]+\s*', '', match.group(0))
            new_lines.append(f"## {content}")
        else:
            new_lines.append(line)
    return '\n'.join(new_lines)

def inject_images_sync_filter(text, image_map):
    """
    [V26] åŒæ­¥éæ¿¾æ³¨å…¥
    é‡åˆ° Marker çš„åœ–ç‰‡æ¨™ç±¤æ™‚ï¼Œæª¢æŸ¥å°æ‡‰çš„ PyMuPDF åœ–ç‰‡æ˜¯å¦ç‚ºé›œè¨Šã€‚
    - å¦‚æœæ˜¯é›œè¨Šï¼šåˆªé™¤æ¨™ç±¤ (ä¸é¡¯ç¤º)ã€‚
    - å¦‚æœæ˜¯æ­£æ–‡åœ–ï¼šæ­£å¸¸æ³¨å…¥ã€‚
    - å¦‚æœå°æ‡‰ä¸åˆ° (Vectoråœ–)ï¼šé¡¯ç¤ºæç¤ºã€‚
    """
    pattern = re.compile(r'!\[(.*?)\]\((.*?)_page_(\d+)_Picture_.*?\)')
    parts = []
    last_end = 0
    page_counter = {} 

    for match in pattern.finditer(text):
        parts.append(text[last_end:match.start()])
        alt = match.group(1) or "Figure"
        page_idx = int(match.group(3))
        
        if page_idx not in page_counter: page_counter[page_idx] = 0
        current_idx = page_counter[page_idx]
        
        images_on_page = image_map.get(page_idx, [])
        
        if current_idx < len(images_on_page):
            img_data = images_on_page[current_idx]
            
            if img_data["is_noise"]:
                # æ˜¯é›œè¨Š (ä¾‹å¦‚ Logo)ï¼Œç›´æ¥éš±è—ï¼Œä¸è¦ä½”ä½
                # ä½†è¨ˆæ•¸å™¨è¦ +1ï¼Œå› ç‚º Marker ä¹Ÿæœ‰ç®—é€™å¼µåœ–
                pass 
            else:
                # æ˜¯å¥½åœ–ï¼Œæ³¨å…¥
                fname = img_data["filename"]
                parts.append(f"![{alt}](images/{fname})")
            
            page_counter[page_idx] += 1
        else:
            # Marker èªç‚ºæœ‰åœ–ï¼Œä½† PyMuPDF æ²’æŠ“åˆ° (å¯èƒ½æ˜¯å‘é‡åœ–)
            # é€™ç¨®æƒ…æ³é€šå¸¸ä¸æ˜¯é›œè¨Š (é›œè¨Šé€šå¸¸æ˜¯ Logoï¼Œæ˜¯é»é™£åœ–)
            parts.append(f"> *[Figure: Vector/Text - Not Extracted]*")
            
        last_end = match.end()
    parts.append(text[last_end:])
    return "".join(parts)

def convert_with_marker_and_fix(pdf_path, output_dir):
    images_dir = os.path.join(output_dir, "images")
    if not os.path.exists(images_dir):
        os.makedirs(images_dir)

    try:
        rendered = converter(pdf_path)
        ret_val = text_from_rendered(rendered)
        full_text = ret_val[0] if isinstance(ret_val, tuple) and len(ret_val) >= 2 else str(ret_val)

        # 1. æå–åœ–ç‰‡ä¸¦æ¨™è¨˜é›œè¨Š
        smart_image_map = extract_images_with_boundary_check(pdf_path, output_dir)
        
        # 2. æ³¨å…¥åœ–ç‰‡ (è‡ªå‹•éæ¿¾é›œè¨Š)
        full_text = inject_images_sync_filter(full_text, smart_image_map)
        
        # 3. æ¸…æ´—æ–‡å­—é›œè¨Š (Header/Footer Text)
        full_text = clean_text_noise(full_text)
        
        return full_text
    except Exception as e:
        print(f"âŒ Marker Conversion Failed: {e}", file=sys.stderr)
        return None

def split_text_into_logical_blocks(text):
    lines = text.split('\n')
    blocks = []
    current_block = []
    in_table = False
    in_code = False
    for line in lines:
        if line.strip().startswith("```"): in_code = not in_code
        if '|' in line and len(line) > 5: in_table = True
        elif line.strip() == "": in_table = False
        current_block.append(line)
        if not in_table and not in_code and line.strip() == "":
            content = "\n".join(current_block).strip()
            if content: blocks.append("\n".join(current_block))
            current_block = []
    if current_block: blocks.append("\n".join(current_block))
    return blocks

def call_gemini(prompt):
    is_windows = sys.platform.startswith("win")
    for attempt in range(MAX_RETRIES):
        try:
            process = subprocess.Popen(
                [GEMINI_CMD], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                text=True, encoding='utf-8', shell=is_windows 
            )
            stdout, stderr = process.communicate(input=prompt)
            if process.returncode == 0 and len(stdout.strip()) > 0: return stdout
            if "429" in stderr or "quota" in stderr.lower():
                time.sleep(10)
            else:
                time.sleep(2)
        except Exception as e:
            print(f"âŒ API Error: {e}", file=sys.stderr)
            time.sleep(5)
    return None

def translate_batch(batch_blocks, start_id):
    prompt_text = ""
    for i, block in enumerate(batch_blocks):
        prompt_text += f"<<<ID_{start_id + i}>>>\n{block}\n\n"

    prompt = f"""
SYSTEM_MODE: ACADEMIC_TRANSLATOR
**TASK:** Translate to Traditional Chinese (Taiwan).

**OUTPUT FORMAT:**
<<<ID_x>>>
[Chinese Translation]

**RULES:**
1. **Style:** Academic, formal.
2. **SKIP:** If block is Table, Code, `![]`, or Reference list, output: `[ORIGINAL]`
3. **No English:** Output ONLY Chinese.

**INPUT:**
{prompt_text}

**OUTPUT:**
"""
    result = call_gemini(prompt)
    translations = {}
    if result:
        matches = re.finditer(r'<<<ID_(\d+)>>>\s*(.*?)(?=(<<<ID_|\Z))', result, re.DOTALL)
        for match in matches:
            idx = int(match.group(1))
            content = match.group(2).strip()
            translations[idx] = content
    return translations

def process_paper(pdf_path, output_path):
    output_dir = os.path.dirname(output_path)
    raw_output_path = str(Path(output_path).with_suffix('.raw.md'))
    
    print(f"ğŸ”¥ Processing PDF with Marker...", file=sys.stderr)
    raw_md = convert_with_marker_and_fix(pdf_path, output_dir)
    if not raw_md: return False

    print("ğŸ“ Normalizing Headers...", file=sys.stderr)
    raw_md = force_normalize_headers(raw_md)

    ref_match = re.search(r'^##\s+(References|Bibliography)', raw_md, re.MULTILINE | re.IGNORECASE)
    
    pre_text = ""
    body_text = raw_md
    post_text = ""
    
    if ref_match:
        split_idx = ref_match.start()
        post_text = raw_md[split_idx:]
        body_text = raw_md[:split_idx]
    
    abstract_match_in_body = re.search(r'^##\s+Abstract', body_text, re.MULTILINE | re.IGNORECASE)
    if abstract_match_in_body:
        split_idx = abstract_match_in_body.start()
        pre_text = body_text[:split_idx]
        body_text = body_text[split_idx:]

    original_blocks = split_text_into_logical_blocks(body_text)
    print(f"ğŸ§© Translating body: {len(original_blocks)} blocks.", file=sys.stderr)

    final_blocks = []
    current_batch = []
    current_batch_len = 0
    batch_start_index = 0

    print(f"ğŸš€ Starting Batch Translation (Max: {BATCH_SIZE_LIMIT} chars)...", file=sys.stderr)

    for i, block in enumerate(original_blocks):
        current_batch.append(block)
        current_batch_len += len(block)

        if current_batch_len >= BATCH_SIZE_LIMIT or i == len(original_blocks) - 1:
            print(f"ğŸ“¤ Sending Batch: {batch_start_index} to {i}...", file=sys.stderr)
            translations = translate_batch(current_batch, batch_start_index)
            
            for j, orig_block in enumerate(current_batch):
                global_idx = batch_start_index + j
                trans_text = translations.get(global_idx, "")
                
                if trans_text == "[ORIGINAL]" or not trans_text:
                    final_blocks.append(orig_block)
                else:
                    final_blocks.append(f"{orig_block}\n\n> {trans_text}")

            current_batch = []
            current_batch_len = 0
            batch_start_index = i + 1
            time.sleep(2)

    full_content = ""
    if pre_text: full_content += pre_text + "\n\n"
    
    body_content = "\n\n".join(final_blocks)
    body_content = re.sub(r'<<<ID_\d+>>>', '', body_content)
    full_content += body_content
    
    if post_text: full_content += "\n\n" + post_text

    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
        print(f"âœ… Success: {output_path}", file=sys.stderr)
    except Exception as e:
        print(f"âŒ Write Error: {e}", file=sys.stderr)

if __name__ == "__main__":
    if len(sys.argv) < 2: sys.exit(1)
    input_file = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) >= 3 else str(Path(input_file).with_suffix('.zh.md'))
    process_paper(input_file, output_file)