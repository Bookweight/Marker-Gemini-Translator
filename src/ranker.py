import logging
import numpy as np
from typing import List, Dict, Any, Optional
from sklearn.cluster import KMeans

class PaperRanker:
    """
    è«–æ–‡æ’åºèˆ‡éæ¿¾å¼•æ“ (Engineering Grade)
    
    åŠŸèƒ½:
    1. è¨ˆç®—è«–æ–‡æ¬Šé‡åˆ†æ•¸ (è€ƒæ…®å¼•ç”¨æ•¸ã€å¹´ä»½ã€è·¨é ˜åŸŸæ‡²ç½°ã€å€‹äººåŒ–å‘é‡)ã€‚
    2. åŸ·è¡Œå¤šæ¨£æ€§éæ¿¾ (K-Means Clustering)ã€‚
    """

    def __init__(self, config: Dict[str, Any]):
        self.logger = logging.getLogger(__name__)
        self.config = config
        
        # è®€å–è¨­å®šåƒæ•¸
        filter_conf = self.config.get('filters', {})
        self.cross_domain_penalty = filter_conf.get('cross_domain_penalty', 0.6)
        self.whitelist = set(filter_conf.get('whitelist_fields', ["Computer Science", "Mathematics"]))
        self.cross_domain_tags = set(filter_conf.get('blacklist_tags', ["Biology", "Medicine", "Geology"]))
        
    def rank_candidates(self, papers: List[Dict[str, Any]], top_k: int = 5, user_vector: Optional[np.ndarray] = None) -> List[Dict[str, Any]]:
        """
        æ ¸å¿ƒæ’åºæµç¨‹
        :param papers: å¾ API æŠ“å›ä¾†çš„è«–æ–‡åˆ—è¡¨
        :param top_k: æœ€çµ‚è¦æ¨è–¦å¹¾ç¯‡
        :param user_vector: ä½¿ç”¨è€…åå¥½å‘é‡ (ç”¨æ–¼å€‹æ€§åŒ–åŠ åˆ†)
        """
        if not papers:
            self.logger.warning("ğŸ“­ å€™é¸åˆ—è¡¨ç‚ºç©ºï¼Œç„¡æ³•é€²è¡Œæ’åº")
            return []

        self.logger.info(f"âš–ï¸ é–‹å§‹æ’åº {len(papers)} ç¯‡å€™é¸è«–æ–‡...")

        # 1. è¨ˆç®—æ¯ç¯‡è«–æ–‡çš„åŸºç¤åˆ†æ•¸
        scored_papers = []
        for paper in papers:
            if not paper.get('fieldsOfStudy'):
                paper['fieldsOfStudy'] = []
                
            score, is_cross = self._calculate_score(paper)
            
            # --- å€‹æ€§åŒ–åŠ åˆ†é‚è¼¯ (ä¿®å¾© None Type éŒ¯èª¤) ---
            similarity_bonus = 0.0
            # åªæœ‰ç•¶åˆ†æ•¸å¤§æ–¼ 0 (æœªè¢«å‰”é™¤) ä¸”æœ‰ä½¿ç”¨è€…å‘é‡æ™‚æ‰è¨ˆç®—
            if user_vector is not None and score > 0:
                # FIX: ä½¿ç”¨ (get() or {}) è™•ç† embedding ç‚º None çš„æƒ…æ³
                embedding_data = paper.get('embedding') or {}
                paper_vec_data = embedding_data.get('specter_v2')
                
                if paper_vec_data:
                    try:
                        p_v = np.array(paper_vec_data)
                        # è¨ˆç®— Cosine Similarity
                        norm_u = np.linalg.norm(user_vector)
                        norm_p = np.linalg.norm(p_v)
                        
                        if norm_u > 0 and norm_p > 0:
                            cos_sim = np.dot(user_vector, p_v) / (norm_u * norm_p)
                            similarity_bonus = max(0, cos_sim)
                            paper['sim_score'] = similarity_bonus
                    except Exception as e:
                        self.logger.debug(f"å‘é‡è¨ˆç®—éŒ¯èª¤ (PaperID: {paper.get('paperId')}): {e}")

            # è‹¥åˆ†æ•¸ç‚º 0 ä»£è¡¨è¢«ç™½åå–®éæ¿¾æ‰äº†
            if score > 0:
                # æœ€çµ‚åˆ†æ•¸ = åŸºç¤åˆ† * (1 + ç›¸ä¼¼åº¦æ¬Šé‡)
                paper['final_score'] = score * (1 + 1.0 * similarity_bonus)
                paper['is_cross_domain'] = is_cross
                scored_papers.append(paper)

        # æ ¹æ“šåˆ†æ•¸ç”±é«˜åˆ°ä½æ’åº
        scored_papers.sort(key=lambda x: x['final_score'], reverse=True)
        self.logger.info(f"âœ… åˆæ­¥ç¯©é¸å¾Œå‰©é¤˜ {len(scored_papers)} ç¯‡æœ‰æ•ˆè«–æ–‡")

        # 2. å¤šæ¨£æ€§éæ¿¾ (Diversity Enforcement)
        if len(scored_papers) > top_k * 2:
            return self._apply_diversity_filter(scored_papers, top_k)
        else:
            self.logger.info("âš ï¸ å€™é¸æ•¸é‡ä¸è¶³ä»¥é€²è¡Œå¤šæ¨£æ€§èšé¡ï¼Œç›´æ¥å›å‚³ Top K")
            return scored_papers[:top_k]

    def _calculate_score(self, paper: Dict[str, Any]) -> tuple[float, bool]:
        """è¨ˆç®—å–®ç¯‡è«–æ–‡åˆ†æ•¸"""
        fields = set(paper.get('fieldsOfStudy', []))
        
        # A. åš´æ ¼ç™½åå–®æª¢æŸ¥
        if not fields.intersection(self.whitelist):
            return 0.0, False

        # B. åˆ¤æ–·æ˜¯å¦ç‚ºè·¨é ˜åŸŸ
        is_cross_domain = not fields.isdisjoint(self.cross_domain_tags)
        
        # C. å–å¾—å½±éŸ¿åŠ›å¼•ç”¨æ•¸
        citations = paper.get('influentialCitationCount') or paper.get('citationCount', 0)
        
        # D. æ‡‰ç”¨æ¬Šé‡
        multiplier = self.cross_domain_penalty if is_cross_domain else 1.0
        final_score = citations * multiplier
        
        return final_score, is_cross_domain

    def _apply_diversity_filter(self, ranked_papers: List[Dict[str, Any]], target_k: int) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ K-Means ç¢ºä¿æ¨è–¦å¤šæ¨£æ€§"""
        # å–å‡ºå‰ N ç¯‡å€™é¸
        pool_size = min(len(ranked_papers), 20)
        candidate_pool = ranked_papers[:pool_size]
        
        embeddings = []
        valid_indices = []
        
        for idx, p in enumerate(candidate_pool):
            # FIX: é€™è£¡åŒæ¨£éœ€è¦è™•ç† embedding ç‚º None çš„æƒ…æ³
            embedding_data = p.get('embedding') or {}
            emb = embedding_data.get('specter_v2')
            
            if emb:
                embeddings.append(emb)
                valid_indices.append(idx)
        
        if len(embeddings) < target_k:
            self.logger.warning("âš ï¸ å…·æœ‰å‘é‡çš„è«–æ–‡ä¸è¶³ï¼Œè·³éå¤šæ¨£æ€§éæ¿¾")
            return ranked_papers[:target_k]

        self.logger.info(f"ğŸ¨ åŸ·è¡Œå¤šæ¨£æ€§èšé¡: å¾ {len(embeddings)} ç¯‡ä¸­é¸å‡º {target_k} é¡ä»£è¡¨ä½œ")
        
        # åŸ·è¡Œ K-Means
        kmeans = KMeans(n_clusters=target_k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(embeddings)
        
        final_selection = []
        cluster_found = set()
        
        # ä¾ç…§åˆ†æ•¸é«˜ä½é¸å–æ¯å€‹ Cluster çš„ä»£è¡¨ä½œ
        for i in range(len(valid_indices)):
            original_idx = valid_indices[i]
            cluster_id = labels[i]
            
            if cluster_id not in cluster_found:
                final_selection.append(candidate_pool[original_idx])
                cluster_found.add(cluster_id)
                
            if len(final_selection) >= target_k:
                break
        
        # è£œé½Šä¸è¶³çš„æ•¸é‡
        if len(final_selection) < target_k:
            for p in candidate_pool:
                if p not in final_selection:
                    final_selection.append(p)
                    if len(final_selection) >= target_k:
                        break
        
        final_selection.sort(key=lambda x: x['final_score'], reverse=True)
        return final_selection