import sys
import os
import re
import time
import subprocess
import shutil
from pathlib import Path
import fitz  # PyMuPDF

# --- è¨­å®š ---
GEMINI_CMD = "gemini"
MAX_RETRIES = 3
BATCH_SIZE_LIMIT = 10000 

print("ğŸš€ Loading Marker AI models (V28 Math Zero-Interference)...", file=sys.stderr)

try:
    from marker.converters.pdf import PdfConverter
    from marker.models import create_model_dict
    from marker.output import text_from_rendered
    from marker.config.parser import ConfigParser
except ImportError:
    print("âŒ Critical Modules missing.", file=sys.stderr)
    sys.exit(1)

# Marker è¨­å®š
config_dict = {
    "output_format": "markdown",
    "disable_image_extraction": False,
    "disable_table_extraction": False,
    "paginate_output": False
}
config_parser = ConfigParser(config_dict)

converter = PdfConverter(
    config=config_parser.generate_config_dict(),
    artifact_dict=create_model_dict(),
    processor_list=config_parser.get_processors(),
    renderer=config_parser.get_renderer()
)

def clean_filename(name):
    return re.sub(r'[^a-zA-Z0-9_.-]', '_', name)

def detect_content_boundaries(page):
    """é‚Šç•Œåµæ¸¬ (V26 Logic)"""
    page_height = page.rect.height
    top_limit = 0
    bottom_limit = page_height
    paths = page.get_drawings()
    horizontal_lines = []
    
    for p in paths:
        rect = p["rect"]
        if rect.width > page.rect.width * 0.4 and rect.height < 5:
            horizontal_lines.append(rect.y0)
            
    if horizontal_lines:
        horizontal_lines.sort()
        header_candidates = [y for y in horizontal_lines if y < page_height * 0.2]
        if header_candidates: top_limit = header_candidates[-1]
        footer_candidates = [y for y in horizontal_lines if y > page_height * 0.75]
        if footer_candidates: bottom_limit = footer_candidates[0]
            
    return top_limit, bottom_limit

def extract_images_with_boundary_check(pdf_path, output_dir):
    """åœ–ç‰‡æå– (V26 Logic)"""
    images_dir = os.path.join(output_dir, "images")
    if not os.path.exists(images_dir):
        os.makedirs(images_dir)
        
    print("âš ï¸ Extracting images...", file=sys.stderr)
    doc = fitz.open(pdf_path)
    extracted_map = {} 
    
    for page_index in range(len(doc)):
        page = doc[page_index]
        image_list = page.get_images(full=True)
        extracted_map[page_index] = []
        top_limit, bottom_limit = detect_content_boundaries(page)
        
        if image_list:
            for img_index, img in enumerate(image_list):
                xref = img[0]
                is_noise = False
                rects = page.get_image_rects(xref)
                if rects:
                    rect = rects[0]
                    mid_y = (rect.y0 + rect.y1) / 2
                    if mid_y < top_limit or mid_y > bottom_limit:
                        is_noise = True
                
                try:
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image_ext = base_image["ext"]
                    image_name = f"p{page_index}_img{img_index}.{image_ext}"
                    image_path = os.path.join(images_dir, image_name)
                    with open(image_path, "wb") as f:
                        f.write(image_bytes)
                    extracted_map[page_index].append({
                        "filename": image_name, "is_noise": is_noise
                    })
                except Exception:
                    pass
    return extracted_map

def clean_text_noise(text):
    """æ–‡å­—æ¸…æ´— (V26 Logic)"""
    lines = text.split('\n')
    cleaned_lines = []
    patterns = [
        r'^\s*\d+\s+Page\s+\d+\s+of\s+\d+',
        r'^\s*Page\s+\d+\s*$',
        r'^\s*arXiv:\d+\.\d+.*$',
        r'^\s*https?://doi\.org/.*$',
        r'.*Â©.*Permission\s+to\s+make.*',
        r'^\s*Vol\.\s+\d+,\s+No\.\s+\d+.*$'
    ]
    compiled_patterns = [re.compile(p, re.IGNORECASE) for p in patterns]
    
    for line in lines:
        is_noise = False
        if len(line) < 100:
            for p in compiled_patterns:
                if p.match(line):
                    is_noise = True
                    break
        if not is_noise: cleaned_lines.append(line)
    return '\n'.join(cleaned_lines)

def force_normalize_headers(text):
    """æ¨™é¡Œä¿®å¾© (V23 Logic)"""
    lines = text.split('\n')
    new_lines = []
    h2_pattern = re.compile(r'^[*#]*\s*(\d+\.?\s+[A-Z].*?)[*#]*$') 
    h3_pattern = re.compile(r'^[*#]*\s*(\d+\.\d+\.?\s+.*?)[*#]*$')
    # [Fix] Roman Numeral Support (I., II., III., etc.)
    roman_pattern = re.compile(r'^[*#]*\s*([IVX]+\.?\s+[A-Z].*?)[*#]*$')
    special_headers = ["Abstract", "References", "Introduction", "Conclusion"]

    for line in lines:
        clean_line = line.strip()
        if not clean_line:
            new_lines.append(line)
            continue
        content = re.sub(r'^[*#]+\s*', '', clean_line)
        if h3_pattern.match(clean_line):
            match = h3_pattern.match(clean_line)
            content = re.sub(r'^[*#]+\s*', '', match.group(0))
            new_lines.append(f"### {content}")
        # [Fix] é‡å° Abstract*** é€™ç¨®é»åœ¨ä¸€èµ·çš„æƒ…æ³ï¼Œå…ˆå˜—è©¦ç”¨æ­£å‰‡æŠ“å‡ºé—œéµå­—
        elif h2_pattern.match(clean_line):
             match = h2_pattern.match(clean_line)
             content = re.sub(r'^[*#]+\s*', '', match.group(0))
             new_lines.append(f"## {content}")
        # [Fix] Handle Roman Numeral Headers (e.g. II. METHOD)
        elif roman_pattern.match(clean_line):
             match = roman_pattern.match(clean_line)
             content = re.sub(r'^[*#]+\s*', '', match.group(0))
             new_lines.append(f"## {content}")
        else:
             # æª¢æŸ¥æ˜¯å¦ç‚ºç‰¹æ®Šæ¨™é¡Œ (å¿½ç•¥å¤§å°å¯«èˆ‡å¾Œç¶´ç¬¦è™Ÿ)
             is_special = False
             for h in special_headers:
                 if re.match(rf'^[*#]*\s*{h}', clean_line, re.IGNORECASE):
                    # å¼·åˆ¶æ­£è¦åŒ–
                    clean_content = re.sub(r'[*#]', '', clean_line).strip()
                    new_lines.append(f"## {clean_content}")
                    is_special = True
                    break
             
             if not is_special:
                 new_lines.append(line)
        
        # [Fix] Post-process: Check if the last added line is a header that absorbed content
        # e.g. "## Abstract***â€”This paper..." -> "## Abstract" + "This paper..."
        last_line = new_lines[-1]
        if last_line.startswith("## ") or last_line.startswith("### "):
            # Split header and content if they are on same line
            # [Fix] Use character class for separators to handle ***, ---, etc.
            header_match = re.match(r'^(#+\s+)(.*?)([\*â€”â€“-]{3,}|:|â€”)(.*)', last_line)
            if header_match:
                 # Check if the "Heading Part" is a known special header or short enough
                 heading_text = header_match.group(2).strip()
                 separator = header_match.group(3)
                 body_text = header_match.group(4).strip()
                 
                 # Only split if it looks like a section title (short)
                 if len(heading_text) < 50:
                     new_lines.pop()
                     new_lines.append(f"{header_match.group(1)}{heading_text}")
                     new_lines.append("") # [Fix] Add empty line to ensure separation into different blocks
                     new_lines.append(body_text)

    return '\n'.join(new_lines)

def inject_images_sync_filter(text, image_map):
    """åœ–ç‰‡æ³¨å…¥ (V26 Logic)"""
    # [Fix] Broadened regex to catch variations like "filename_page_1_Image_0.png" or similar
    pattern = re.compile(r'!\[(.*?)\]\(.*?_page_(\d+).*?\)')
    parts = []
    last_end = 0
    page_counter = {} 
    
    for match in pattern.finditer(text):
        parts.append(text[last_end:match.start()])
        alt = match.group(1) or "Figure"
        # [Fix] Updated group index to 2 because we removed one capturing group in the regex
        page_idx = int(match.group(2))
        
        if page_idx not in page_counter: page_counter[page_idx] = 0
        current_idx = page_counter[page_idx]
        images_on_page = image_map.get(page_idx, [])
        
        if current_idx < len(images_on_page):
            img_data = images_on_page[current_idx]
            if not img_data["is_noise"]:
                fname = img_data["filename"]
                parts.append(f"![{alt}](images/{fname})")
            page_counter[page_idx] += 1
        else:
            parts.append(f"> *[Figure: Vector/Text - Not Extracted]*")
        last_end = match.end()
    parts.append(text[last_end:])
    return "".join(parts)

def convert_with_marker_and_fix(pdf_path, output_dir):
    images_dir = os.path.join(output_dir, "images")
    if not os.path.exists(images_dir):
        os.makedirs(images_dir)

    try:
        rendered = converter(pdf_path)
        ret_val = text_from_rendered(rendered)
        full_text = ret_val[0] if isinstance(ret_val, tuple) and len(ret_val) >= 2 else str(ret_val)

        smart_image_map = extract_images_with_boundary_check(pdf_path, output_dir)
        full_text = inject_images_sync_filter(full_text, smart_image_map)
        full_text = clean_text_noise(full_text)
        return full_text
    except Exception as e:
        print(f"âŒ Marker Conversion Failed: {e}", file=sys.stderr)
        return None

def split_text_into_logical_blocks(text):
    """
    [V28 Logic] åˆ‡åˆ†æ™‚ä¿è­·æ•¸å­¸å€å¡Šä¸è¢«åˆ‡æ–·
    """
    lines = text.split('\n')
    blocks = []
    current_block = []
    in_table = False
    in_code = False
    in_math = False
    
    for line in lines:
        stripped = line.strip()
        if stripped.startswith("```"): in_code = not in_code
        if stripped == "$$": in_math = not in_math
        if '|' in line and len(line) > 5: in_table = True
        elif stripped == "": in_table = False
        
        current_block.append(line)
        
        if not in_table and not in_code and not in_math and stripped == "":
            content = "\n".join(current_block).strip()
            if content: blocks.append("\n".join(current_block))
            current_block = []
            
    if current_block: blocks.append("\n".join(current_block))
    return blocks

def is_translatable(block):
    """
    [V28 æ–°å¢] åˆ¤æ–·å€å¡Šæ˜¯å¦éœ€è¦ç¿»è­¯
    å›å‚³ False ä»£è¡¨ï¼šé€™æ˜¯å…¬å¼/ä»£ç¢¼/è¡¨æ ¼/åœ–ç‰‡ï¼Œç›´æ¥è·³é API è«‹æ±‚
    """
    # [Fix-V29] Remove HTML tags before checking (e.g. <span id="..."> $$...$$)
    clean_block = re.sub(r'<[^>]+>', '', block).strip()
    
    # 1. æ•¸å­¸å…¬å¼å€å¡Š ($$ ... $$)
    if clean_block.startswith("$$"):
        return False
    # 2. ä»£ç¢¼å€å¡Š (``` ... ```)
    if block.startswith("```"):
        return False
    # 3. åœ–ç‰‡é€£çµ (![...](...))
    if re.match(r'^!\[.*?\]\(.*?\)$', block):
        return False
    # 4. è¡¨æ ¼ (åŒ…å« | åˆ†éš”ç¬¦)
    if "|" in block and "-|-" in block: # ç°¡å–®çš„ Markdown è¡¨æ ¼åµæ¸¬
        return False
    # 5. [Fix] ç¨ç«‹çš„å…¬å¼ç·¨è™Ÿ (ä¾‹å¦‚ "(1)", "(2.1)")
    if re.match(r'^\(\d+(\.\d+)?\)$', block):
        return False
    
    return True

def call_gemini(prompt):
    is_windows = sys.platform.startswith("win")
    for attempt in range(MAX_RETRIES):
        try:
            process = subprocess.Popen(
                [GEMINI_CMD], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                text=True, encoding='utf-8', shell=is_windows 
            )
            stdout, stderr = process.communicate(input=prompt)
            if process.returncode == 0 and len(stdout.strip()) > 0: return stdout
            if "429" in stderr or "quota" in stderr.lower():
                time.sleep(10)
            else:
                time.sleep(2)
        except Exception as e:
            print(f"âŒ API Error: {e}", file=sys.stderr)
            time.sleep(5)
    return None

def translate_batch(batch_blocks, start_id):
    prompt_text = ""
    for i, block in enumerate(batch_blocks):
        prompt_text += f"<<<ID_{start_id + i}>>>\n{block}\n\n"

    # [V28 Prompt] ç§»é™¤å·²ç¶“åœ¨ Python ç«¯éæ¿¾æ‰çš„è¦å‰‡ï¼Œç²¾ç°¡ Prompt
    prompt = f"""
SYSTEM_MODE: ACADEMIC_TRANSLATOR
**TASK:** Translate text blocks to Traditional Chinese (Taiwan).

**OUTPUT FORMAT:**
<<<ID_x>>>
[Chinese Translation]

**RULES:**
1. **Style:** Academic, formal.
2. **Inline Math:** Keep inline LaTeX (`$...$`) EXACTLY as is.
3. **No English:** Output ONLY Chinese.

**INPUT:**
{prompt_text}

**OUTPUT:**
"""
    result = call_gemini(prompt)
    translations = {}
    if result:
        matches = re.finditer(r'<<<ID_(\d+)>>>\s*(.*?)(?=(<<<ID_|\Z))', result, re.DOTALL)
        for match in matches:
            idx = int(match.group(1))
            content = match.group(2).strip()
            translations[idx] = content
    return translations

def process_paper(pdf_path, output_path):
    output_dir = os.path.dirname(output_path)
    raw_output_path = str(Path(output_path).with_suffix('.raw.md'))
    
    print(f"ğŸ”¥ Processing PDF with Marker...", file=sys.stderr)
    raw_md = convert_with_marker_and_fix(pdf_path, output_dir)
    if not raw_md: return False

    print("ğŸ“ Normalizing Headers...", file=sys.stderr)
    raw_md = force_normalize_headers(raw_md)

    with open(raw_output_path, 'w', encoding='utf-8') as f:
        f.write(raw_md)

    ref_match = re.search(r'^##\s+(References|Bibliography)', raw_md, re.MULTILINE | re.IGNORECASE)
    
    pre_text = ""
    body_text = raw_md
    post_text = ""
    
    if ref_match:
        split_idx = ref_match.start()
        post_text = raw_md[split_idx:]
        body_text = raw_md[:split_idx]
    
    abstract_match_in_body = re.search(r'^##\s+Abstract', body_text, re.MULTILINE | re.IGNORECASE)
    if abstract_match_in_body:
        split_idx = abstract_match_in_body.start()
        pre_text = body_text[:split_idx]
        body_text = body_text[split_idx:]

    original_blocks = split_text_into_logical_blocks(body_text)
    print(f"ğŸ§© Translating body: {len(original_blocks)} blocks.", file=sys.stderr)

    final_blocks = []
    current_batch = []
    current_batch_len = 0
    batch_start_index = 0
    
    # é€™è£¡æˆ‘å€‘éœ€è¦ä¸€å€‹ Map ä¾†è¨˜éŒ„ã€Œå“ªäº› ID è¢«è·³éäº†ã€
    # æˆ–è€…ç°¡å–®ä¸€é»ï¼šæˆ‘å€‘åªæŠŠã€Œå¯ç¿»è­¯ã€çš„å€å¡ŠåŠ å…¥ current_batch
    # ä½†æ˜¯ current_batch è£¡çš„ ID å¿…é ˆè·Ÿ original_blocks çš„ index å°æ‡‰å—ï¼Ÿ
    # V28 ç­–ç•¥ï¼šBatch è£¡é¢çš„ ID ä½¿ç”¨ original_blocks çš„çœŸå¯¦ Index
    
    print(f"ğŸš€ Starting Batch Translation (Max: {BATCH_SIZE_LIMIT} chars)...", file=sys.stderr)

    # 1. æ”¶é›†ç¿»è­¯çµæœ
    all_translations = {} 

    for i, block in enumerate(original_blocks):
        # [V28 æ ¸å¿ƒ] æœ¬åœ°ç«¯éæ¿¾ï¼šå¦‚æœä¸å¯ç¿»è­¯ï¼Œç›´æ¥è·³éï¼Œä¸åŠ å…¥ Batch
        if not is_translatable(block):
            continue 
            
        # [Fix] Skip "Not Extracted" placeholders
        if "vector/text - not extracted" in block.lower():
            continue

        current_batch.append((i, block)) # å­˜å…¥ (Index, Content)
        current_batch_len += len(block)

        if current_batch_len >= BATCH_SIZE_LIMIT or i == len(original_blocks) - 1:
            # å»ºæ§‹ payloadï¼Œæ³¨æ„é€™è£¡ start_id ä¸å†æ˜¯å–®ç´”çš„è¨ˆæ•¸ï¼Œè€Œæ˜¯çœŸå¯¦ Index
            prompt_text = ""
            for idx, content in current_batch:
                prompt_text += f"<<<ID_{idx}>>>\n{content}\n\n"
            
            # å‘¼å« API (é€™è£¡æŠŠ translate_batch å…§è¯å±•é–‹ä»¥ä¾¿è™•ç†è‡ªè¨‚ ID)
            # [V28 Prompt] 
            prompt = f"""
SYSTEM_MODE: ACADEMIC_TRANSLATOR
**TASK:** Translate text blocks to Traditional Chinese (Taiwan).

**OUTPUT FORMAT:**
<<<ID_x>>>
[Chinese Translation]

**RULES:**
1. **Style:** Academic, formal.
2. **Inline Math:** Keep inline LaTeX (`$...$`) EXACTLY as is.
3. **No English:** Output ONLY Chinese.

**INPUT:**
{prompt_text}

**OUTPUT:**
"""
            print(f"ğŸ“¤ Sending Batch (Count: {len(current_batch)})...", file=sys.stderr)
            result = call_gemini(prompt)
            
            if result:
                matches = re.finditer(r'<<<ID_(\d+)>>>\s*(.*?)(?=(<<<ID_|\Z))', result, re.DOTALL)
                for match in matches:
                    idx = int(match.group(1))
                    content = match.group(2).strip()
                    all_translations[idx] = content
            
            current_batch = []
            current_batch_len = 0
            time.sleep(2)

    # 2. é‡çµ„æ–‡ç« 
    for i, block in enumerate(original_blocks):
        trans_text = all_translations.get(i, "")
        
        # å¦‚æœ trans_text ç‚ºç©º (å¯èƒ½æ˜¯è¢« is_translatable éæ¿¾æ‰ï¼Œæˆ–æ˜¯ API æ²’å›å‚³)
        # å°±åªé¡¯ç¤ºåŸæ–‡
        if not trans_text or trans_text == "[ORIGINAL]":
            final_blocks.append(block)
        else:
            # [Feature] Header Inline Translation
            # User Request: "### *B. Datasets- è³‡æ–™é›†*"
            if block.strip().startswith("#"):
                # æ¸…é™¤ç¿»è­¯ä¸­å¯èƒ½é‡è¤‡çš„å‰ç¶´ç¬¦è™Ÿ (å¦‚ "### è³‡æ–™é›†" -> "è³‡æ–™é›†")
                # [Fix] Remove newlines to prevent list formatting
                clean_trans = re.sub(r'^#+\s*', '', trans_text).replace('\n', ' ').replace('\r', '').strip()
                # [Fix] Remove HTML tags from translated header to prevent duplication (e.g. <span...>Title)
                clean_trans = re.sub(r'<[^>]+>', '', clean_trans).strip()
                # [Fix] Strip ONLY Chinese numbering artifacts (e.g. "å£¹ã€", "ç”²ã€") per user request.
                # Left digits and Roman numerals alone to avoid removing useful content.
                clean_trans = re.sub(r'^([å£¹è²³åƒè‚†ä¼é™¸æŸ’æŒç–æ‹¾ç”²ä¹™ä¸™ä¸æˆŠ]+[ã€.ï¼])\s*', '', clean_trans).strip()
                final_blocks.append(f"{block.strip()} - {clean_trans}")
            else:
                # [Fix] Prevent duplication if translation is identical to original
                if block.strip() == trans_text.strip():
                     final_blocks.append(block)
                else:
                     final_blocks.append(f"{block}\n\n> {trans_text}")

    full_content = ""
    if pre_text: full_content += pre_text + "\n\n"
    
    body_content = "\n\n".join(final_blocks)
    body_content = re.sub(r'<<<ID_\d+>>>', '', body_content)
    full_content += body_content
    
    if post_text: 
        # [Fix] 1. Join broken lines where a sentence was split into a list item (e.g. "useful\n - life")
        # Match newline followed by "- " and NOT followed by "[n]" (negative lookahead)
        post_text = re.sub(r'\n\s*-\s+(?!\[\d+\])(.*)', r' \1', post_text)
        
        # [Fix] 4. Explode inline references (e.g. [1] A [2] B -> [1] A \n\n [2] B)
        # Handle cases with optional span tags before the bracket
        # Valid patterns: "[1]", "<span...></span>[1]", "</span> [1]"
        post_text = re.sub(r'(<span[^>]*>)?\s*\[\d+\]', r'\n\n\g<0>', post_text)

        # [Fix] 2. Repair reference list formatting: "- [1]" -> "[1]" to avoid checklist rendering
        post_text = re.sub(r'^\s*-\s*\[(\d+)\]', r'[\1]', post_text, flags=re.MULTILINE)
        
        # [Fix] 3. Add spacing between references (Ensure \n\n before [n])
        # Replace existing newlines before [n] with exactly \n\n
        post_text = re.sub(r'\n+\s*(\[\d+\])', r'\n\n\1', post_text)
        
        full_content += "\n\n" + post_text

    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
        print(f"âœ… Success: {output_path}", file=sys.stderr)
    except Exception as e:
        print(f"âŒ Write Error: {e}", file=sys.stderr)

if __name__ == "__main__":
    if len(sys.argv) < 2: sys.exit(1)
    input_file = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) >= 3 else str(Path(input_file).with_suffix('.zh.md'))
    process_paper(input_file, output_file)