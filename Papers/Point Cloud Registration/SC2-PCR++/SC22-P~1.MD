# SC2-PCR++：為實現高效穩健點雲配準的生成與選擇之反思

## 期刊:12358 IEEE 模式分析與機器智能彙刊，第 45 卷，第 10 期，2023 年 10 月

## 作者: Zhi Chen , Kun Sun , Member, IEEE, Fan Yang , Lin Guo , and Wenbing Tao , Member, IEEE


Abstract—Outlier removal is a critical part of feature-based point cloud registration. In this article, we revisit the model generation and selection of the classic RANSAC approach for fast and robust point cloud registration. For the model generation, we propose a second-order spatial compatibility (SC2) measure to compute the similarity between correspondences. It takes into account global compatibility instead of local consistency, allowing for more distinctive clustering between inliers and outliers at an early stage. The proposed measure promises to find a certain number of outlier-free consensus sets using fewer samplings, making the model generation more efficient. For the model selection, we propose a new Feature and Spatial consistency constrained Truncated Chamfer Distance (FS-TCD) metric for evaluating the generated models. It considers the alignment quality, the feature matching properness, and the spatial consistency constraint simultaneously, enabling the correct model to be selected even when the inlier rate of the putative correspondence set is extremely low. Extensive experiments are carried out to investigate the performance of our method. In addition, we also experimentally prove that the proposed SC2 measure and the FS-TCD metric are general and can be easily plugged into deep learning based frameworks.

摘要—離群點移除是基於特徵的點雲配準中的一個關鍵部分。在本文中，我們重新審視了經典 RANSAC 方法的模型生成和選擇，以實現快速而穩健的點雲配準。在模型生成方面，我們提出了一種二階空間相容性 (second-order spatial compatibility, SC2) 度量來計算對應關係之間的相似性。它考慮了全域相容性而非局部一致性，從而在早期階段能夠更清晰地區分內點和離群點的聚類。所提出的度量有望使用更少的採樣找到一定數量的無離群點共識集，從而使模型生成更有效率。在模型選擇方面，我們提出了一種新的特徵與空間一致性約束的截斷 Chamfer 距離 (Feature and Spatial consistency constrained Truncated Chamfer Distance, FS-TCD) 度量來評估生成的模型。它同時考慮了對齊品質、特徵匹配的適當性以及空間一致性約束，即使在假定對應集之內點率極低的情況下，也能夠選擇出正確的模型。我們進行了大量實驗來研究我們方法的性能。此外，我們也透過實驗證明，所提出的 SC2 度量和 FS-TCD 度量是通用的，可以輕易地整合到基於深度學習的框架中。

Index Terms—Point cloud registration, second-order spatial compatibility, constrained truncated chamfer distance, rigid transformation estimation.

索引詞—點雲配準、二階空間相容性、約束截斷倒角距離、剛性變換估計。

# I. I NTRODUCTION

一、緒論

THE alignment of two 3D scans of the same scene, known as Point Cloud Registration (PCR), plays an important role in areas such as Simultaneous Localization and Mapping (SLAM) [1], [2], [3], [4], augmented reality [5], [6] and robotics applications [7]. A canonical solution first establishes feature correspondences and then estimates the 3D rotation and translation that achieve optimal alignment of the common parts.

將同一場景的兩個 3D 掃描對齊，即點雲配準 (Point Cloud Registration, PCR)，在諸如同時定位與地圖構建 (Simultaneous Localization and Mapping, SLAM) [1], [2], [3], [4]、擴增實境 [5], [6] 以及機器人應用 [7] 等領域扮演著重要角色。一個典型的解決方案是首先建立特徵對應，然後估計能夠實現共同部分最佳對齊的 3D 旋轉與平移。


程式碼將於 https://github.com/ZhiChen902/SC2-PCRplusplus 提供。

However, due to challenges such as partial overlap or feature ambiguity, model estimation is prone to outliers in the correspondences, leading to inaccurate or wrong alignment.

然而，由於部分重疊或特徵模糊等挑戰，模型估計容易受到對應關係中離群點的影響，導致對齊不準確或錯誤。

RANdom SAmple Consensus (RANSAC) [8] pioneers the generation-and-selection strategy for model estimation. It generates a lot of hypothetical models through random sampling and selects the best hypothesis with the maximum consensus as the final result. The goal of the sampling process is to obtain an outlier-free set, so as to estimate a robust transformation while excluding the impact of outliers. However, it needs massive samplings or sometimes there is no guarantee of an accurate solution due to the low inlier rate. Spatial Compatibility (SC) [9], [10], [11], [12] is a widely used similarity measure for boosting the robustness and efficiency of the rigid transformation estimation. It assumes that two correspondences will have a higher score if the difference of spatial distance between them, e.g., | d12 − d′ 12 | or | d16 − d′ 16 | in Fig. 1(a), is small. Thus, sampling from compatible correspondences increases the probability of getting inliers. However, such kind of first-order metric still suffers from outliers due to locality and ambiguity. A toy example is shown in Fig. 1. There are five inliers {c1, c2, c3, c4, c5} and two outliers {c6, c7} in Fig. 1(a). As we can see from the yellow cells in Fig 1(b), c6 and c7 are outliers but they show high compatibility scores with some inliers by chance. As a result, the outliers would be inevitably involved in the model estimation process, leading to performance deterioration.

隨機抽樣一致性 (RANdom SAmple Consensus, RANSAC) [8] 開創了模型估計的「生成與選擇」策略。它透過隨機抽樣生成大量假設模型，並選擇具有最大共識的最佳假設作為最終結果。抽樣過程的目標是獲得一個無離群點的集合，以便在排除離群點影響的同時，估計出一個穩健的變換。然而，這需要大量的抽樣，有時由於內點率低，無法保證得到準確的解。空間相容性 (Spatial Compatibility, SC) [9], [10], [11], [12] 是一種廣泛使用的相似性度量，用於提升剛性變換估計的穩健性和效率。它假設如果兩個對應點之間的空間距離差異很小，例如圖 1(a) 中的 | d12 − d′ 12 | 或 | d16 − d′ 16 |，則它們將具有較高的分數。因此，從相容的對應點中抽樣會增加獲得內點的機率。然而，這種一階度量由於其局部性和模糊性，仍然會受到離群點的影響。圖 1 展示了一個玩具範例。圖 1(a) 中有五個內點 {c1, c2, c3, c4, c5} 和兩個離群點 {c6, c7}。從圖 1(b) 的黃色儲存格中我們可以看到，c6 和 c7 是離群點，但它們偶然地與某些內點顯示出高的相容性分數。結果，離群點將不可避免地被捲入模型估計過程中，導致性能下降。

In this article, we propose a new global measure of the similarity between two correspondences. Specifically, we first binarize the spatial compatibility matrix into the hard form, as shown in Fig. 1(c). Then, for two compatible correspondences, we compute the number of correspondences that are simultaneously compatible with both of them as the new similarity between them. The globally common compatibility is set to 0 for any two incompatible correspondences. Therefore, the similarity between two inliers is at least the number of inliers excluding themselves from all the correspondences. However, the outliers do not have such good properties. To be specific, in Fig. 1(d), the similarities within the inliers {c1, c2, c3, c4, c5} are no less than 3, while the similarities related to the outliers {c6, c7} are no more than 1. Therefore, the global compatibility matrix in Fig. 1(d) can better distinguish inliers from outliers. Since the new measure can be expressed as the matrix product of the traditional first-order metric (See (8)), we name it the second-order spatial compatibility (SC2) measure.

在本文中，我們提出了一種新的全域度量方法來衡量兩個對應關係之間的相似性。具體來說，我們首先將空間相容性矩陣二值化為硬形式，如圖 1(c) 所示。然後，對於兩個相容的對應關係，我們計算同時與它們兩者都相容的對應關係數量，作為它們之間新的相似性。對於任何兩個不相容的對應關係，其全域共同相容性設為 0。因此，兩個內點之間的相似性至少是所有對應關係中排除它們自身之外的內點數量。然而，離群點不具備這樣好的特性。具體來說，在圖 1(d) 中，內點 {c1, c2, c3, c4, c5} 之間的相似性不小於 3，而與離群點 {c6, c7} 相關的相似性則不超過 1。因此，圖 1(d) 中的全域相容性矩陣能更好地分辨內點與離群點。由於這個新度量可以表示為傳統一階度量矩陣的乘積（見公式(8)），我們將其命名為二階空間相容性（SC2）度量。


CHEN et al.: SC2 -PCR++: RETHINKING THE GENERATION AND SELECTION FOR EFFICIENT AND ROBUST POINT CLOUD REGISTRATION 12359

CHEN 等人：SC2-PCR++：為高效穩健的點雲配準重新思考生成與選擇 12359

[Image]

[圖像]

Fig. 1. (a): A toy example in which red and green line segments represent outliers and inliers, respectively. (b): The first order compatibility matrix of (a). As highlighted in yellow, the outliers have very high compatibility scores with some inliers. (c): A binarized compatibility matrix of (b) after thresholding. (d): The proposed second-order compatibility matrix of (a). By contrast, the values in the rows and columns of the outliers are small.

圖 1. (a)：一個玩具範例，其中紅色和綠色線段分別代表離群值和內點。 (b)：(a) 的一階相容性矩陣。如黃色突顯部分所示，離群值與某些內點具有非常高的相容性分數。 (c)：(b) 經過閾值處理後的二值化相容性矩陣。 (d)：(a) 的二階相容性矩陣。相比之下，離群值所在行和列的值都很小。

The proposed second-order spatial compatibility measure SC2 has several advantages. 1) The inliers are much easier distinguished from the outliers. Suppose we have m inliers in n correspondences. The scores between any two inliers would be no less than m-2. However, it is difficult for an outlier to be simultaneously compatible with multiple correspondences and the score for it will be much smaller. 2) Based on the proposed SC2 matrix, for each row vector corresponding to an inlier, we can easily find an outlier-free set by selecting the top k correspondences with the highest scores. In this way, the m valid samplings can be obtained by traversing all the n rows of the SC2 matrix. Therefore, we can ensure m reliable model estimations by only n samplings, which makes the model estimation more efficient and robust. 3) We theoretically prove that the SC2 matrix significantly reduces the probability of wrong sampling from a probabilistic view. We define an ambiguity event, in which the score between two inliers is smaller than that between an inlier and an outlier. By computing the probability distributions of this event for both the first-order metric and our second-order metric, the SC2 matrix is much more robust to obtain reliable sampling (see Fig. 4).

我們提出的二階空間相容性度量 SC2 有幾個優點。1) 內點更容易與離群點區分開來。假設在 n 個對應中我們有 m 個內點。任意兩個內點之間的分數將不小於 m-2。然而，一個離群點很難同時與多個對應相容，因此其分數會小得多。2) 基於我們提出的 SC2 矩陣，對於每個對應於內點的行向量，我們可以透過選擇分數最高的 k 個對應，輕易地找到一個無離群點的集合。透過這種方式，遍歷 SC2 矩陣的所有 n 行，即可獲得 m 個有效的採樣。因此，我們僅需 n 次採樣就能確保 m 次可靠的模型估計，這使得模型估計更有效率且更穩健。3) 我們從機率的角度理論上證明了 SC2 矩陣顯著降低了錯誤採樣的機率。我們定義了一個模糊事件，即兩個內點之間的分數小於一個內點和一個離群點之間的分數。透過計算一階度量和我們的二階度量下此事件的機率分佈，我們發現 SC2 矩陣在獲得可靠採樣方面要穩健得多（見圖 4）。

Besides efficient sampling for model generation, another important issue is selecting the best model from the estimated hypotheses. RANSAC adopts the inlier count [8] as the hypothesis evaluation metric. It counts the number of correspondences whose alignment error is less than a pre-defined threshold after aligning by the estimated hypothesis and selects the model with the highest number as the final result. However, due to the limitation of feature descriptors or challenge brought by low overlap, sometimes the number of inliers in the putative correspondences is extremely small. In this case, even if an outlier-free set is sampled and a correct rigid transformation is estimated, the related inlier count value may be still small. This brings great challenges to selecting the correct hypothesis as the final result. To address this issue, we expect to introduce another widely used metric in point cloud processing, i.e., Chamfer distance (CD) [13], [14], [15], [16], to model selection. It calculates the distribution similarity of the two aligned point clouds. Taking the global alignment information into account, it gets rid of the dependence on the inlier rate of the putative correspondences. However, Chamfer distance totally ignores the appearance information, making it susceptible to noises and unstable in low-overlapped scenes. Meanwhile, computing Chamfer distance needs to search for the nearest neighbor in the global scope. Since it could generate lots of hypotheses during the model selection process, directly using it as the selection metric is time-consuming. Based on the above observations, we propose a new Feature and Spatial consistency constrained Truncated Chamfer Distance (FS-TCD) metric for hypothesis selection. Instead of searching for the nearest neighbor in the global set, it utilizes the feature information to narrow the search space, making it more efficient. Meanwhile, due to introducing the constraint of feature information, the anti-noise ability of our method is greatly enhanced. In order to eliminate the influence of noises, we further adopt the spatial consistency constraint [9], [10] into the FS-TCD metric. In Section V-B, we also show that FS-TCD is a more generalized inlier count, which greatly reduces the dependence on the inlier rate of the putative correspondences.

除了高效的模型生成採樣外，另一個重要議題是從估計的假設中選擇最佳模型。RANSAC 採用內點計數 [8] 作為假設評估指標。它計算在估計的假設對齊後，對齊誤差小於預定閾值的對應數量，並選擇數量最多的模型作為最終結果。然而，由於特徵描述子的限制或低重疊帶來的挑戰，有時假定對應中的內點數量極少。在這種情況下，即使採樣到一個無離群點的集合並估計出正確的剛性變換，相關的內點計數值可能仍然很小。這給選擇正確的假設作為最終結果帶來了巨大挑戰。為了解決這個問題，我們期望引入點雲處理中另一個廣泛使用的指標，即倒角距離 (Chamfer distance, CD) [13], [14], [15], [16]，用於模型選擇。它計算兩個對齊點雲的分佈相似性。考慮到全域對齊資訊，它擺脫了對假定對應之內點率的依賴。然而，倒角距離完全忽略了外觀資訊，使其容易受到雜訊影響，在低重疊場景中不穩定。同時，計算倒角距離需要在全域範圍內搜索最近鄰。由於在模型選擇過程中可能會生成大量假設，直接將其用作選擇指標非常耗時。基於以上觀察，我們提出了一種新的特徵與空間一致性約束的截斷倒角距離 (Feature and Spatial consistency constrained Truncated Chamfer Distance, FS-TCD) 指標用於假設選擇。它不是在全域集合中搜索最近鄰，而是利用特徵資訊來縮小搜索空間，使其更有效率。同時，由於引入了特徵資訊的約束，我們方法的抗雜訊能力大大增強。為了消除雜訊的影響，我們進一步將空間一致性約束 [9], [10] 納入 FS-TCD 指標。在第五節-B 中，我們也將證明 FS-TCD 是一種更廣義的內點計數，它大大降低了對假定對應之內點率的依賴。

Based on the proposed SC2 measure and the FS-TCD metric, we design an efficient and robust point cloud registration method, named SC2-PCR++. Following [9], [17], [18], it first selects several seeds that are likely to be inliers. Then a two-stage sampling is carried out to construct a consensus set for each seed. Afterward, the weighted SVD is used to estimate a tentative model for each consensus set. Finally, the FS-TCD metric is adopted to select the best model as the final result. In a nutshell, this paper distinguishes itself from existing methods in the following aspects.

基於所提出的 SC2 度量和 FS-TCD 度量，我們設計了一種高效且穩健的點雲配準方法，命名為 SC2-PCR++。遵循 [9]、[17]、[18] 的方法，它首先選擇幾個可能是內點的種子點。然後進行兩階段採樣，為每個種子點構建一個共識集。之後，使用加權 SVD 為每個共識集估計一個初步模型。最後，採用 FS-TCD 度量來選擇最佳模型作為最終結果。簡而言之，本文在以下幾個方面與現有方法有所區別。

 For model generation, a second-order spatial compatibility (SC2) measure is proposed. We prove that SC2 significantly reduces the probability of an outlier being involved in the consensus set. Since the proposed method encodes richer information beyond the first-order metric, it enhances the robustness against outliers.

 針對模型生成，提出了一種二階空間相容性（SC2）度量。我們證明了 SC2 顯著降低了離群點被納入共識集的機率。由於所提出的方法編碼了超越一階度量的更豐富資訊，它增強了對離群點的穩健性。

 For model selection, a new Feature and Spatial consistency constrained Truncated Chamfer Distance (FS-TCD) metric is proposed. It integrates the feature similarity, spatial consistency, and alignment quality, so that it can still find the best-estimated model even if the inlier ratio is extremely low.

 針對模型選擇，提出了一種新的特徵與空間一致性約束截斷倒角距離（FS-TCD）度量。它整合了特徵相似性、空間一致性和對齊品質，因此即使在內點率極低的情況下，仍能找到最佳估計模型。

 Compared with state-of-the-art learning methods such as [9], [11], [19], [20], our method is a light weighted solution that does not need training. It shows no bias across different datasets and generalizes well on various scenarios, which is elaborated in the experiments.

 與最先進的學習方法（如 [9], [11], [19], [20]）相比，我們的方法是一種無需訓練的輕量級解決方案。它沒有表現出偏差橫跨不同資料集，並在各種場景下具有良好的泛化能力，這在實驗中有詳細闡述。

 The proposed method is general. Although we implement it in a handcrafted fashion, it could be easily plugged into other deep learning frameworks such as PointDSC [9]. We show in the experiments that PointDSC produces better results when combined with the proposed SC2 measure and FS-TCD metric.

 所提出的方法是通用的。雖然我們以手工製作的方式實現它，但它可以很容易地插入到其他深度學習框架中，例如 PointDSC [9]。我們在實驗中表明，當與所提出的 SC2 度量和 FS-TCD 度量相結合時，PointDSC 會產生更好的結果。

This paper is an extension of our previous work SC2-PCR (CVPR 2022) [21]. We have made several additions: 1) More exhaustive analyses and discussions about SC2 measure are added. We present the detailed derivation of the ambiguity probability of SC2 measure in Section IV-B, and add the experiments of parameters for SC2 measure based sampling. 2) A new FS-TCD metric is employed to select the best hypothesis over the models produced by the SC2 measure. Based on the FS-TCD metric, we develop a two-stage model selection strategy to accelerate the selection process. 3) A new pipeline named SC2-PCR++ is designed. Different from SC2-PCR, it adopts different matching methods for model generation and selection to overcome the wrong model selection in low-inlier-rate scenes. 4) More experiments are conducted to validate the performance of the proposed method. We use more datasets and competing methods in the experimental section.

本文是我們之前工作 SC2-PCR (CVPR 2022) [21] 的擴展。我們做了幾項補充：1) 增加了更多關於 SC2 度量的詳盡分析和討論。我們在第四節-B 中給出了 SC2 度量模糊機率的詳細推導，並增加了基於 SC2 度量採樣的參數實驗。2) 採用新的 FS-TCD 度量來從 SC2 度量產生的模型中選擇最佳假設。基於 FS-TCD 度量，我們開發了一個兩階段模型選擇策略來加速選擇過程。3) 設計了一個名為 SC2-PCR++ 的新流程。與 SC2-PCR 不同，它在模型生成和選擇中採用了不同的匹配方法，以克服在低內點率場景下的錯誤模型選擇問題。4) 進行了更多實驗來驗證所提方法的性能。我們在實驗部分使用了更多的資料集和競爭方法。

# II. RELATED WORKS

二、相關研究

## A. 3D Feature Matching

A. 3D 特徵匹配

Traditional Feature Matching: An important step of featurebased point cloud registration is to establish correspondences by matching local descriptors. Some methods utilize the histograms of spatial distribution to generate the local descriptors. Spin Image (SI) [22], [23] uses the Principal Component Analysis (PCA) to compress the spin-images so that it is efficient enough for recognition from large model libraries. 3D Shape Context (3DSC) [24] directly extends the 2D shape context to three dimensions and shows that it is more robust to noisy scenes. Unique Shape Context (USC) [25] presents a comprehensive proposal that does not need to compute multiple descriptors for a detected key point. Some other works represent the local descriptor by geometric attribute histogram. Persistent Feature Histograms (PFH) [26] uses a 16D feature for characterizing the local geometry. It improves the robustness to the variations of position, orientation, or sampling density. Fast Point Feature Histogram (FPFH) [27] simplifies the PFH as SPFH by only considering the neighbors of the point, and generates the FPFH utilizing the weighted sum of SPFH. A more comprehensive study about hand-crafted feature matching can be found in [28].

傳統特徵匹配：基於特徵的點雲配準的一個重要步驟是透過匹配局部描述子來建立對應關係。一些方法利用空間分佈的直方圖來生成局部描述子。自旋影像（Spin Image, SI）[22], [23] 使用主成分分析（Principal Component Analysis, PCA）來壓縮自旋影像，使其足以有效地從大型模型庫中進行識別。3D 形狀上下文（3D Shape Context, 3DSC）[24] 直接將 2D 形狀上下文擴展到三維，並證明其在雜訊場景中更為穩健。唯一形狀上下文（Unique Shape Context, USC）[25] 提出了一個全面的方案，無需為一個檢測到的關鍵點計算多個描述子。其他一些工作則透過幾何屬性直方圖來表示局部描述子。持久特徵直方圖（Persistent Feature Histograms, PFH）[26] 使用一個 16 維的特徵來描述局部幾何。它提高了對位置、方向或採樣密度變化的穩健性。快速點特徵直方圖（Fast Point Feature Histogram, FPFH）[27] 將 PFH 簡化為 SPFH，僅考慮點的鄰居，並利用 SPFH 的加權和生成 FPFH。關於手工特徵匹配的更全面研究可見於 [28]。

Learning-Based Feature Matching: Recently, deep learning techniques are also introduced to learn 3D local descriptors [29], [30], [31], [32]. The pioneering 3DMatch [29] builds a Siamese Network for extracting local descriptors and provides the 3DMatch benchmark for evaluating the performance of point cloud registration. Following 3DMatch, some methods boost the performance by designing more suitable architectures for extracting local and global information. PPFNet [33] and the unsupervised PPFFoldNet [34] combine the point pair features with the global context generated by the PointNet [35]. 3DSmoothNet [36] and FCGF [30] build fully convolution networks by using voxelized smoothed density value (SDV) representation and Minkowski convolution [37] respectively. Keypoint detection modules [31], [38], [39] are also integrated into the descriptor learning networks. More recently, the successfully applied Transformer [40] architecture is also introduced to 3D feature matching area. Predator [41] designs an overlap-aware module by the self-cross-self attention operations. CoFINet [42] reformulates the Predator into a coarse-to-fine pipeline. Lepard [43] and GeoTransformer [44] add the new position encoding techniques into the attention operations.

基於學習的特徵匹配：最近，深度學習技術也被引入來學習 3D 局部描述子 [29], [30], [31], [32]。開創性的 3DMatch [29] 建立了一個用於提取局部描述子的 Siamese 網路，並提供了 3DMatch 基準來評估點雲配準的性能。繼 3DMatch 之後，一些方法透過設計更適合提取局部和全域資訊的架構來提升性能。PPFNet [33] 和無監督的 PPFFoldNet [34] 將點對特徵與 PointNet [35] 生成的全域上下文相結合。3DSmoothNet [36] 和 FCGF [30] 分別使用體素化的平滑密度值 (SDV) 表示和 Minkowski 卷積 [37] 來構建全卷積網路。關鍵點檢測模組 [31], [38], [39] 也被整合到描述子學習網路中。最近，成功應用的 Transformer [40] 架構也被引入到 3D 特徵匹配領域。Predator [41] 透過自交叉自注意力操作設計了一個重疊感知模組。CoFINet [42] 將 Predator 重新構建為一個由粗到精的流程。Lepard [43] 和 GeoTransformer [44] 在注意力操作中加入了新的位置編碼技術。

Although these methods achieve remarkable performance improvements, they can hardly establish a totally outlier-free correspondence set. They depend on the model fitting method for robust rigid transformation estimation.

儘管這些方法取得了顯著的性能提升，但它們很難建立一個完全沒有離群點的對應集。它們依賴於模型擬合方法來進行穩健的剛性變換估計。

## B. Model Fitting

B. 模型擬合

Generation and Selection Framework: The Generation and Selection framework is the most significant pipeline for modelfitting, starting with the well-known RANSAC [8]. It can be applied to model estimation tasks in which the model is clearly defined and it is robust to outlier points. In past decades, many of its variants [45], [46], [47], [48] have been proposed. Instead of random sampling, PROSAC [47] computes the ordering by the similarity of local descriptors, and adopts the progressive sampling strategy to accelerate the RANSAC. EVSAC [49] further transforms the descriptor similarity to the confidence value for sampling by means of the extreme value theory. Lo-RANSAC [50] and accelerated FLo-RANSAC [51] perform local optimization to reduce the noises brought by the minimum subset. More recently, Graph-cut RANSAC [52], [53] proposes to use the Graph-cut technique for performing the local optimization step on the so-far-the-best model. Magsac [54] proposes a σ-consensus to build a threshold-free method for RANSAC. A more comprehensive study about RANSAC family can be found in [55].

生成與選擇框架：生成與選擇框架是模型擬合最重要的流程，始於著名的 RANSAC [8]。它可應用於模型定義清晰且對離群點穩健的模型估計任務。在過去幾十年中，已提出了許多其變體 [45], [46], [47], [48]。PROSAC [47] 不採用隨機抽樣，而是根據局部描述子的相似性計算排序，並採用漸進式抽樣策略來加速 RANSAC。EVSAC [49] 進一步利用極值理論將描述子相似性轉換為抽樣的信賴值。Lo-RANSAC [50] 和加速的 FLo-RANSAC [51] 執行局部優化以減少最小子集帶來的雜訊。最近，Graph-cut RANSAC [52], [53] 提議使用圖割技術對迄今為止最好的模型執行局部優化步驟。Magsac [54] 提出了一種 σ-共識來為 RANSAC 建立一個無閾值的方法。關於 RANSAC 家族的更全面研究可在 [55] 中找到。

Learning-Based Model Fitting: Recent works also adopt deep learning techniques, which were studied earlier in the 2D matching area, to model fitting tasks. The 2D correspondence selection network CN-Net [56] and its variants [57], [58], [59], [60], [61], [62], [63] formulate the model fitting as a combination of a correspondence classification module and a model estimation module. Recent attempts [9], [11], [19], [20] also introduce deep learning networks for 3D correspondence pruning. 3DRegNet [19] reformulates the CN-Net [56] into 3D form and designs a regression module to solve rigid transformation. DGR [20] introduces the full convolution to better capture global context for correspondence classification, and uses weight Procrustes for model estimation. PointDSC [9] develops a spatial consistency based non-local module and a Neural Spectral matching to accelerate the model generation and selection. DetarNet [64] presents decoupling solutions for translation and rotation. DHVR [11] exploits the deep Hough voting to identify the consensus from the Hough space, so as to predict the final transformation. COTReg [65] presents a coupled optimal transport based correspondence prediction module and integrates it into the network.

基於學習的模型擬合：近期的研究也採用了深度學習技術來處理模型擬合任務，這些技術早期在 2D 匹配領域已有研究。2D 對應選擇網路 CN-Net [56] 及其變體 [57], [58], [59], [60], [61], [62], [63] 將模型擬合表述為一個對應分類模組和一個模型估計模組的組合。近期的嘗試 [9], [11], [19], [20] 也引入了深度學習網路用於 3D 對應的修剪。3DRegNet [19] 將 CN-Net [56] 重新構建為 3D 形式，並設計了一個迴歸模組來解決剛性變換問題。DGR [20] 引入了全卷積以更好地捕捉用於對應分類的全域上下文，並使用加權 Procrustes 進行模型估計。PointDSC [9] 開發了一個基於空間一致性的非局部模組和一個神經譜匹配來加速模型的生成和選擇。DetarNet [64] 提出了平移和旋轉的解耦方案。DHVR [11] 利用深度霍夫投票從霍夫空間中識別共識，以預測最終的變換。COTReg [65] 提出了一個基於耦合最優傳輸的對應預測模組，並將其整合到網路中。

Spatial Compatibility: Spatial compatibility (SC), which is defined by the length consistency of inliers, is widely applied in point cloud registration. Spectral matching (SM) [66] directly predicts inliers and outliers based on how strongly a correspondence belongs to the main cluster in SC matrix. FGR [67] filters outliers by checking the compatibility between randomly sampled tuples as pre-processing. In CG-SAC [10], the SC is considered as the guidance for efficient sampling of the RANSAC pipeline. SAC-COT [12] further designs a compatibility triangle to benefit the sampling in the early iteration stage. In [68] and [69], a two-stage voting scheme is developed by ranking the geometric consistency in the global and local scope. Besides, the SC is also combined with deep learning techniques in more recent works. PointDSC [9] designs a non-local module taking advantage of the guidance of SC. DHVR [11] generates the hypotheses for deep Hough voting by the SC-validated tuples. TriVoC [70] reformulates the SC-based inlier voting into a deep learning framework. MI-PCR [71] uses the SC to build a learning pipeline for multi-instance point cloud registration. In this article, we try to solve the ambiguity problem when applying SC for efficient sampling.

空間相容性：空間相容性（SC）由內點的長度一致性定義，廣泛應用於點雲配準。譜匹配（SM）[66] 根據一個對應關係在 SC 矩陣中屬於主聚類的強度，直接預測內點和離群點。FGR [67] 透過檢查隨機抽樣元組之間的相容性作為預處理來過濾離群點。在 CG-SAC [10] 中，SC 被視為 RANSAC 流程高效抽樣的指導。SAC-COT [12] 進一步設計了一個相容性三角形，以利於早期迭代階段的抽樣。在 [68] 和 [69] 中，透過對全域和局部範圍內的幾何一致性進行排序，開發了一種兩階段投票方案。此外，在最近的研究中，SC 也與深度學習技術相結合。PointDSC [9] 設計了一個利用 SC 指導的非局部模組。DHVR [11] 透過 SC 驗證的元組為深度霍夫投票生成假設。TriVoC [70] 將基於 SC 的內點投票重新構建為一個深度學習框架。MI-PCR [71] 使用 SC 為多實例點雲配準建立一個學習流程。在本文中，我們試圖解決在應用 SC 進行高效抽樣時的模糊性問題。

## C. Non-Feature-Matching-Based Methods

C. 非基於特徵匹配的方法

In addition to using feature matching, some methods adopt non-feature-matching pipelines to achieve end-to-end registration. The widely used Iterative Closest Point (ICP) algorithm [13] divides point cloud registration into two iterative sub-problems: establishing correspondences by searching the closest neighbor in the coordinate space and solving the rigid transformation by Singular Value Decomposition (SVD). Following the ICP pipeline, some methods try to speed up the searching and convergence process with different strategies, such as designing the point-to-plane error term [72] and adding a probabilistic model to the cost construction [73]. ICP and its variants have a simple form to be applied, but they are sensitive to the initial perturbation. To solve this drawback, some branch-and-bound (BnB) based methods are proposed, such as Go-ICP [74], Gogma [75], Gosma [76]. These methods alleviate the local minimum problem through global optimization, but they are time-consuming and not practical in some extreme scenarios.

除了使用特徵匹配，一些方法採用非特徵匹配的流程來實現端到端的配準。廣泛使用的迭代最近點（Iterative Closest Point, ICP）演算法 [13] 將點雲配準分為兩個迭代的子問題：透過在座標空間中搜索最近鄰來建立對應關係，以及透過奇異值分解（Singular Value Decomposition, SVD）來求解剛性變換。遵循 ICP 的流程，一些方法試圖用不同的策略來加速搜索和收斂過程，例如設計點到平面的誤差項 [72] 和在成本建構中加入機率模型 [73]。ICP 及其變體形式簡單易用，但對初始擾動很敏感。為了解決這個缺點，一些基於分支定界（branch-and-bound, BnB）的方法被提出，例如 Go-ICP [74]、Gogma [75]、Gosma [76]。這些方法透過全域優化來緩解局部最小值問題，但它們非常耗時，在某些極端場景下不實用。

Recently, some researchers also try to develop learning-based end-to-end registration frameworks. PointNetLK [77] modifies the Lucas & Kanade (LK) algorithm as a recurrent neural network and combines it with the PointNet [35] to achieve endto-end registration. DCP [78] utilizes an attention-based module to learn the soft correspondences, and performs weighted SVD to get the rigid transformation. Following them, some methods integrate the optimal transport algorithm [79], graph matching framework [80] or Gaussian Mixture Model [81] to better learn the soft correspondence matrix. PR-Net [82] and OM-Net [83] try to solve the partial-to-partial problem by predicting key points and overlapping masks. RegTR [84] uses attention mechanisms to replace the role of explicit feature matching and RANSAC to directly predict the final set of correspondences.

最近，一些研究人員也試圖開發基於學習的端到端配準框架。PointNetLK [77] 將 Lucas & Kanade (LK) 演算法修改為循環神經網路，並將其與 PointNet [35] 結合以實現端到端配準。DCP [78] 利用基於注意力的模組來學習軟對應，並執行加權 SVD 以獲得剛性變換。繼他們之後，一些方法整合了最優傳輸演算法 [79]、圖匹配框架 [80] 或高斯混合模型 [81] 以更好地學習軟對應矩陣。PR-Net [82] 和 OM-Net [83] 試圖透過預測關鍵點和重疊遮罩來解決部分對部分的配準問題。RegTR [84] 使用注意力機制來取代顯式特徵匹配和 RANSAC 的作用，直接預測最終的對應集。

[Image]

[圖像]

Fig. 2. The voxel grid based point cloud down-sampling. dv is the length of a voxel grid, and dr is the size of the voxelized scene. The red points in the right part are the retained points after down-sampling.

圖 2. 基於體素網格的點雲降採樣。dv 是一個體素網格的長度，dr 是體素化場景的大小。右側的紅點是降採樣後保留的點。

# III. BACKGROUND
 三、背景

## A. Problem Definition

A. 問題定義

Given two point clouds to be aligned, we first build voxel grids to downsample the point clouds, as shown in Fig. 2, and the downsampled point clouds are denoted as source point cloud X ∈ RN ×3 and target point cloud Y ∈ RM ×3. Then, we use either hand-crafted or deep learning-based descriptors to extract feature descriptors for both of them, i.e. FX ∈ RN ×D and F Y ∈ RM ×D . The downsampled point clouds with the extracted feature descriptors are taken as input to the proposed SC2- PCR++. In our method, we first generate the correspondences by performing feature matching on the feature descriptors. Then, the proposed method finds the correct correspondences and estimates the rigid transformation between the two point clouds, i.e., the rotation matrices (R ∈ R3×3) and the translation vectors (t ∈ R3).

給定兩個待對齊的點雲，我們首先建立體素網格來對點雲進行降採樣，如圖 2 所示，降採樣後的點雲分別表示為源點雲 X ∈ RN×3 和目標點雲 Y ∈ RM×3。然後，我們使用手工製作或基於深度學習的描述子為它們提取特徵描述子，即 FX ∈ RN×D 和 FY ∈ RM×D。降採樣後的點雲及其提取的特徵描述子將作為我們提出的 SC2-PCR++ 的輸入。在我們的方法中，我們首先透過對特徵描述子進行特徵匹配來生成對應關係。然後，所提出的方法找到正確的對應關係，並估計兩個點雲之間的剛性變換，即旋轉矩陣 (R ∈ R3×3) 和平移向量 (t ∈ R3)。

## B. Overview

B. 總覽

Our method can be considered as a variant of RANSAC, so we first briefly review the RANSAC and then clarify the difference between our method and RANSAC. RANSAC adopts the generation-and-selection pipeline for robust model fitting. In the generation step, it randomly samples the minimal set to generate massive potential rigid transformations for the two point clouds. In the selection step, it computes the inlier count (IC) metric for each estimation and returns the estimation with the highest IC as the final result. In the proposed SC2-PCR++, the random sampling step is replaced by the second-order spatial compatibility (SC2) measure guided sampling to improve the robustness and efficiency of the model generation, and the IC metric is replaced by the proposed Feature and Spatial consistency constrained Truncated Chamfer Distance metric (FS-TCD).

我們的方法可以被視為 RANSAC 的一個變體，因此我們先簡要回顧 RANSAC，然後闡明我們的方法與 RANSAC 之間的差異。RANSAC 採用「生成與選擇」的流程來進行穩健的模型擬合。在生成步驟中，它隨機抽樣最小集合以生成兩個點雲的大量潛在剛性變換。在選擇步驟中，它為每個估計計算內點計數 (IC) 指標，並返回具有最高 IC 的估計作為最終結果。在我們提出的 SC2-PCR++ 中，隨機抽樣步驟被二階空間相容性 (SC2) 度量引導的抽樣所取代，以提高模型生成的穩健性和效率，而 IC 指標則被我們提出的特徵與空間一致性約束的截斷 Chamfer 距離指標 (FS-TCD) 所取代。

The remainder of this paper is organized as follows: In Section IV, we review the Spatial Compatibility (SC) measure and introduce the proposed SC2 measure. Then, we compare the robustness of using them for model generation from a probabilistic view. In Section V, we describe the proposed FS-TCD. After that, we illustrate the whole pipeline of the SC2-PCR++ in Section VI. Finally, we conduct extensive experiments to validate the performance in Section VII, followed by concluding remarks in Section VIII.

本文其餘部分的組織如下：在第四節中，我們回顧了空間相容性（SC）度量並介紹了我們提出的 SC2 度量。然後，我們從機率的角度比較了它們在模型生成中的穩健性。在第五節中，我們描述了所提出的 FS-TCD。之後，我們在第六節中說明了 SC2-PCR++ 的整個流程。最後，我們在第七節中進行了廣泛的實驗來驗證其性能，並在第八節中給出結論。

# IV. SECOND-ORDER SPATIAL COMPATIBILITY

四、二階空間相容性

In this section, we first briefly review the commonly used Spatial Compatibility (SC) measure [9], [10], [11], [66]. Then, we introduce the new SC2 approach proposed in this article. It is designed for measuring the similarity of correspondences to improve the quality of sampling. We consider that the robustness of a measure-based sampling method can be reflected in the probability of an ambiguity event. Here, the probability of ambiguity event is defined as:

在本節中，我們首先簡要回顧常用的空間相容性 (SC) 度量 [9], [10], [11], [66]。然後，我們介紹本文提出的新 SC2 方法。它旨在測量對應關係的相似性，以提高採樣品質。我們認為，基於度量的採樣方法的穩健性可以透過模糊事件的機率來反映。此處，模糊事件的機率定義為：

Pam(M ) = P(Min,out > Min,in), (1)

Pam(M ) = P(Min,out > Min,in), (1)

where M is a specific metric for measuring correspondence-wise similarity. P(Z) is the probability of an event Z (For convenience we use this notation in the following part). Min,out is the similarity between an inlier and an outlier, while Min,in is the similarity between two inliers. When Min,out > Min,in, the outlier is a closer neighbor compared to the inlier, and the sampling in this case tends to fail. For example, as shown in Fig. 1, c1 is an inlier. When we use the SC measure as the guidance for sampling, we try to find the correspondences with higher similarity with c1 in Fig. 1(b). However, c7, which is an outlier, has higher similarity with c1 than other inliers, so it will also be selected to estimate the rigid transformation with c1. In this case, although c1 is an inlier, using it and its neighbors can not lead to a correct estimation. Instead, for the SC2 matrix in Fig. 1(c), the similarity between c1 and other inliers is higher than outliers, which makes the near neighbors of c1 all inliers in SC2 measure. So using c1 with its neighbors in the SC2 matrix can result in a correct estimation. Thus, the lower the probability in (1) is, the method will be more robust to noises and the metric-based sampling will be more effective. Next, we will compare the ambiguity probability of the SC and SC2 measure on 3DMatch dataset as an example.

其中 M 是用於測量對應相似度的特定度量。P(Z) 是事件 Z 的機率（為方便起見，我們在下文中使用此符號）。Min,out 是一個內點和一個離群點之間的相似度，而 Min,in 是兩個內點之間的相似度。當 Min,out > Min,in 時，離群點相對於內點是一個更近的鄰居，這種情況下的採樣往往會失敗。例如，如圖 1 所示，c1 是一個內點。當我們使用 SC 度量作為採樣指導時，我們試圖在圖 1(b) 中找到與 c1 具有更高相似度的對應。然而，c7 是一個離群點，它與 c1 的相似度高於其他內點，因此它也會被選中與 c1 一起估計剛性變換。在這種情況下，儘管 c1 是一個內點，但使用它及其鄰居無法得到正確的估計。相反，對於圖 1(c) 中的 SC2 矩陣，c1 與其他內點之間的相似度高於離群點，這使得 c1 在 SC2 度量中的近鄰都是內點。因此，在 SC2 矩陣中使用 c1 及其鄰居可以得到正確的估計。因此，(1) 中的機率越低，該方法對雜訊的魯棒性就越強，基於度量的採樣就越有效。接下來，我們將以 3DMatch 資料集為例，比較 SC 和 SC2 度量的模糊機率。

## A. Review of Spatial Compatibility

A. 空間相容性回顧

The Spatial Compatibility (SC) measure between correspondence i and j is defined as follows:

對應 i 和 j 之間的空間相容性 (SC) 度量定義如下：

SCij = φ(dij ), dij =| d(xi, xj ) − d(yi, yj ) |, (2)

SCij = φ(dij ), dij =| d(xi, xj ) − d(yi, yj ) |, (2)

in which (xi, yi) and (xj , yj ) are the matched points of correspondences i and j. φ(·) is a monotonically decreasing kernel function. d(·, ·) is the euclidean distance. As shown in Fig. 1, the distance difference between two inliers din,in should be equal to 0 due to the length consistency of rigid transformation. However, because of the noises introduced by data acquisition and point cloud downsampling (Fig. 2), din,in is not exactly equal to 0, but less than a threshold dthr . Referring to [9], an approximate value of dthr is twice of the voxel size (dv in Fig. 2). For convenience, we assume that din,in is uniformly distributed over dthr and get the probability density function (PDF) of the distance difference between two inliers as follows:

其中 (xi, yi) 和 (xj, yj) 是對應 i 和 j 的匹配點。φ(·) 是一個單調遞減的核心函數。d(·, ·) 是歐幾里得距離。如圖 1 所示，由於剛性變換的長度一致性，兩個內點之間的距離差 din,in 應等於 0。然而，由於資料獲取和點雲降採樣（圖 2）引入的雜訊，din,in 並不完全等於 0，而是小於一個閾值 dthr。參考 [9]，dthr 的近似值是體素大小（圖 2 中的 dv）的兩倍。為方便起見，我們假設 din,in 在 dthr 上均勻分佈，並得到兩個內點之間距離差的機率密度函數 (PDF) 如下：

PDFin,in(l) = 1/dthr , 0 ≤ l ≤ dthr . (3)

PDFin,in(l) = 1/dthr , 0 ≤ l ≤ dthr . (3)

Differently, there is no related constraint between two outliers or an inlier and an outlier due to the random distribution of outliers. We consider the distance difference between two unrelated points to be identically distributed and assume the probability

不同的是，由於離群點的隨機分佈，兩個離群點之間或一個內點和一個離群點之間沒有相關的約束。我們認為兩個不相關點之間的距離差是同分佈的，並假設機率

[Image]

[圖像]

Fig. 3. The empirical probability density function (F ) of the distance difference between two unrelated correspondences, i.e., din,out and dout,out.

圖 3. 兩個不相關對應（即 din,out 和 dout,out）之間距離差的經驗機率密度函數（F）。

density function (PDF) as F (·):

密度函數 (PDF) 為 F(·)：

PDFin,out(l) = F (l), PDFout,out(l) = F (l); 0 ≤ l ≤ dr , (4)

PDFin,out(l) = F (l), PDFout,out(l) = F (l); 0 ≤ l ≤ dr , (4)

where dr is the range of din,out and dout,out. In fact, it is hard to directly express F into a specific formula due to the randomness of the outliers. To make it clear, we plot the empirical F on 3DMatch dataset in Fig. 3. As we can see, the dthr is much smaller than the size of voxelized scene dr . For convenience, we assume that F (l) is in uniform distribution within (0, dthr ) and the value of f0 is the quotient of the integral in (0, dthr ) and dthr . Then, we can get:

其中 dr 是 din,out 和 dout,out 的範圍。事實上，由於離群點的隨機性，很難將 F 直接表示為一個特定的公式。為了清楚起見，我們在圖 3 中繪製了 3DMatch 資料集上的經驗 F。我們可以看到，dthr 遠小於體素化場景的大小 dr。為方便起見，我們假設 F(l) 在 (0, dthr) 內呈均勻分佈，且 f0 的值是 (0, dthr) 內積分與 dthr 的商。然後，我們可以得到：

F (l) = f0, 0 ≤ l ≤ dthr . (5)

F (l) = f0, 0 ≤ l ≤ dthr . (5)

Note that we can also model the F in (0, dthr ) as other forms, and it does not change the conclusion. The reason is that we just have to compute the integral in (0, dthr ), and f0 is obtained by the integral in (0, dthr ). So modeling F (l) as a constant does not change the integral result. Next, we compute the ambiguity probability of SC as (1), i.e., P(SCin,out > SCin,in). According to (2), (3), (4) and (5), it can be computed as follows:

請注意，我們也可以將 (0, dthr) 中的 F 建模為其他形式，這並不會改變結論。原因在於我們只需要計算 (0, dthr) 中的積分，而 f0 是由 (0, dthr) 中的積分得到的。因此，將 F(l) 建模為常數不會改變積分結果。接下來，我們計算 SC 的模糊機率，如 (1) 所示，即 P(SCin,out > SCin,in)。根據 (2)、(3)、(4) 和 (5)，可以計算如下：

P(SCin,out > SCin,in) = P(din,out < din,in) = ∫ dthr 0 ∫ l 0 PDFin,in(l) · PDFin,out(x)dxdl = ∫ dthr 0 ∫ l 0 1 dthr · f0dxdl = dthr · f0 2 . (6)

P(SCin,out > SCin,in) = P(din,out < din,in) = ∫ dthr 0 ∫ l 0 PDFin,in(l) · PDFin,out(x)dxdl = ∫ dthr 0 ∫ l 0 1 dthr · f0dxdl = dthr · f0 2 . (6)

## B. Second-Order Spatial Compatibility

B. 二階空間相容性

Next, we describe the proposed second-order spatial compatibility measure (SC2 ∈ RN ×N ). Specifically, we first build a hard compatibility matrix C (C ∈ RN ×N ):

接下來，我們描述所提出的二階空間相容性度量 (SC2 ∈ RN×N)。具體來說，我們首先建立一個硬相容性矩陣 C (C ∈ RN×N)：

Cij = {1; dij ≤ dthr , 0; dij > dthr . (7)

Cij = {1; dij ≤ dthr , 0; dij > dthr . (7)

C considers that two correspondences satisfying length consistency are compatible (Ci,j is set to 0 when i = j). Then, SC2 ij counts the number of common compatibility correspondences of i and j when they are compatible, as follows:

C 認為滿足長度一致性的兩個對應是相容的（當 i = j 時，Ci,j 設為 0）。然後，SC2 ij 計算共同相容對應的數量當 i 和 j 相容時，它們的共同點如下：

SC2 ij = Cij · N∑ k=1 Cik · Ckj . (8)

Similarly, we analyze the ambiguity probability of SC2, i.e., P(SC2 in,out > SC2 in,in). For convenience, suppose there are N pairs of correspondences and the inlier ratio is α.

同樣地，我們分析 SC2 的模糊機率，即 P(SC2 in,out > SC2 in,in)。為方便起見，假設有 N 對對應，內點率為 α。

Remark 1. The ambiguity probability of SC2 measure, i.e., P(SC2 in,out > SC2 in,in), can be written as follows:

註 1. SC2 度量的模糊機率，即 P(SC2 in,out > SC2 in,in)，可寫為：

P(SC2 in,out > SC2 in,in) = p · P(X > (N · α − 2)), X ∼ S((N α − 1)p + (N (1 − α) − 1)p2, N (1 − α)p2), p = dthr · f0, (9)

P(SC2 in,out > SC2 in,in) = p · P(X > (N · α − 2)), X ∼ S((N α − 1)p + (N (1 − α) − 1)p2, N (1 − α)p2), p = dthr · f0, (9)

where S(·, ·) is the Skellam distribution [85], [86], [87].

其中 S(·, ·) 是 Skellam 分布 [85], [86], [87]。

Derivation of Remark 1: We first reformulate (8) as follows:

註 1 的推導：我們首先將 (8) 重新表述如下：

SC2 ij = Cij · Mij , Mij = N∑ k=1 Cik · Ckj . (10)

SC2 ij = Cij · Mij , Mij = N∑ k=1 Cik · Ckj . (10)

Mij counts the quantity of the commonly compatible correspondences of i and j in the global set. According to (7) and (3), we can obtain that:

Mij 計算了在全域集合中，i 和 j 的共同相容對應的數量。根據 (7) 和 (3)，我們可以得到：

P(Cin,in = 1) = 1. (11)

P(Cin,in = 1) = 1. (11)

According to (7), (4) and (5), we can get that

根據 (7)、(4) 和 (5)，我們可以得到

P(Cin,out = 1) = ∫ dthr 0 F (l)dl = dthr · f0 = p. (12)

P(Cin,out = 1) = ∫ dthr 0 F (l)dl = dthr · f0 = p. (12)

P(Cout,out = 1) = ∫ dthr 0 F (l)dl = dthr · f0 = p. (13)

P(Cout,out = 1) = ∫ dthr 0 F (l)dl = dthr · f0 = p. (13)

According to (10), to make SC2 in,out > SC2 in,in hold, two conditions need to be met: Cin,out = 1 and Min,out > Min,in. According to (12), we can obtain the following equation:

根據 (10)，要使 SC2 in,out > SC2 in,in 成立，需要滿足兩個條件：Cin,out = 1 和 Min,out > Min,in。根據 (12)，我們可以得到以下方程式：

P(SC2 in,out > SC2 in,in) = P(Cin,out = 1) · P(Min,out > Min,in) = p · P(Min,out > Min,in). (14)

P(SC2 in,out > SC2 in,in) = P(Cin,out = 1) · P(Min,out > Min,in) = p · P(Min,out > Min,in). (14)

Next, we compute the distribution of Min,out and Min,in. Since inliers have different distributions from outliers, we compute them separately and reformulate (10) as follows:

接下來，我們計算 Min,out 和 Min,in 的分佈。由於內點和離群點的分佈不同，我們分開計算它們，並將 (10) 重新表述如下：

Mij = ∑ m∈I Cim · Cmj + ∑ n∈O Cin · Cnj , (15)

Mij = ∑ m∈I Cim · Cmj + ∑ n∈O Cin · Cnj , (15)

where I is the inlier set while O is the outlier set. (For convenience we use this notation in the following part).

其中 I 是內點集，而 O 是離群點集。（為方便起見，我們在下文中使用此符號）。

We first discuss the value in M matrix between two inliers, i.e. Min,in. According to (11), we can find that any two inliers are compatible. Thus, when correspondence i and j are inliers, the number of correspondences compatible with both of them in the inlier set is the number of inliers excluding themselves (Cii = 0, Cjj = 0), i.e.:

我們首先討論 M 矩陣中兩個內點之間的值，即 Min,in。根據 (11)，我們可以發現任何兩個內點都是相容的。因此，當對應 i 和 j 都是內點時，在內點集中與它們兩者都相容的對應數量，就是排除它們自身之外的內點數量（Cii = 0, Cjj = 0），即：

∑ m∈I Cim · Cmj = N · α − 2; i ∈ I, j ∈ I, (16)

∑ m∈I Cim · Cmj = N · α − 2; i ∈ I, j ∈ I, (16)

where α is the inlier rate. For outliers, according to (12), the probability that an outlier is compatible with an inlier is p. Then the probability that an outlier is compatible with both i and j is p2. The number of outliers in the whole correspondence set is N (1 − α). So the number of correspondences compatible with both of them in the outlier set is in a Bernoulli distribution [88] as follows:

其中 α 是內點率。對於離群點，根據 (12)，一個離群點與一個內點相容的機率是 p。那麼一個離群點與 i 和 j 都相容的機率是 p2。整個對應集中的離群點數量是 N(1 - α)。因此，在離群點集中與它們兩者都相容的對應數量服從伯努利分佈 [88]，如下所示：

∑ n∈O Cin · Cnj ∼ B(N (1 − α), p2); i ∈ I, j ∈ I, (17)

∑ n∈O Cin · Cnj ∼ B(N (1 − α), p2); i ∈ I, j ∈ I, (17)

where B(·, ·) is the Bernoulli distribution. Thus, Min,in is in the following distribution:

其中 B(·, ·) 是伯努利分佈。因此，Min,in 服從以下分佈：

Min,in ∼ N · α − 2 + B(N (1 − α), p2). (18)

Min,in ∼ N · α − 2 + B(N (1 − α), p2). (18)

After that, we discuss the distribution of the value in M matrix between an inlier and an outlier, i.e., Min,out. For convenience, we assume correspondence i is an inlier while j is an outlier. For the inlier set except correspondence i (Cii = 0), any of them is compatible with i (11), and the probability that one of them is compatible with j is p (12). So the number of correspondences compatible with both correspondence i and j in the inlier set is in the following distribution:

之後，我們討論 M 矩陣中一個內點和一個離群點之間值的分布，即 Min,out。為方便起見，我們假設對應 i 是一個內點，而 j 是一個離群點。對於除了對應 i 之外的內點集（Cii = 0），它們中的任何一個都與 i 相容（11），而其中一個與 j 相容的機率是 p（12）。因此，在內點集中同時與對應 i 和 j 相容的對應數量服從以下分布：

∑ m∈I Cim · Cmj ∼ B(N α − 1, p); i ∈ I, j ∈ O. (19)

∑ m∈I Cim · Cmj ∼ B(N α − 1, p); i ∈ I, j ∈ O. (19)

Meanwhile, for each outlier except correspondence j (Cjj = 0), the probability that it is compatible with i or j are both p according to (12) and (13). So the probability that an outlier is both compatible with i and j is p2. Thus, we can get the following distribution:

同時，對於除了對應 j（Cjj = 0）之外的每個離群點，根據（12）和（13），它與 i 或 j 相容的機率都是 p。因此，一個離群點同時與 i 和 j 相容的機率是 p2。因此，我們可以得到以下分佈：

∑ n∈O Cin · Cnj ∼ B(N (1 − α) − 1, p2); i ∈ I, j ∈ O. (20)

∑ n∈O Cin · Cnj ∼ B(N (1 − α) − 1, p2); i ∈ I, j ∈ O. (20)

So the distribution of Min,out is as follows:

所以 Min,out 的分佈如下：

Min,out ∼ B(N α − 1, p) + B(N (1 − α) − 1, p2). (21)

Min,out ∼ B(N α − 1, p) + B(N (1 − α) − 1, p2). (21)

Since p is a small value, the Binomial distribution in (18) and (21) can be approximately equivalent to the Poisson distribution [88], i.e.:

由於 p 是一個很小的值，(18) 和 (21) 中的二項分佈可以近似等價於卜瓦松分佈 [88]，即：

Min,in ∼ N · α − 2 + π(N (1 − α)p2), Min,out ∼ π((N α − 1)p) + π((N (1 − α) − 1)p2), (22)

Min,in ∼ N · α − 2 + π(N (1 − α)p2), Min,out ∼ π((N α − 1)p) + π((N (1 − α) − 1)p2), (22)

where π(·) is the Poisson distribution. Furthermore, for two Poisson distribution: X1 ∼ π(λ1) and X2 ∼ π(λ2), their sum is also in the Poisson distribution [88] as follows:

其中 π(·) 是卜瓦松分佈。此外，對於兩個卜瓦松分佈：X1 ∼ π(λ1) 和 X2 ∼ π(λ2)，它們的和也服從卜瓦松分佈 [88]，如下所示：

X1 + X2 ∼ π(λ1 + λ2). (23)

X1 + X2 ∼ π(λ1 + λ2). (23)

So we can convert Min,out in (22) into following form:

所以我們可以將 (22) 中的 Min,out 轉換為以下形式：

Min,out ∼ π((N α − 1)p + (N (1 − α) − 1)p2). (24)

Min,out ∼ π((N α − 1)p + (N (1 − α) − 1)p2). (24)

Meanwhile, we can convert P(Min,out > Min,in) into following form:

同時，我們可以將 P(Min,out > Min,in) 轉換為以下形式：

P(Min,out > Min,in) = P(Min,out − Min,in > 0) = P(X > N · α − 2), (25)

P(Min,out > Min,in) = P(Min,out − Min,in > 0) = P(X > N · α − 2), (25)

[Image]

[圖像]

Fig. 4. The probability of ambiguity event. SC is spatial consistency measure. SC2-N (N = 5000, 2500, 1000) is the second-order spatial consistency measure with N correspondences.

圖 4. 模糊事件的機率。SC 是空間一致性度量。SC2-N (N = 5000, 2500, 1000) 是具有 N 個對應的二階空間一致性度量。

where X is in the following distribution:

其中 X 服從以下分佈：

π((N α − 1)p + (N (1 − α) − 1)p2) − π(N (1 − α)p2). (26)

π((N α − 1)p + (N (1 − α) − 1)p2) − π(N (1 − α)p2). (26)

For two Poisson distribution: X1 ∼ π(λ1) and X2 ∼ π(λ2), their difference is in the Skellam distribution [85], [86], [87], i.e.:

對於兩個卜瓦松分佈：X1 ∼ π(λ1) 和 X2 ∼ π(λ2)，它們的差服從 Skellam 分佈 [85], [86], [87]，即：

X1 − X2 ∼ S(λ1, λ2). (27)

X1 − X2 ∼ S(λ1, λ2). (27)

So the distribution of X in (26) can be converted as follows:

因此，(26) 中 X 的分佈可以轉換如下：

S((N α − 1)p + (N (1 − α) − 1)p2, N (1 − α)p2). (28)

S((N α − 1)p + (N (1 − α) − 1)p2, N (1 − α)p2). (28)

Combining (14), (25) and (28), we compute the value of P(SC2 in,out > SC2 in,in) as (9).

結合 (14)、(25) 和 (28)，我們計算 P(SC2 in,out > SC2 in,in) 的值如 (9)。

## C. Ambiguity Probability Comparison

C. 模糊機率比較

As derived above, the ambiguity probabilities of SC and SC2 measures are shown in (6) and (9) respectively. The ambiguity probability of the SC measure is a constant independent of the inlier rate α. Take the 3DMatch [29] dataset as an example. Following [9], we set dthr = 2 · dv = 10 cm, then the ambiguity probability of SC measure is about 0.1 according to (6). Considering the number of outliers might be large, the impact of wrong correspondences is not negligible even at this probability. Differently, according to (9), the ambiguity probability of SC2 measure is related to the inlier rate α and the correspondence number N . According to the properties of Skellam distribution, the value of P(SC2 in,out > SC2 in,in) is going to approach 0 very quickly as α increases.

如上所述，SC 和 SC2 度量的模糊機率分別如 (6) 和 (9) 所示。SC 度量的模糊機率是一個與內點率 α 無關的常數。以 3DMatch [29] 資料集為例。遵循 [9]，我們設定 dthr = 2 · dv = 10 公分，那麼根據 (6)，SC 度量的模糊機率約為 0.1。考慮到離群點的數量可能很大，即使在這樣的機率下，錯誤對應的影響也不可忽略。不同的是，根據 (9)，SC2 度量的模糊機率與內點率 α 和對應數量 N 有關。根據 Skellam 分佈的性質，隨著 α 的增加，P(SC2 in,out > SC2 in,in) 的值將非常迅速地趨近於 0。

In order to make a clearer comparison between the proposed SC2 measure and the previous SC measure, we plot the curves of ambiguity probability with respect to the inlier rate α for both of them according to (6) and (9) on 3DMatch benchmark. Since the ambiguity probability of SC2 measure is related to the correspondence number N , we plot its curves with different N . For the value of Skellam distribution, we use the Scipy library [89], which is a math kit, to compute it. As shown in Fig. 4, the ambiguity probability of the proposed SC2 measure is significantly lower than the SC measure, even when the inlier rate is close to 0. It shows that using SC2 measure as guidance for sampling is easier to obtain an outlier-free set. When the inliers rate reaches 1%, the ambiguity probability of SC2 measure is

為了更清楚地比較我們提出的 SC2 度量和先前的 SC 度量，我們根據 (6) 和 (9) 在 3DMatch 基準上繪製了兩者關於內點率 α 的模糊機率曲線。由於 SC2 度量的模糊機率與對應數 N 有關，我們繪製了不同 N 下的曲線。對於 Skellam 分佈的值，我們使用數學工具包 Scipy 函式庫 [89] 來計算。如圖 4 所示，即使內點率接近 0，我們提出的 SC2 度量的模糊機率也顯著低於 SC 度量。這表明使用 SC2 度量作為採樣指導更容易獲得無離群點的集合。當內點率達到 1% 時，SC2 度量的模糊機率為

[Image]

[圖像]

Fig. 5. (a) The putative correspondences of a pair of point clouds to be aligned, in which the inlier rate is low. (b) The wrong model selected by the IC metric. (c) The correctly estimated model. For IC, TCD and FS-TCD, higher is better. For CD, lower is better. The IC, CD, and TCD metrics can not instruct the correct model. Using the proposed FS-TCD metric can select the correct model.

圖 5. (a) 一對待對齊點雲的假定對應，其中內點率很低。(b) 由 IC 度量選擇的錯誤模型。(c) 正確估計的模型。對於 IC、TCD 和 FS-TCD，值越高越好。對於 CD，值越低越好。IC、CD 和 TCD 度量無法指示正確的模型。使用所提出的 FS-TCD 度量可以選擇正確的模型。

close to 0, which ensures a robust sampling on the data with a low inlier rate.

接近 0，這確保了在低內點率資料上的穩健採樣。

# V. CONSTRAINED TRUNCATED CHAMFER DISTANCE

五、約束截斷倒角距離

## A. Challenges of Model Selection

A. 模型選擇的挑戰

In RANSAC, after generating some hypotheses, the inlier count (IC) is adopted as the metric for selecting the best estimation among them. The inlier count is computed based on a set of putative correspondences. More concretely, suppose there are two point clouds with pre-computed feature descriptors to be aligned: source point cloud X ∈ RN ×3 and target point cloud Y ∈ RM ×3. Formally, it first forms N pairs of correspondences by finding the nearest neighbor for each source point among target points. Then, the inlier count (IC) for evaluating k-th hypothesis (rotation Rk and translation tk) is defined as follows:

在 RANSAC 中，生成一些假設後，採用內點計數 (IC) 作為度量標準，從中選出最佳估計。內點計數是基於一組假定的對應關係計算的。更具體地說，假設有兩個帶有預先計算的特徵描述子的點雲需要對齊：源點雲 X ∈ RN×3 和目標點雲 Y ∈ RM×3。形式上，它首先為每個源點在目標點中找到最近的鄰居，從而形成 N 對對應關係。然後，用於評估第 k 個假設（旋轉 Rk 和平移 tk）的內點計數 (IC) 定義如下：

ICk = N∑ i=1 [‖Rkxi + tk − yi‖ < τ ], (29)

ICk = N∑ i=1 [‖Rkxi + tk − yi‖ < τ ], (29)

where N is the number of putative correspondences. (xi, yi) is a pair of correspondence. [·] is the Iverson bracket, i.e., if the condition in [] is true, then it returns 1, and otherwise returns 0. Once Rk and tk are correctly estimated, the ICk should be close to the number of inliers. However, when there are only rare inliers in the putative correspondences, the IC value of the correct model is also small. An example of 3DLoMatch dataset is shown in Fig. 5. Fig. 5(a) shows a pair of point clouds to be aligned, with the putative correspondences. The inlier rate and inlier number in the putative correspondences are 0.027 and 77 respectively. Fig. 5(b) is the final model selected by the IC metric, and Fig. 5(c) is one of the correctly estimated models. Since the inlier number is only 77, the IC value of the correct model is75, while IC value of the wrong model is 89 by chance. In this case, the best hypothesis can not be selected using the IC metric, leading to the failure estimation of point cloud registration.

其中 N 是假定對應的數量。(xi, yi) 是一對對應。[·] 是 Iverson 括號，即如果 [] 中的條件為真，則返回 1，否則返回 0。一旦 Rk 和 tk 被正確估計，ICk 應該接近內點的數量。然而，當假定對應中只有很少的內點時，正確模型的 IC 值也很小。圖 5 顯示了 3DLoMatch 資料集的一個例子。圖 5(a) 顯示了一對待對齊的點雲及其假定對應。假定對應中的內點率和內點數分別為 0.027 和 77。圖 5(b) 是由 IC 度量選擇的最終模型，圖 5(c) 是其中一個正確估計的模型。由於內點數只有 77，正確模型的 IC 值為75，而錯誤模型的 IC 值偶然為 89。在這種情況下，無法使用 IC 度量選擇最佳假設，導致點雲配準估計失敗。

The drawback of the IC metric is the dependence on putative correspondences and the lack of global alignment information. In the IC metric, the correspondences are counted independently without considering the spatial position relationship between them. In fact, if the rigid transformation between two point clouds is correctly estimated, then a continuous area should be aligned, which means it should be measured by a non-local metric. Therefore, we expect to introduce the Chamfer distance (CD) [14], [15], [16] metric to address the issue of IC metric. We first review the definition of CD metric. Generally, the one-way CD metric for Rk and tk in point cloud registration is defined as follows:

IC 度量的缺點在於其對假定對應的依賴性以及缺乏全域對齊資訊。在 IC 度量中，對應是獨立計算的，沒有考慮它們之間的空間位置關係。事實上，如果兩個點雲之間的剛性變換被正確估計，那麼一個連續的區域應該被對齊，這意味著它應該由一個非局部度量來衡量。因此，我們期望引入倒角距離 (CD) [14], [15], [16] 度量來解決 IC 度量的問題。我們首先回顧 CD 度量的定義。通常，點雲配準中 Rk 和 tk 的單向 CD 度量定義如下：

CDk = N∑ i=1 min yj ∈Y ‖Rkxi + tk − yj ‖. (30)

For each source point, it finds the nearest neighbor in the target point set after aligning by Rk and tk, and computes the distance between them. A smaller CD value measures a better alignment quality of the two point clouds after aligning. However, it is not suitable to directly use the CD metric for selecting the best hypothesis from the following three aspects: 1) For each source point, CD needs to search nearest neighbor in the whole set of target points. Considering that thousands of hypotheses could be generated and the selection is performed on them, computing CD for them is time-consuming. 2) CD metric assumes the two point clouds have a similar shape after aligning. However, when the two point clouds have low-overlapped areas, this assumption is not valid. 3) Since the CD metric does not consider the feature information, the nearest neighbor in the coordinate space could be a wrong alignment. This causes the CD to be sensitive to noises. As shown in Fig. 5, the wrong model in Fig. 5(b) achieves a lower CD metric than the correct model in Fig. 5(c). This also reveals the problem of using CD as the selection metric.

對於每個源點，它在經過 Rk 和 tk 對齊後的目標點集中找到最近的鄰居，並計算它們之間的距離。較小的 CD 值表示對齊後兩個點雲的對齊品質較好。然而，直接使用 CD 度量來選擇最佳假設並不合適，原因有三：1) 對於每個源點，CD 需要在整個目標點集中搜索最近鄰。考慮到可能會生成數千個假設並對其進行選擇，為它們計算 CD 非常耗時。2) CD 度量假設兩個點雲在對齊後具有相似的形狀。然而，當兩個點雲的重疊區域很小時，這個假設不成立。3) 由於 CD 度量不考慮特徵資訊，座標空間中的最近鄰可能是一個錯誤的對齊。這導致 CD 對雜訊很敏感。如圖 5 所示，圖 5(b) 中的錯誤模型比圖 5(c) 中的正確模型獲得了更低的 CD 度量。這也揭示了使用 CD 作為選擇度量的問題。

## B. Constrained Truncated Chamfer Distance

B. 約束截斷倒角距離

Based on the above observations, we propose a Feature and Spatial consistency constrained Truncated Chamfer Distance metric, named FS-TCD, to address both the problems of CD and IC. Specifically, we first reformulate the CD metric as a truncated form (TCD):

基於以上觀察，我們提出了一種名為 FS-TCD 的特徵與空間一致性約束截斷倒角距離度量，以解決 CD 和 IC 的問題。具體來說，我們首先將 CD 度量重新表述為截斷形式 (TCD)：

TCDk = N∑ i=1 [(min yj ∈Y ‖Rkxi + tk − yj ‖) < η], (31)

TCDk = N∑ i=1 [(min yj ∈Y ‖Rkxi + tk − yj ‖) < η], (31)

where [·] is also an Iverson bracket. For each xi, it finds the nearest neighbor among the target points after being aligned by the Rk and tk, and computes the number of neighbor pairs whose alignment error is less than the threshold η. The reason for threshold truncation is that the shapes of the two point clouds are not exactly the same, so they cannot be perfectly aligned. Therefore, it is meaningless to calculate the alignment error between point clouds that are not in the overlapping region. Intuitively, TCD reflects the size of the overlap area between the two clouds after alignment, which measures the global quality of alignment. The greater the overlap between the two point clouds, the more likely they are to be aligned correctly.

其中 [·] 也是一個 Iverson 括號。對於每個 xi，它在經過 Rk 和 tk 對齊後，在目標點中找到最近的鄰居，並計算對齊誤差小於閾值 η 的鄰居對數量。進行閾值截斷的原因是兩個點雲的形狀不完全相同，因此無法完美對齊。因此，計算非重疊區域點雲之間的對齊誤差是沒有意義的。直觀地說，TCD 反映了對齊後兩個雲之間重疊區域的大小，這衡量了對齊的全域品質。兩個點雲之間的重疊越大，它們被正確對齊的可能性就越大。

In TCD, when Rk and tk are incorrectly estimated, it is still possible that the alignment area is miscounted because some points are incorrectly aligned together by chance. In this case, it is likely to compute an abnormally big value of TCD for the wrong model, and select it as the final result. As shown in Fig. 5, the wrong model in Fig. 5(b) has a bigger TCD value than the correct model in Fig. 5(c). To suppress this situation, we introduce two constraints on the TCD. The first one is the feature matching constraint. Specifically, we first build a relaxed hard matching matrix H ∈ RN ×M between the source point cloud and target point cloud by the feature descriptor information. We adopt a simple top-K strategy for building H matrix: if yj is a top-K neighbor of xi in feature space, then Hij = 1. Otherwise, Hij = 0. Note that each source point xi can be matched with K points in target points, so H matrix represents a relaxed feature matching relationship. The H matrix is utilized as the feature constraint to build the F-TCD as follows:

在 TCD 中，當 Rk 和 tk 估計不正確時，仍有可能因為一些點偶然地被錯誤地對齊在一起而導致對齊區域被錯誤計算。在這種情況下，很可能會為錯誤的模型計算出一個異常大的 TCD 值，並將其選為最終結果。如圖 5 所示，圖 5(b) 中的錯誤模型比圖 5(c) 中的正確模型具有更大的 TCD 值。為了抑制這種情況，我們對 TCD 引入了兩個約束。第一個是特徵匹配約束。具體來說，我們首先利用特徵描述子資訊，在源點雲和目標點雲之間建立一個鬆弛的硬匹配矩陣 H ∈ RN×M。我們採用一個簡單的 top-K 策略來建立 H 矩陣：如果 yj 是 xi 在特徵空間中的 top-K 鄰居，則 Hij = 1。否則，Hij = 0。請注意，每個源點 xi 可以與目標點中的 K 個點匹配，因此 H 矩陣表示一個鬆弛的特徵匹配關係。H 矩陣被用作特徵約束來建立 F-TCD，如下所示：

F-TCDk = N∑ i=1 [( min Hij =1 ‖Rkxi + tk − yj ‖) < η]. (32)

In F-TCD, we search the nearest neighbor for each xi based on the relaxed feature matching relationship represented by the H matrix. If we can find a yj to ensure that the alignment error of (xi, yj ) is less than η, and Hij = 1 holds in the H matrix, then we consider xi and yj are successfully registered. We check all the source points and count the number of correctly registered pairs as F-TCD metric. F-TCD searches the neighbor by the H matrix instead of in the global scope, making it less expensive to calculate. Meanwhile, due to the introducing of the feature information, F-TCD is more robust to noises. Compared with the IC metric, F-TCD reduces the dependence on the inlier rate of putative matching. Feature information is not used to establish one-to-one matches in F-TCD, but to establish relaxed one-tomany matches. In fact, we can see that IC is a special case of F-TCD. When we set K = 1 and η = τ , then F-TCD has the same formulation as IC.

在 F-TCD 中，我們根據 H 矩陣所表示的鬆弛特徵匹配關係，為每個 xi 搜索最近鄰。如果我們能找到一個 yj，確保 (xi, yj) 的對齊誤差小於 η，並且在 H 矩陣中 Hij = 1 成立，那麼我們就認為 xi 和 yj 成功配準。我們檢查所有的源點，並將成功配準的對數作為 F-TCD 度量。F-TCD 透過 H 矩陣而不是在全域範圍內搜索鄰居，使其計算成本更低。同時，由於引入了特徵資訊，F-TCD 對雜訊更具穩健性。與 IC 度量相比，F-TCD 降低了對假定匹配內點率的依賴。在 F-TCD 中，特徵資訊不用於建立一對一的匹配，而是用於建立鬆弛的一對多匹配。事實上，我們可以看到 IC 是 F-TCD 的一個特例。當我們設定 K = 1 且 η = τ 時，F-TCD 的公式與 IC 相同。

We further integrate the spatial consistency constraint into F-TCD to obtain the proposed FS-TCD metric. As mentioned in Section IV-A, the inlier correspondences satisfy the spatial consistency. So we check the established match pairs in F-TCD, and find mutually compatible matches. The number of validated matches is viewed as the FS-TCD value. It further removes the potential mismatches which are possibly considered by the metric. As a result, the FS-TCD value of the model in Fig. 5(c) is greatly bigger than that of the model in Fig. 5(b), which helps to select the best result.

我們進一步將空間一致性約束整合到 F-TCD 中，以獲得所提出的 FS-TCD 度量。如第四節-A 所述，內點對應滿足空間一致性。因此，我們檢查 F-TCD 中已建立的匹配對，並找到相互相容的匹配。已驗證匹配的數量被視為 FS-TCD 值。它進一步移除了可能被該度量考慮的潛在不匹配。結果，圖 5(c) 中模型的 FS-TCD 值遠大於圖 5(b) 中模型的 FS-TCD 值，這有助於選擇最佳結果。

# VI. PIPELINE

六、流程

The pipeline of the proposed method is shown in Fig. 6. It can be divided into two main components: model generation and model selection. Different from the classic RANSAC, we use different matching strategies in model generation and model selection. In the model generation process, for each point in the source point cloud, we find its nearest neighbor in the feature

我們提出的方法的流程如圖 6 所示。它可以分為兩個主要部分：模型生成和模型選擇。與傳統的 RANSAC 不同，我們在模型生成和模型選擇中使用不同的匹配策略。在模型生成過程中，對於源點雲中的每個點，我們在特徵中找到其最近的鄰居


[Image]

[圖像]

Fig. 6. Pipeline of our method. The input is the source and target points with extracted features. The proposed method rebuilds the model generation and model selection process of the classic RANSAC.

圖 6. 我們方法的流程圖。輸入是帶有提取特徵的源點和目標點。所提出的方法重建了經典 RANSAC 的模型生成和模型選擇過程。

space among the target points to form N pairs of putative correspondences. Then, we use the proposed SC2 measure to compute the correspondence-wise similarity matrix, and utilize it to guide the sampling for model generation. In the model selection process, we relax the matching condition, and allow one-to-many matching relationships to avoid failed selections due to the low inlier rate. Next, we will describe the model generation and model selection in detail.

在目標點中形成 N 對假定對應。然後，我們使用提出的 SC2 度量來計算對應相似度矩陣，並利用它來指導模型生成的採樣。在模型選擇過程中，我們放寬了匹配條件，允許一對多的匹配關係，以避免因低內點率而導致的選擇失敗。接下來，我們將詳細描述模型生成和模型選擇。

## A. Model Generation

A. 模型生成

First, we use the proposed SC2 measure to compute the correspondence-wise similarity matrix. Then, we use a spectral matching technique with Non-Maximum Suppression (NMS) to select some reliable correspondences, termed seeds. Next, we propose a two-stage sampling strategy to build a consensus set for each seed. After that, we perform local spectral matching among the consensus set of each seed, and generate an estimation of rigid transformation (rotation R and translation t).

首先，我們使用提出的 SC2 度量來計算對應相似度矩陣。然後，我們使用帶有非極大值抑制 (NMS) 的譜匹配技術來選擇一些可靠的對應，稱為種子。接下來，我們提出一個兩階段採樣策略來為每個種子建立一個共識集。之後，我們在每個種子的共識集中執行局部譜匹配，並生成剛性變換（旋轉 R 和平移 t）的估計。

Reliable Seed Selection: As mentioned in Section IV, there are high similarities between inlier correspondences by the proposed SC2 measure. Then, as long as we find an inlier correspondence, we can construct a consensus set by finding its k nearest neighbors in the metric space. Obviously, traversing all the correspondences must find an inlier, but it is not necessary. We only need to pick some reliable points called seed points to accelerate the registration process. We perform the spectral matching technique [66] to select seed points. Specifically, we first build the similarity matrix for all of the correspondences and normalize the value in the matrix to 0-1, following [66]. Then, following [9], [66], the association of each correspondence with the leading eigenvector is adopted as the confidence for this correspondence. The leading eigenvector is solved by the power iteration algorithm [90]. In order to ensure an even distribution of seed points, the correspondences with local maximum confidence score within its neighborhood of radius R are selected. The number of seed points (Ns) is determined by a proportion of the number of whole correspondences.

可靠種子點選擇：如第四節所述，透過我們提出的 SC2 度量，內點對應之間存在高度相似性。因此，只要我們找到一個內點對應，就可以透過在度量空間中找到其 k 個最近鄰來構建一個共識集。顯然，遍歷所有對應必然能找到一個內點，但這並非必要。我們只需要挑選一些可靠的點，稱為種子點，來加速配準過程。我們執行譜匹配技術 [66] 來選擇種子點。具體來說，我們首先為所有對應建立相似性矩陣，並遵循 [66] 將矩陣中的值歸一化到 0-1。然後，遵循 [9]、[66]，每個對應與主導特徵向量的關聯被用作該對應的信賴度。主導特徵向量透過冪迭代演算法 [90] 求解。為了確保種子點的均勻分佈，我們選擇在其半徑 R 的鄰域內具有局部最大信賴度分數的對應。種子點的數量 (Ns) 由總對應數量的一個比例決定。

Two-Stage Consensus Set Sampling: As some seed points are selected, we extend each of them into a consensus set. We adopt a two-stage selection strategy to perform a coarse-to-fine sampling. In the first stage, we select K1 correspondences for each seed by finding its top-K1 neighbors in the SC2 measure space. As mentioned before, the ambiguity probability P(SC2 in,out > SC2 in,in) is very small. Thus, when a seed is an inlier correspondence, the consensus set also mainly contains inliers. Meanwhile, the similarity expressed by SC2 measure focuses on global information instead of local consistency. Therefore, the neighbors selected in the SC2 measure space are distributed more evenly rather than clustered together, which benefits the estimation of rigid transformation [9].

兩階段共識集抽樣：選定一些種子點後，我們將每個種子點擴展成一個共識集。我們採用兩階段選擇策略來執行由粗到精的抽樣。在第一階段，我們透過在 SC2 度量空間中找到每個種子點的前 K1 個鄰居，為其選擇 K1 個對應。如前所述，模糊機率 P(SC2 in,out > SC2 in,in) 非常小。因此，當一個種子點是內點對應時，其共識集也主要包含內點。同時，由 SC2 度量表示的相似性關注全域資訊而非局部一致性。因此，在 SC2 度量空間中選擇的鄰居分佈更均勻，而不是聚集在一起，這有利於剛性變換的估計 [9]。

The second stage of the sampling operation is adopted to further filter potential outliers in the set obtained in the first stage. The SC2 matrices are reconstructed within each set produced by the first stage instead of the whole set. We select top-K2 (K2 < K1) correspondences of the seed by the newly constructed local SC2 matrices. As shown in Fig. 4, since the higher inlier rate ensures a lower ambiguity probability, the potential outliers can also be further pruned. Note that we only discussed the case that the seed point is an inlier. In fact, when the seed point is an outlier, it can also form a local consistency, especially when there are aggregated false matches in the correspondence set. We encourage these sets to also generate hypotheses and filter them at the final hypothesis selection step (Section VI-B) rather than at the early stage. In this way, we can avoid some correct assumptions being filtered out early.

採樣操作的第二階段用於進一步過濾第一階段獲得的集合中的潛在離群點。SC2 矩陣在第一階段產生的每個集合內重新構建，而不是在整個集合上。我們透過新構建的局部 SC2 矩陣選擇種子的前 K2 個（K2 < K1）對應。如圖 4 所示，由於較高的內點率確保了較低的模糊機率，潛在的離群點也可以被進一步修剪。請注意，我們只討論了種子點是內點的情況。事實上，當種子點是離群點時，它也可以形成局部一致性，特別是當對應集中存在聚集的錯誤匹配時。我們鼓勵這些集合也生成假設，並在最終的假設選擇步驟（第六節-B）中過濾它們，而不是在早期階段。這樣，我們可以避免一些正確的假設被過早地過濾掉。


Local Spectral Matching: In this step, we perform the weighted SVD [91] on the consensus set to generate an estimation of rigid transformation for each seed. Although the previously proposed sampling strategy can obtain outlier-free correspondence set, we find that the weighted SVD achieves better performance than treating all correspondences equally. This may be because the inliers still have different degrees of noise. So correspondences with bigger noises should have smaller weights when estimating rigid transformation. Traditional spectral matching [66] method analyzes the SC matrix to assign a weight for each correspondence, which is affected by ambiguity problem [9]. Since the proposed SC2 measure is more robust against ambiguity, we also replace the SC matrix with the SC2 measure.

局部譜匹配：在此步驟中，我們對共識集執行加權 SVD [91]，為每個種子生成剛性變換的估計。儘管先前提出的採樣策略可以獲得無離群點的對應集，但我們發現加權 SVD 的性能優於平等對待所有對應。這可能是因為內點仍然存在不同程度的雜訊。因此，在估計剛性變換時，雜訊較大的對應應具有較小的權重。傳統的譜匹配 [66] 方法分析 SC 矩陣為每個對應分配權重，但會受到模糊問題 [9] 的影響。由於我們提出的 SC2 度量對模糊性更具穩健性，我們也用 SC2 度量取代了 SC 矩陣。

Specifically, for each consensus set, we build a local graph by considering each correspondence in the consensus set as a node, and the SC2 value between the correspondences as edge. In order to facilitate matrix analysis, we convert the SC2 measure into soft form ( ̃SC2) as follows:

具體來說，對於每個共識集，我們將共識集中的每個對應視為一個節點，並將對應之間的 SC2 值視為邊，從而建立一個局部圖。為了方便矩陣分析，我們將 SC2 度量轉換為軟形式 ( ̃SC2)，如下所示：

̃SC2 = ̃C · ( ̃C × ̃C), ̃Cij = ReLU(1 − d2 ij /d2 thr ), (1 ≤ i ≤ K2, 1 ≤ j ≤ K2) (33)

̃SC2 = ̃C · ( ̃C × ̃C), ̃Cij = ReLU(1 − d2 ij /d2 thr ), (1 ≤ i ≤ K2, 1 ≤ j ≤ K2) (33)

where · is Hadamard product and × is matrix product. The ̃SC2 has a similar property with SC2, i.e., there are higher similarity values between inliers and inliers than that between inliers and outliers. Thus, the inliers in the graph are clustered together. Then we conduct local spectral decomposition on the adjacent matrix of the local graph, i.e., ̃SC2, to obtain a weight wi for correspondence i. According to [9], [66], the leading eigenvector of matrix ̃SC2 can be considered as the association of each correspondence with a main cluster. In the consensus set, when the seed is an inlier, this set mainly contains inliers, so the main cluster is the inlier set. Thus, the association value with the main cluster, i.e. the leading eigenvector can be explained as the inlier probability. Therefore, we use the leading eigenvector of the ̃SC2 as the weight for SVD. In our method, we use the power iteration algorithm [90] to efficiently compute the leading eigenvector. Finally, the rotation Rk and translation tk of seed k are computed by performing weighted SVD [20] within its consensus set.

其中 · 是哈達瑪積，× 是矩陣積。 ̃SC2 具有與 SC2 相似的性質，即內點與內點之間的相似度值高於內點與離群點之間的相似度值。因此，圖中的內點會聚集在一起。然後我們對局部圖的鄰接矩陣，即 ̃SC2，進行局部分解，以獲得對應 i 的權重 wi。根據 [9]、[66]，矩陣 ̃SC2 的主導特徵向量可以被視為每個對應與一個主聚類的關聯。在共識集中，當種子是內點時，該集合主要包含內點，因此主聚類是內點集。因此，與主聚類的關聯值，即主導特徵向量，可以解釋為內點機率。因此，我們使用 ̃SC2 的主導特徵向量作為 SVD 的權重。在我們的方法中，我們使用冪迭代演算法 [90] 來有效地計算主導特徵向量。最後，透過在其共識集內執行加權 SVD [20] 來計算種子 k 的旋轉 Rk 和平移 tk。

## B. Model Selection

B. 模型選擇

After model generation, we select the best estimation over the rigid transformations produced by all the consensus sets. We use the proposed Feature and Spatial consistency constrained Truncated Chamfer Distance (FS-TCD) metric to select the final estimation. As shown in Fig. 6, before obtaining the FS-TCD for each estimation, we first build the relaxed matching between source points and target points, represented as the H matrix, as described in Section V. Compared with Chamfer Distance (CD), FS-TCD is more efficient due to the reduction of the nearest neighbor search area. However, FS-TCD is still more time-consuming than Inlier Count (IC). In order to reduce unnecessary calculations of FS-TCD to accelerate the selection process, we use the IC to remove some spurious solutions in advance.

模型生成後，我們從所有共識集產生的剛性變換中選擇最佳估計。我們使用提出的特徵與空間一致性約束截斷倒角距離 (FS-TCD) 度量來選擇最終估計。如圖 6 所示，在為每個估計獲取 FS-TCD 之前，我們首先如第五節所述，建立源點和目標點之間的鬆弛匹配，表示為 H 矩陣。與倒角距離 (CD) 相比，FS-TCD 由於縮小了最近鄰搜索區域而更有效率。然而，FS-TCD 仍然比內點計數 (IC) 更耗時。為了減少不必要的 FS-TCD 計算以加速選擇過程，我們先使用 IC 來移除一些虛假的解決方案。

More concretely, as described in Section VI-A, we choose Ns seeds with consensus set, resulting in Ns hypotheses. Then, for the estimation of k-th seed Rk and tk, we compute the IC metric, and retain N ′ s estimations with the highest IC scores. After that, the FS-TCD metric is figured for each of the remaining hypotheses, and the R∗ k and t∗ k with the highest FS-TCD is selected as the final result.

更具體地說，如第六節-A 所述，我們選擇 Ns 個帶有共識集的種子，從而產生 Ns 個假設。然後，對於第 k 個種子 Rk 和 tk 的估計，我們計算 IC 度量，並保留具有最高 IC 分數的 N's 個估計。之後，為每個剩餘的假設計算 FS-TCD 度量，並選擇具有最高 FS-TCD 的 R*k 和 t*k 作為最終結果。

# VII. EXPERIMENT

七、實驗

## A. Datasets and Experimental Setup

A. 資料集與實驗設定

Indoor Scenes: We use the 3DMatch benchmark [29] for evaluating the performance on indoor scenes. It contains 1623 pairs of point clouds with ground-truth camera poses, which are obtained by 8 different RGBD sequences. For each pair of point clouds, we set the voxel size dv as 5 cm to downsample the point cloud. Then we extract the local feature descriptors and match them to form the putative correspondences. In order to test the performance of each algorithm more comprehensively, we use FPFH [27] (handcrafted descriptor) and FCGF [30] (learning-based descriptors) as feature descriptors respectively.

室內場景：我們使用 3DMatch 基準 [29] 來評估在室內場景的性能。它包含 1623 對帶有真實相機姿態的點雲，這些點雲是從 8 個不同的 RGBD 序列中獲得的。對於每一對點雲，我們將體素大小 dv 設為 5 公分來對點雲進行降採樣。然後我們提取局部特徵描述子並進行匹配，以形成假定的對應關係。為了更全面地測試每種演算法的性能，我們分別使用 FPFH [27]（手工描述子）和 FCGF [30]（基於學習的描述子）作為特徵描述子。

Partial overlapping is challenging in point cloud registration. In order to further test the performance of our method, 3DLoMatch benchmark [41] is adopted to further verify the performance of the algorithm on low-overlapped point cloud registration. It contains 1781 pairs of point clouds with low overlapping. Following [9], [11], we use FCGF and Predator to generate putative correspondences.

部分重疊在點雲配準中具有挑戰性。為了進一步測試我們方法的性能，我們採用 3DLoMatch 基準 [41] 來進一步驗證演算法在低重疊點雲配準上的性能。它包含 1781 對低重疊的點雲。遵循 [9]、[11]，我們使用 FCGF 和 Predator 來生成假定的對應關係。

Outdoor Scenes: The KITTI dataset [93] is composed of 11 outdoor driving scenarios of point clouds. Following [20], [30], we choose the 8 to 10 scenarios as test datasets. For all the LIDAR scans, we use the first scan that is taken at least 10 cm apart within each sequence to create a pair, which can obtain 555 pairs of point clouds for testing. Then we construct 30 cm voxel grids (dv = 30 cm) to downsample the point cloud and form the putative correspondences by FPFH and FCGF respectively.

戶外場景：KITTI 資料集 [93] 由 11 個戶外駕駛場景的點雲組成。遵循 [20]、[30]，我們選擇 8 到 10 個場景作為測試資料集。對於所有的 LIDAR 掃描，我們使用每個序列中相距至少 10 公分的第一次掃描來創建一對，這樣可以獲得 555 對點雲用於測試。然後我們構建 30 公分的體素網格（dv = 30 公分）來對點雲進行降採樣，並分別透過 FPFH 和 FCGF 形成假定的對應關係。

Multi-Way Registration Dataset: We use the Augmented ICL_NUIM [94], [95] dataset for testing the performance on the multi-way registration task. The dataset contains four scenes of indoor environments: two sequences of living rooms and two sequences of offices. For each pair of point clouds, we also use 5 cm voxel grids (dv = 5 cm) to downsample the point cloud and extract FPFH descriptors.

多路配準資料集：我們使用增強的 ICL_NUIM [94], [95] 資料集來測試多路配準任務的性能。該資料集包含四個室內環境場景：兩個客廳序列和兩個辦公室序列。對於每一對點雲，我們也使用 5 公分的體素網格（dv = 5 公分）來對點雲進行降採樣並提取 FPFH 描述子。

Evaluation Criteria: We first report the registration recall (RR) under an error threshold (the unit of RR is % in the following experiments). For the indoor scenes, the threshold is set to (15 deg, 30 cm), while the threshold for outdoor scenes is (5 deg, 60 cm). For a pair of point clouds to be aligned, we calculate the errors of translation and rotation estimation separately. We compute the isotropic rotation error (RE) [96] and L2 translation error (TE) as follows:

評估標準：我們首先報告在一個誤差閾值下的配準召回率 (RR)（在以下實驗中，RR 的單位是 %）。對於室內場景，閾值設定為 (15 度, 30 公分)，而對於室外場景，閾值為 (5 度, 60 公分)。對於一對待對齊的點雲，我們分別計算平移和旋轉估計的誤差。我們計算各向同性旋轉誤差 (RE) [96] 和 L2 平移誤差 (TE) 如下：

RE = acos ( trace( ˆR−1R) − 1 2 ) , TE = ‖t − ˆt‖2, (34)

RE = acos ( trace( ˆR−1R) − 1 2 ) , TE = ‖t − ˆt‖2, (34)

Authorized licensed use limited to: National Taiwan Univ of Science and Technology. Downloaded on November 14,2025 at 12:39:52 UTC from IEEE Xplore. Restrictions apply.

授權有限使用於：國立台灣科技大學。於 2025 年 11 月 14 日 12:39:52 UTC 從 IEEE Xplore 下載。適用限制。

12368 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 45, NO. 10, OCTOBER 2023

12368 IEEE 模式分析與機器智能彙刊，第 45 卷，第 10 期，2023 年 10 月

TABLE I QUANTITATIVE R ESULTS ON 3DMATCH DATASET. THE METRIC W ITH ↑ MEANS THAT HIGHER IS B ETTER , W HILE A ↓ MEANS THE OPPOSITE . METHODS W ITH * ARE C ORRESPONDENCE-FREE METHODS

表 I 3DMATCH 資料集上的量化結果。帶有 ↑ 的指標表示越高越好，而帶有 ↓ 的指標表示越低越好。帶有 * 的方法是無對應方法。

[Image]

[圖像]

where R and t are ground-truth pose, while ˆR and ˆt are the estimated pose. The units of RE and TE are deg and cm respectively. Meanwhile, following [9], we also report the outlier removal results using the following three evaluation criteria: inlier precision (IP, %), inlier recall (IR, %) and F1-measure (F1, %). For the multi-way registration, following [20], we report the absolute trajectory error (ATE, cm) as the measurement.

其中 R 和 t 是真實姿態，而 ˆR 和 ˆt 是估計的姿態。RE 和 TE 的單位分別是度和公分。同時，遵循 [9]，我們也使用以下三個評估標準報告離群點移除結果：內點精確率 (IP, %)、內點召回率 (IR, %) 和 F1-測量 (F1, %)。對於多路配準，遵循 [20]，我們報告絕對軌跡誤差 (ATE, 公分) 作為測量值。

Implementation Details: When computing the SC2 matrix, the dthr is set to twice as the voxel size for down-sampling (10 cm for indoor scenes and 60 cm for outdoor scenes). The number of seed (Ns in Section VI-A) is set to 0.2 * N , where N is the number of correspondences. When sampling the consensus set, we select 30 nearest neighbors (K1 = 30) of the seed point at the first sampling stage, and remain 20 correspondences (K2 = 20) to form the consensus set. When performing model selection, we use the Inlier Count (IC) to filter some wrong estimations, and remain 50 models to be further selected (N ′ s = 50 in Section VI-B). All the experiments are conducted on a machine with an INTEL Xeon E5-2620 CPU and a single NVIDIA GTX1080Ti.

實作細節：在計算 SC2 矩陣時，dthr 設定為降採樣體素大小的兩倍（室內場景為 10 公分，室外場景為 60 公分）。種子數量（第六節-A 中的 Ns）設定為 0.2 * N，其中 N 是對應數量。在採樣共識集時，我們在第一採樣階段選擇種子點的 30 個最近鄰（K1 = 30），並保留 20 個對應（K2 = 20）以形成共識集。在執行模型選擇時，我們使用內點計數（IC）來過濾一些錯誤的估計，並保留 50 個模型以供進一步選擇（第六節-B 中的 N's = 50）。所有實驗皆在一台配備 INTEL Xeon E5-2620 CPU 和單一 NVIDIA GTX1080Ti 的機器上進行。

## B. Evaluation on Indoor Scenes

B. 室內場景評估

We first report the results on 3DMatch dataset in Table I. We compare our method with 16 baselines: DCP [78], PointNetLK [77], OM-Net [83], RegTR [84], 3DRegNet [19], DGR [20], DHVR [11], PointDSC [9], SM [66], ICP [13], FGR [67], TEASER [92], GC-RANSAC [52], RANSAC [8], CG-SAC [10] and SC2-PCR [21] (the previous version of our method in CVPR2022). The first 8 methods are based on deep learning, while the last 8 methods are geometric. For the deep learning methods, we use the provided pre-trained model of them for testing. DCP, PointNetLK, OM-Net, RegTR, and ICP are correspondence-free methods, so we do not report the correspondence-related metrics for them. For DCP, PointNetLK, OM-Net, and RegTR, they use neural networks for feature extraction, so we only compare them with learning-based descriptor for a fair comparison.

我們首先在表 I 中報告 3DMatch 資料集的結果。我們將我們的方法與 16 個基準方法進行比較：DCP [78]、PointNetLK [77]、OM-Net [83]、RegTR [84]、3DRegNet [19]、DGR [20]、DHVR [11]、PointDSC [9]、SM [66]、ICP [13]、FGR [67]、TEASER [92]、GC-RANSAC [52]、RANSAC [8]、CG-SAC [10] 和 SC2-PCR [21]（我們在 CVPR2022 中的方法的先前版本）。前 8 種方法基於深度學習，而後 8 種方法是幾何方法。對於深度學習方法，我們使用它們提供的預訓練模型進行測試。DCP、PointNetLK、OM-Net、RegTR 和 ICP 是無對應方法，因此我們不報告它們的對應相關指標。對於 DCP、PointNetLK、OM-Net 和 RegTR，它們使用神經網路進行特徵提取，因此我們僅將它們與基於學習的描述子進行比較以求公平。

Combined With FPFH: We first use the FPFH descriptor to generate the correspondences, in which the mean inlier rate is 6.84%. As shown in Table I, the SC2-PCR greatly outperforms all of the other methods. For the registration recall (RR), which is the most important criterion, SC2-PCR improves it by about 6% over the closest competitors among the retested results (PointDSC and CG-SAC). Following [9], [20], since the part of failed registration can generate a large error of translation and rotation, we only compute the mean rotation (RE) and translation error (TE) of successfully registered point cloud pairs of each method to avoid unreliable metrics. This strategy of measurement makes methods with high registration recall more likely to have a large mean error, because they include more difficult data when calculating mean error. Nevertheless, SC2-PCR still achieves competitive results on RE and TE. SC2-PCR is slightly worse than PointDSC on TE and RE, and better than other methods. For the outlier rejection results, SC2-PCR achieves the highest inlier recall (IR) and F1-measure. The F1 of SC2-PCR outperforms the PointDSC by 5.35%.

結合 FPFH：我們首先使用 FPFH 描述子來生成對應，其中平均內點率為 6.84%。如表 I 所示，SC2-PCR 顯著優於所有其他方法。對於最重要的標準——配準召回率 (RR)，SC2-PCR 在重新測試的結果中，比最接近的競爭者 (PointDSC 和 CG-SAC) 提高了約 6%。遵循 [9]、[20]，由於配準失敗的部分會產生較大的平移和旋轉誤差，我們只計算每種方法成功配準的點雲對的平均旋轉 (RE) 和平移誤差 (TE)，以避免不可靠的指標。這種測量策略使得具有高配準召回率的方法更有可能具有較大的平均誤差，因為它們在計算平均誤差時包含了更困難的數據。儘管如此，SC2-PCR 在 RE 和 TE 上仍然取得了具有競爭力的結果。SC2-PCR 在 TE 和 RE 上略遜於 PointDSC，但優於其他方法。在離群點剔除結果方面，SC2-PCR 取得了最高的內點召回率 (IR) 和 F1-measure。SC2-PCR 的 F1 比 PointDSC 高出 5.35%。

Compared with the SC2-PCR, SC2-PCR++ further achieves a significant performance improvement. Since the proposed FSTCD metric can better find the best hypothesis, SC2-PCR++ can largely improve the registration recall (RR) from 83.98% to 87.18%. Meanwhile, the outlier rejection performance of SC2- PCR++ is also better than SC2-PCR, achieving 4.01%, 3.39%, and 3.72% improvement in terms of inlier precision (IP), inlier recall (IR) and F1-measure.

與 SC2-PCR 相比，SC2-PCR++ 進一步取得了顯著的性能提升。由於所提出的 FSTCD 度量能更好地找到最佳假設，SC2-PCR++ 能將配準召回率 (RR) 從 83.98% 大幅提升至 87.18%。同時，SC2-PCR++ 的離群點剔除性能也優於 SC2-PCR，在內點精確率 (IP)、內點召回率 (IR) 和 F1-measure 方面分別提升了 4.01%、3.39% 和 3.72%。

Combined With FCGF: To further verify the performance, we also adopt the recent FCGF descriptor to generate putative correspondences and report the registration results. The mean inlier rate of putative correspondences is 25.61%. As shown in Table I, since the inlier rate is higher than the correspondences obtained by FPFH descriptor, the performances of all of the feature-based methods are boosted. SC2-PCR++ still achieves the best performance over all the methods, with 2.71%

結合 FCGF：為了進一步驗證性能，我們也採用了近期的 FCGF 描述子來生成假定的對應關係並報告配準結果。假定對應的平均內點率為 25.61%。如表 I 所示，由於內點率高於 FPFH 描述子獲得的對應，所有基於特徵的方法的性能都得到了提升。SC2-PCR++ 在所有方法中仍然取得了最佳性能，提升了 2.71%


TABLE II QUANTITATIVE R ESULTS ON 3DL O MATCH DATASET

表 II 3DLoMatch 資料集上的量化結果

[Image]

[圖像]

improvement over RANSAC on registration recall. Compared with SC2-PCR, SC2-PCR++ boosts the registration recall (RR) by 0.87%, and achieves 1.63%, 1.30% and 1.51% improvement in terms of inlier precision (IP), inlier recall (IR) and F1-measure.

在配準召回率上優於 RANSAC。與 SC2-PCR 相比，SC2-PCR++ 將配準召回率 (RR) 提升了 0.87%，並在內點精確率 (IP)、內點召回率 (IR) 和 F1-measure 方面分別取得了 1.63%、1.30% 和 1.51% 的提升。

Besides, the mean registration time for a pair of point clouds is also reported. Since SC2-PCR only needs to sample a few seed points with their consensus set rather than a large number of samples, it is competitive in terms of time-consuming. As shown in Table I, the mean registration time of SC2-PCR is 0.11 s. SC2-PCR++ adds some time overhead due to the more complex hypothesis selection strategy, requiring an average registration time of 0.28 s. Considering the performance improvement, the added time cost is well worth it. Nevertheless, SC2-PCR++ is still over 10× faster than RANSAC with 4 M iterations.

此外，也報告了一對點雲的平均配準時間。由於 SC2-PCR 只需要對少數種子點及其共識集進行採樣，而不是大量的樣本，因此在時間消耗方面具有競爭力。如表 I 所示，SC2-PCR 的平均配準時間為 0.11 秒。由於更複雜的假設選擇策略，SC2-PCR++ 增加了一些時間開銷，平均配準時間為 0.28 秒。考慮到性能的提升，增加的時間成本是值得的。儘管如此，SC2-PCR++ 仍然比進行 4 百萬次迭代的 RANSAC 快 10 倍以上。

Robustness to Lower Overlap: Furthermore, we report the results on the low overlapped scenarios: 3DLoMatch [41]. Following PointDSC [9] and DHVR [11], we adopt the FCGF [30] and Predator [41] descriptors to generate correspondences. There are two versions of Predator. To avoid unnecessary misunderstanding, we specify that the version we used is the updated one. Similarly, the registration recall (RR), rotation error (RE), translation error (TE), inlier precision (IP), inlier recall (IR), and F1-measure (F1) are reported in Table II. As shown by the data, whether combined with FCGF or Predator descriptor, SC2-PCR++ achieves the highest registration recall. Compared with SC2-PCR, SC2-PCR++ improves the RR from 57.83% to 61.15% when combined with FCGF descriptor, and from 69.46% to 71.59% when combined with predator descriptor. The evaluation criteria related to the established correspondences, including IP, IR, and F1, have all been improved to a certain extent.

對較低重疊的穩健性：此外，我們報告了在低重疊場景下的結果：3DLoMatch [41]。遵循 PointDSC [9] 和 DHVR [11]，我們採用 FCGF [30] 和 Predator [41] 描述子來生成對應。Predator 有兩個版本。為避免不必要的誤解，我們在此說明我們使用的是更新後的版本。同樣地，表 II 中報告了配準召回率 (RR)、旋轉誤差 (RE)、平移誤差 (TE)、內點精確率 (IP)、內點召回率 (IR) 和 F1-measure (F1)。數據顯示，無論是結合 FCGF 還是 Predator 描述子，SC2-PCR++ 都達到了最高的配準召回率。與 SC2-PCR 相比，當結合 FCGF 描述子時，SC2-PCR++ 將 RR 從 57.83% 提高到 61.15%；當結合 Predator 描述子時，從 69.46% 提高到 71.59%。與已建立對應相關的評估標準，包括 IP、IR 和 F1，都有一定程度的改善。

GeoTransformer [44], which uses a geometry-based transformer architecture to establish correspondences, achieves

GeoTransformer [44] 使用基於幾何的 transformer 架構來建立對應關係，實現了

[Image]

[圖像]

Fig. 7. Qualitative comparison on 3DMatch and 3DLoMatch dataset. From left to right are: FGR [67], RANSAC [8], PointDSC [9] and Ours.

圖 7. 在 3DMatch 和 3DLoMatch 資料集上的定性比較。從左到右分別是：FGR [67]、RANSAC [8]、PointDSC [9] 和我們的方法。

SOTA performance for the correspondence learning on 3DLoMatch dataset. In order to further validate the proposed method, we also use the GeoTransformer to generate correspondences and compare the registration results of our method with other methods. Since the Geotransformer produces correspondences in a coarse-to-fine method without descriptors, while DGR [20] and FGR [67] require descriptors as input, we do not report their results. LGR [44] is an outlier removal method proposed by GeoTransformer as the post-process. As shown in Table II, when combined with GeoTransformer, the proposed SC2-PCR++ still achieves the best performance.

在 3DLoMatch 資料集上的對應學習達到了最先進的性能。為了進一步驗證所提出的方法，我們也使用 GeoTransformer 來生成對應，並將我們方法的配準結果與其他方法進行比較。由於 Geotransformer 以由粗到精的方式生成對應，無需描述子，而 DGR [20] 和 FGR [67] 需要描述子作為輸入，因此我們不報告它們的結果。LGR [44] 是 GeoTransformer 提出的作為後處理的離群點移除方法。如表 II 所示，當與 GeoTransformer 結合時，所提出的 SC2-PCR++ 仍然取得了最佳性能。

Meanwhile, we also present some qualitative results on 3DLoMatch dataset. As shown in Fig. 7, we compare our method with RANSAC, FGR and PointDSC and report the visualized alignment results of some challenging scenes. Our method can successfully align two point clouds where the low overlap ratio is clearly visible.

同時，我們也展示了在 3DLoMatch 資料集上的一些定性結果。如圖 7 所示，我們將我們的方法與 RANSAC、FGR 和 PointDSC 進行了比較，並報告了一些具挑戰性場景的視覺化對齊結果。我們的方法可以成功地對齊兩個點雲，其中低重疊率清晰可見。

## C. Evaluation on Outdoor Scenes

C. 戶外場景評估

In this experiment, we test on the outdoor KITTI [93] dataset. The results of DHVR [11], DGR [20], PointDSC [9], RANSAC [8], FGR [67], CG-SAC [10] are reported as comparison. DHVR, DGR, and PointDSC are deep learning based methods, while the remaining methods are non-learning. For DHVR, the authors have neither released the training code nor the pretrained model on KITTI dataset, so we report the results provided by their paper. As shown in Table III, the SC2-PCR remarkably surpasses the non-learning methods, especially combined with the FPFH descriptor. The registration recall (RR) of our method is 25.23% higher than that of RANSAC when combined with the FPFH descriptor, and 0.54% higher when combined with the FCGF descriptor. The errors of translation and rotation are also lower than RANSAC. The SC2-PCR and SC2-PCR++ with FPFH descriptor obtain the results with the highest registration recall and lowest error of rotation and translation. This not only proves the flexibility of our method, but also proves the competitiveness of traditional hand-crafted descriptors in some scenarios. For the learning networks, our method can achieve close performance with them with high efficiency.

在本實驗中，我們在戶外 KITTI [93] 資料集上進行測試。我們報告了 DHVR [11]、DGR [20]、PointDSC [9]、RANSAC [8]、FGR [67]、CG-SAC [10] 的結果作為比較。DHVR、DGR 和 PointDSC 是基於深度學習的方法，其餘方法則非學習方法。對於 DHVR，作者既未發布訓練程式碼，也未發布在 KITTI 資料集上的預訓練模型，因此我們報告其論文中提供的結果。如表 III 所示，SC2-PCR 顯著超越了非學習方法，特別是與 FPFH 描述子結合時。與 FPFH 描述子結合時，我們方法的配準召回率 (RR) 比 RANSAC 高 25.23%，與 FCGF 描述子結合時高 0.54%。平移和旋轉的誤差也低於 RANSAC。帶有 FPFH 描述子的 SC2-PCR 和 SC2-PCR++ 獲得了最高的配準召回率以及最低的旋轉和平移誤差。這不僅證明了我們方法的靈活性，也證明了傳統手工描述子在某些場景下的競爭力。對於學習網路，我們的方法能夠以高效率達到與其相近的性能。

表 III KITTI 資料集上的量化結果

[Image]

[圖像]

TABLE IV ABSOLUTE TRAJECTORY ERROR (ATE, C M ) ON THE 4 SCENES OF AUGMENTED ICL-NUIM DATASET W ITH SIMULATED DEPTH NOISES. THE AVERAGE ATE OVER ALL THE SCENES IS R EPORTED IN THE LAST C OLUMN . (L OWER IS B ETTER .)

表 IV 增強 ICL-NUIM 資料集 4 個場景在模擬深度雜訊下的絕對軌跡誤差 (ATE, CM)。最後一欄報告了所有場景的平均 ATE。（越低越好。）

[Image]

[圖像]

## D. Multi-Way Registration

D. 多路配準

In order to further validate the performance of the proposed method, we integrate it into a multi-way registration pipeline and test it on the ICL_NUIM [94], [95] dataset. Following [9], [20], we first extract the FPFH descriptor for each frame, and then use the proposed method to initialize the pose of each frame by pairwise registration. After that, the poses are globally optimized by the graph optimization method (the g2o method [97] implemented in Open3d [98] library is utilized). We also combine other registration methods with the multi-way pipeline, including DGR, PointDSC, DHVR, FGR and RANSAC. Meanwhile, the results of the state-of-the-art online SLAM methods, including ElasticFusion [99], InfiniTAM [100] and BAD-SLAM [101], are also reported as a comparison. As shown in Table IV, we present the Absolute trajectory error (ATE) of each scene and the average result. Since BAD-SLAM can not lead to a successful result, we do not put the average result of it. As we can see, the non-learning methods FGR and RANSAC lead to worse results compared with deep learning based methods. The proposed SC2-PCR and SC2-PCR++ are also non-learning methods, but achieve great performance among all the methods. The SC2-PCR++ achieves the best performance on the Living1 and Office2 scenes and the lowest average ATE over the four test scenes.

為了進一步驗證所提方法的性能，我們將其整合到一個多路配準流程中，並在 ICL_NUIM [94], [95] 資料集上進行測試。遵循 [9], [20] 的做法，我們首先為每一幀提取 FPFH 描述子，然後使用所提方法透過成對配準來初始化每一幀的姿態。之後，透過圖優化方法（使用 Open3d [98] 函式庫中實現的 g2o 方法 [97]）對姿態進行全域優化。我們也將其他配準方法與多路流程相結合，包括 DGR、PointDSC、DHVR、FGR 和 RANSAC。同時，也報告了最先進的線上 SLAM 方法的結果作為比較，包括 ElasticFusion [99]、InfiniTAM [100] 和 BAD-SLAM [101]。如表 IV 所示，我們展示了每個場景的絕對軌跡誤差 (ATE) 和平均結果。由於 BAD-SLAM 無法成功得出結果，我們不列出其平均結果。我們可以看到，與基於深度學習的方法相比，非學習方法 FGR 和 RANSAC 的結果較差。所提出的 SC2-PCR 和 SC2-PCR++ 也是非學習方法，但在所有方法中取得了優異的性能。SC2-PCR++ 在 Living1 和 Office2 場景上取得了最佳性能，並在四個測試場景中取得了最低的平均 ATE。

TABLE V GENERALIZATION R ESULTS . THE R EGISTRATION R ECALL (%) ON 3DMATCH , 3DL O MATCH AND KITTI DATASETS ARE R EPORTED

表 V 泛化結果。報告了在 3DMatch、3DLoMatch 和 KITTI 資料集上的配準召回率 (%)。

[Image]

[圖像]

## E. Generalization and Robustness

E. 泛化與穩健性

Generalization Experiments: As reported above, deep learning based methods also achieve competitive performance on the 3DMatch, 3DLoMatch and KITTI datasets. Compared with these methods based on deep learning, the other advantage of our method is that it has no bias cross different datasets, while deep learning based methods have performance degradation when generalized between different datasets. To demonstrate this, we perform the generalization experiments on both 3DMatch, 3DLoMatch and KITTI datasets. For the recent learning based methods, including DGR and PointDSC, we report the crossdataset results. Specifically, we adopt their pre-trained model by KITTI to test on 3DMatch and 3DLoMatch and use 3DMatch’s model to test on KITTI. As shown in Table V, both the previous version SC2-PCR and the updated version SC2-PCR++ show significant improvements in registration recall without the generalization problem. This further demonstrates the effectiveness of our method.

泛化實驗：如上所述，基於深度學習的方法在 3DMatch、3DLoMatch 和 KITTI 資料集上也取得了具有競爭力的性能。與這些基於深度學習的方法相比，我們方法的另一個優點是它在不同資料集之間沒有偏差，而基於深度學習的方法在不同資料集之間泛化時性能會下降。為了證明這一點，我們在 3DMatch、3DLoMatch 和 KITTI 資料集上進行了泛化實驗。對於近期的基於學習的方法，包括 DGR 和 PointDSC，我們報告了跨資料集的結果。具體來說，我們採用它們在 KITTI 上預訓練的模型來測試 3DMatch 和 3DLoMatch，並使用 3DMatch 的模型來測試 KITTI。如表 V 所示，無論是先前的 SC2-PCR 版本還是更新的 SC2-PCR++ 版本，在配準召回率上都顯示出顯著的提升，且沒有泛化問題。這進一步證明了我們方法的有效性。

Robustness to Noises: An important factor to measure the model fitting method is the stability under low inlier rate. In order to further verify the performance of our method, we report the results under different inlier ratios in Fig. 8. Specifically, we first use FPFH to generate initial match pairs for the 3DMatch dataset. Then, according to the inlier ratio, all the point cloud pairs are divided into 6 groups: < 1%, 1% - 2%, 2% - 4%, 4% - 6%, 6% - 10% and > 10%. The number of point cloud pairs in each group is 141, 208, 346, 252, 323, and 353. As shown in Fig. 8(a), when the inlier rate is less than 2%, SC2-PCR++ is significantly better than other baselines. Compared with SC2-PCR, SC2-PCR++ has a more robust performance in low inlier-rate scenes. Furthermore, we generate more challenging test pairs with lower inlier rates on 3DLoMatch dataset by FPFH descriptor, as shown in Fig. 8(b). The mean inlier rate of the putative correspondences on this dataset is 1.6%. Similarly, we also divided all the pairs into 6 groups according to the inlier rate: < 0.2%, 0.2% - 0.5%, 0.5% - 1%, 1% - 2%, 2% - 5% and > 5%, with 214, 322, 370, 410, 365 and 100 samples in each group. As we can see, when the inlier rate is lower than 0.5%, all methods fail to get the correct estimation. In this case, the inlier rate is too low, making the model fitting an ill problem. When the inlier rate is 0.5% - 1%, the success rate of most methods is extremely low, while SC2-PCR++ significantly improves the success rate. The above experimental results demonstrate the robustness anti noises of our method.

對雜訊的穩健性：衡量模型擬合方法的一個重要因素是在低內點率下的穩定性。為了進一步驗證我們方法的性能，我們在圖 8 中報告了不同內點率下的結果。具體來說，我們首先使用 FPFH 為 3DMatch 資料集生成初始匹配對。然後，根據內點率，將所有點雲對分為 6 組：< 1%、1% - 2%、2% - 4%、4% - 6%、6% - 10% 和 > 10%。每組的點雲對數量分別為 141、208、346、252、323 和 353。如圖 8(a) 所示，當內點率小於 2% 時，SC2-PCR++ 明顯優於其他基線。與 SC2-PCR 相比，SC2-PCR++ 在低內點率場景中表現更穩健。此外，我們使用 FPFH 描述子在 3DLoMatch 資料集上生成了更具挑戰性的、內點率更低的測試對，如圖 8(b) 所示。該資料集上假定對應的平均內點率為 1.6%。同樣，我們也根據內點率將所有對分為 6 組：< 0.2%、0.2% - 0.5%、0.5% - 1%、1% - 2%、2% - 5% 和 > 5%，每組分別有 214、322、370、410、365 和 100 個樣本。我們可以看到，當內點率低於 0.5% 時，所有方法都無法得到正確的估計。在這種情況下，內點率太低，使得模型擬合成為一個病態問題。當內點率在 0.5% - 1% 之間時，大多數方法的成功率極低，而 SC2-PCR++ 顯著提高了成功率。以上實驗結果證明了我們方法的抗雜訊穩健性。

Effect of Voxel Size: As mentioned in Section III-A, before extracting feature descriptors, we first use the voxel grid to downsample the two point clouds. Here we present the effect

體素大小的影響：如第三節-A 所述，在提取特徵描述子之前，我們首先使用體素網格對兩個點雲進行降採樣。這裡我們展示了其效果


[Image]

[圖像]

Fig. 8. The registration recall under the different inlier ratio of the putative correspondences.

圖 8. 在不同假定對應之內點率下的配準召回率。

[Image]

[圖像]

Fig. 9. The registration recall with different voxel sizes for downsampling the point cloud.

圖 9. 不同體素大小下點雲降採樣的配準召回率。

of voxel size on registration results in Fig. 9. As we can see, as the voxel size increases, the number of points in the downsampled point clouds will decrease, and the registration results will also decrease, especially on 3DLoMatch dataset. The reason is that the two point clouds in 3DLoMatch dataset share a small overlapping area. When the voxel size increases, the overlapping points will be too insufficient, making it hard to establish correspondences between point clouds. In general, SC2-PCR++ achieves the highest registration recall in different voxel sizes.

圖 9 顯示了體素大小對配準結果的影響。我們可以看到，隨著體素大小的增加，降採樣後點雲中的點數會減少，配準結果也會下降，尤其是在 3DLoMatch 資料集上。原因在於 3DLoMatch 資料集中的兩個點雲共享一個小的重疊區域。當體素大小增加時，重疊點將變得過於稀少，使得在點雲之間建立對應變得困難。總體而言，SC2-PCR++ 在不同體素大小下均取得了最高的配準召回率。

Efficiency With Different N ′ s: As described in Section VI-B, in SC2-PCR++, we first use the IC metric to select N ′ s candidate models, and then use the FS-TCD to select the best one. When N ′ s is 1, SC2-PCR++ will become SC2-PCR. As N ′ s increases, we can retain more possible models to be finely selected, but it will also add computation time. It not only means that SC2-PCR++ is a more generalized version of SC2-PCR, but also means that we can balance the efficiency and recall by the N ′ s parameter. To better understand the SC2-PCR++, we report the running time and registration recall with different N ′ s in Table VI.

不同 N's 下的效率：如第六節-B 所述，在 SC2-PCR++ 中，我們首先使用 IC 度量來選擇 N's 個候選模型，然後使用 FS-TCD 來選擇最佳模型。當 N's 為 1 時，SC2-PCR++ 將變為 SC2-PCR。隨著 N's 的增加，我們可以保留更多可能的模型進行精細選擇，但這也會增加計算時間。這不僅意味著 SC2-PCR++ 是 SC2-PCR 的一個更廣義的版本，也意味著我們可以透過 N's 參數來平衡效率和召回率。為了更好地理解 SC2-PCR++，我們在表 VI 中報告了不同 N's 下的運行時間和配準召回率。

Robustness to Parameters: In order to decide the parameters for the method, we conduct a series of experiments in this part. The most important parameter for our method is the number of correspondences when sampling the consensus set. As described in Section VI-A, the SC2-PCR adopts a two-stage sampling strategy. It finds K1 instances for each seed in the first stage, and remains K2(K2 < K1) samples for model estimation in the

對參數的穩健性：為了決定方法的參數，我們在這部分進行了一系列實驗。對我們的方法來說，最重要的參數是在採樣共識集時的對應數量。如第六節-A 所述，SC2-PCR 採用兩階段採樣策略。它在第一階段為每個種子找到 K1 個實例，並保留 K2（K2 < K1）個樣本用於模型估計。

TABLE VI THE RUNNING TIME AND R EGISTRATION R ECALL (RR) WITH DIFFERENT N ′ s

表 VI 不同 N's 下的運行時間和配準召回率 (RR)

[Image]

[圖像]

second stage. As shown in Fig. 10, we set (K1, K2) to be (10, 5), (20, 10), (30, 20), (40, 30), (50, 40), (60, 50), (70, 60) and (80, 70) respectively. The registration recall (RR) on 3DMatch and 3DLoMatch datasets with different descriptors are represented. As we can see from the curves, in general, RR increases first and then decreases as the number of correspondences in the consensus set increases. In fact, it only takes three correspondences to estimate the correct rigid transformation, but more correspondences may prevent the samples from clustering together, which can reduce the noise on the estimated transformation. On the whole, the results with (K1, K2) being (30, 20) are the best, so this combination of parameters is selected in the final version of our method. It is worth mentioning that changing parameters

第二階段。如圖 10 所示，我們分別將 (K1, K2) 設為 (10, 5)、(20, 10)、(30, 20)、(40, 30)、(50, 40)、(60, 50)、(70, 60) 和 (80, 70)。圖中表示了在 3DMatch 和 3DLoMatch 資料集上使用不同描述子的配準召回率 (RR)。從曲線中我們可以看到，總體而言，隨著共識集中對應數量的增加，RR 先增加後減少。事實上，估計正確的剛性變換只需要三個對應，但更多的對應可能會阻止樣本聚集在一起，這可以減少估計變換上的雜訊。總體而言，(K1, K2) 為 (30, 20) 的結果是最好的，因此在我們方法的最終版本中選擇了這個參數組合。值得一提的是，改變參數


[Image]

[圖像]

Fig. 10. Experiments results for the analysis of the sampling parameters.

圖 10. 採樣參數分析的實驗結果。

TABLE VII THE R EGISTRATION R ECALL OF C OMBINING SC2 MEASURE AND FS-TCD METRIC W ITH LEARNING BASED NETWORK

表 VII 結合 SC2 度量和 FS-TCD 度量與基於學習的網路的配準召回率

[Image]

[圖像]

does not have a great impact on registration recall, which can also demonstrate the robustness of our method.

對配準召回率沒有太大影響，這也證明了我們方法的穩健性。

## F. Combined With Learning Network

F. 結合學習網路

To verify the flexibility of our proposed approach, we combine our approach with a recent deep learning approach PointDSC [9]. It adopts the spatial consistency matrix to guide the non-local module. First, since the proposed SC2 measure is more robust to the ambiguity, we replace the spatial consistency matrix in PointDSC with SC2. Instead of retraining the network, we directly plug our metrics into it. The registration recall of their vanilla version and combined version are shown in Table VII. It can be seen that adding our metrics can significantly boost the performance of the network, especially for the generalization performance of the network. Next, we also integrate the FS-TCD metric into the PointDSC network. The original version of PointDSC generates some hypotheses and uses the Inlier Count (IC) metric to select the best estimation. We replace it with the proposed FS-TCD based selection strategy. It can be seen that the performance of all settings is improved. Finally, the SC2 measure and FS-TCD metric are both added into the PointDSC, which achieves better results than only using one of them. The above results demonstrate that the proposed measure is flexible to combine with other methods.

為了驗證我們所提方法的靈活性，我們將其與近期的深度學習方法 PointDSC [9] 相結合。該方法採用空間一致性矩陣來指導非局部模組。首先，由於我們提出的 SC2 度量對模糊性更具穩健性，我們用 SC2 取代了 PointDSC 中的空間一致性矩陣。我們沒有重新訓練網路，而是直接將我們的度量插入其中。表 VII 顯示了其原始版本和組合版本的配準召回率。可以看出，加入我們的度量可以顯著提升網路的性能，特別是網路的泛化性能。接下來，我們也將 FS-TCD 度量整合到 PointDSC 網路中。PointDSC 的原始版本會生成一些假設，並使用內點計數 (IC) 度量來選擇最佳估計。我們將其替換為我們提出的基於 FS-TCD 的選擇策略。可以看出，所有設定的性能都得到了改善。最後，將 SC2 度量和 FS-TCD 度量都加入到 PointDSC 中，其結果優於僅使用其中之一。以上結果表明，我們提出的度量可以靈活地與其他方法相結合。

## G. Ablation Study

G. 消融研究

In this section, we perform ablation studies on 3DMatch dataset. We use the FPFH and FCGF descriptors to form correspondences respectively. The classic RANSAC is adopted as our baseline, as shown in Row 1 and Row 10 of Table VIII. We progressively add the proposed modules to the baseline and report the results.

在本節中，我們在 3DMatch 資料集上進行消融研究。我們分別使用 FPFH 和 FCGF 描述子來形成對應。我們採用經典的 RANSAC 作為我們的基線，如表 VIII 的第 1 行和第 10 行所示。我們逐步將所提出的模組添加到基線中並報告結果。

Second-Order Spatial Compatibility: We first add the SecondOrder Spatial Compatibility (SC2) measure as the guidance for the sampling of RANSAC. Each correspondence is extended into a consensus set by searching the k-nearest neighbors in metric space. The Spatial Compatibility (SC) adopted by previous works [9], [10], [12] is also utilized as the sampling guidance, and the results are reported as a comparison. As shown in Row 1, 3, and 10, 12 of Table VIII, the registration recall obtained by using SC2 measure as guidance is 14.79% higher than RANSAC when combined with FPFH, and 1.66% higher when combined with FCGF. Meanwhile, since SC2 measure can narrow the sampling space, the mean registration time of SC2 measure is much smaller than RANSAC. Besides, using SC2 measure as guidance can achieve better performance than using SC measure by comparing Row 2, 3 and 11, 12. This is because SC is disturbed by the ambiguity problem, while SC2 measure can eliminate the ambiguity.

二階空間相容性：我們首先加入二階空間相容性 (SC2) 度量作為 RANSAC 採樣的指導。每個對應透過在度量空間中搜索 k-最近鄰擴展成一個共識集。先前研究 [9], [10], [12] 採用的空間相容性 (SC) 也被用作採樣指導，並報告其結果作為比較。如表 VIII 的第 1、3 行和第 10、12 行所示，使用 SC2 度量作為指導獲得的配準召回率，在與 FPFH 結合時比 RANSAC 高 14.79%，在與 FCGF 結合時高 1.66%。同時，由於 SC2 度量可以縮小採樣空間，SC2 度量的平均配準時間遠小於 RANSAC。此外，透過比較第 2、3 行和第 11、12 行，使用 SC2 度量作為指導可以比使用 SC 度量取得更好的性能。這是因為 SC 受到模糊問題的干擾，而 SC2 度量可以消除模糊性。

Two-Stage Selection: We further adopt a two-stage selection strategy for generating the consensus set for each seed. When one seed is an inlier correspondence, it has almost removed most of the outliers in the consensus set formed in the first stage. Since SC2 becomes more stable when the inlier rate increases, we construct a local SC2 matrix to remove potential outliers. Comparing Row 3, 4, and 12, 13 in Table VIII, using two-stage selection achieves a recall improvement of 1.96% when combined with FPFH, and 0.12% improvement when combined with FCGF.

兩階段選擇：我們進一步採用兩階段選擇策略來為每個種子生成共識集。當一個種子是內點對應時，它幾乎已經移除了第一階段形成的共識集中的大部分離群點。由於當內點率增加時 SC2 變得更穩定，我們構建一個局部 SC2 矩陣來移除潛在的離群點。比較表 VIII 中的第 3、4 行和第 12、13 行，使用兩階段選擇在與 FPFH 結合時召回率提高了 1.96%，在與 FCGF 結合時提高了 0.12%。

Local Spectral Matching: When a minimum set is sampled, RANSAC adopts the instance-equal SVD to generate an estimation of translation and rotation, which is sensitive to errors. We replace the instance-equal SVD [91] with the weighted SVD [19], [20], so that less reliable correspondences are assigned lower weights for robust registration. We construct a soft SC2 matrix in each consensus set, and then use local spectral matching to compute the association between each correspondence with the main cluster. The association value is utilized as the weight for weighted SVD. Comparing Row 4, 5, and 13, 14 in Table VIII, using local spectral matching can boost the performance, especially for the mean rotation and translation error.

局部譜匹配：當採樣到最小集合時，RANSAC 採用實例均等 SVD 來生成平移和旋轉的估計，這對誤差很敏感。我們用加權 SVD [19], [20] 取代實例均等 SVD [91]，這樣可靠性較低的對應在穩健配準中會被賦予較低的權重。我們在每個共識集中構建一個軟 SC2 矩陣，然後使用局部譜匹配來計算每個對應與主聚類的關聯。關聯值被用作加權 SVD 的權重。比較表 VIII 中的第 4、5 行和第 13、14 行，使用局部譜匹配可以提升性能，特別是對於平均旋轉和平移誤差。

Seed Selection: So far, each correspondence is treated as a seed. However, it does not need to generate a consensus set for all correspondences and estimate a rigid transformation. We only need to select a few reliable points, and use the aggregation

種子點選擇：到目前為止，每個對應都被視為一個種子點。然而，並不需要為所有對應生成一個共識集並估計一個剛性變換。我們只需要選擇一些可靠的點，並使用聚合


TABLE VIII ABLATION STUDY ON 3DMATCH DATASET. SC: SPATIAL C OMPATIBILITY MEASURE. SC2: SECOND -ORDER SPATIAL C OMPATIBILITY MEASURE. TS: TWO -STAGE SELECTION FOR C ONSENSUS SET SAMPLING . LSM LOCAL SPECTRAL MATCHING . SEED: USING SEED POINTS TO R EDUCE THE NUMBER OF SAMPLING . TCD: TRUNCATED C HAMFER DISTANCE . F-TCD: FEATURE -C ONSTRAINED TRUNCATED C HAMFER DISTANCE . FS-TCD: FEATURE AND SPATIAL C ONSISTENCY C ONSTRAINED TRUNCATED C HAMFER DISTANCE

表 VIII 3DMATCH 資料集上的消融研究。SC：空間相容性度量。SC2：二階空間相容性度量。TS：共識集採樣的兩階段選擇。LSM：局部譜匹配。SEED：使用種子點減少採樣數量。TCD：截斷倒角距離。F-TCD：特徵約束截斷倒角距離。FS-TCD：特徵和空間一致性約束截斷倒角距離。

[Image]

[圖像]

among the inliers to collect the set without outliers, so as to further improve the efficiency of registration. We use the global spectral matching combined with Non-Maximum Suppression to find several correspondences as seeds instead of all of the set. Row 5, 6, and 14, 15 of Table VIII shows that Seed Selection can reduce registration time by more than half without much performance degradation.

在內點中收集無離群點的集合，以進一步提高配準效率。我們使用全域譜匹配結合非極大值抑制來尋找幾個對應作為種子，而不是整個集合。表 VIII 的第 5、6 行和第 14、15 行顯示，種子選擇可以在不顯著降低性能的情況下，將配準時間減少一半以上。

Hypothesis Selection Strategies: Finally, we replace the Inlier Count (IC) with the proposed selection metric. As mentioned in Section VI-B, in order to accelerate the selection process, we use IC to filter some spurious models, and then use other metrics for finer chosen. First, the original Chamer distance (CD) is first utilized as a comparison. Since CD is prone to low-overlap and fails in most scenes, we use the Truncated form of Chamer distance, i.e. TCD as reformulated in (31). As shown in Row 7 and 16 of Table VIII, using TCD as re-selection metric leads to worse results than directly using IC for selection. The reason is that TCD finds the nearest neighbor for each source point in the whole target points, which considers too many wrong alignments. Next, we add the constraint into TCD and adopt the feature-constrained truncated Chamfer distance (F-TCD) to re-selection. Comparing Row 6, 8 and 15, 17, we can find that F-TCD achieves 2.90% and 0.44% improvement of RR when combined with FPFH and FCGF descriptors respectively. Meanwhile, the IP, IR, and F1 criteria are also improved, which means TCD also results in better correspondences. Finally, the spatial consistency constraint is also appended into the TCD as the proposed feature and spatial consistency constrained truncated Chamfer distance (FS-TCD). As shown in Row 9 and 18 of Table VIII, the registration performance can be further boosted.

假設選擇策略：最後，我們用提出的選擇度量取代內點計數 (IC)。如第六節-B 所述，為了加速選擇過程，我們先用 IC 過濾掉一些虛假的 模型，然後再用其他度量進行更精細的選擇。首先，原始的倒角距離 (CD) 被用作比較。由於 CD 在低重疊場景中容易出錯且在大多數場景中失敗，我們使用倒角距離的截斷形式，即 (31) 中重新定義的 TCD。如表 VIII 的第 7 行和第 16 行所示，使用 TCD 作為重新選擇度量的結果比直接使用 IC 進行選擇更差。原因在於 TCD 為每個源點在整個目標點中尋找最近鄰，這考慮了太多錯誤的對齊。接下來，我們在 TCD 中加入約束，並採用特徵約束的截斷倒角距離 (F-TCD) 進行重新選擇。比較第 6、8 行和第 15、17 行，我們發現當分別與 FPFH 和 FCGF 描述子結合時，F-TCD 使 RR 提高了 2.90% 和 0.44%。同時，IP、IR 和 F1 標準也得到了改善，這意味著 TCD 也產生了更好的對應。最後，將空間一致性約束也附加到 TCD 中，作為提出的特徵和空間一致性約束的截斷倒角距離 (FS-TCD)。如表 VIII 的第 9 行和第 18 行所示，配準性能可以得到進一步提升。

# VIII. CONCLUSION

八、結論

In this article, we present a second-order spatial compatibility (SC2) measure based point cloud registration method, called SC2-PCR++. The core component of our method is to cluster inliers by the proposed SC2 measure at an early stage while eliminating ambiguity. Specifically, some reliable correspondences are selected by a global spectral decomposition with NonMaximum Suppression firstly, called seed points. Then a twostage sampling strategy is adopted to extend the seed points into some consensus sets. After that, each consensus set produces a rigid transformation by local spectral matching. Finally, the best estimation is selected by the proposed Feature and Spatial consistency constrained Truncated Chamfer Distance (FS-TCD) metric as the final result. Extensive experiments demonstrate that our method achieves state-of-the-art performance and high efficiency. Meanwhile, we also demonstrate the proposed SC2 and FS-TCD are flexible measures, which can be combined with learning networks to further boost their performance.

在本文中，我們提出了一種基於二階空間相容性 (SC2) 度量的點雲配準方法，稱為 SC2-PCR++。我們方法的核心組件是在早期階段利用所提出的 SC2 度量對內點進行聚類，同時消除模糊性。具體來說，首先透過帶有非極大值抑制的全域譜分解選出一些可靠的對應，稱為種子點。然後採用兩階段採樣策略將種子點擴展成一些共識集。之後，每個共識集透過局部譜匹配產生一個剛性變換。最後，透過所提出的特徵與空間一致性約束的截斷倒角距離 (FS-TCD) 度量選出最佳估計作為最終結果。大量實驗證明，我們的方法達到了最先進的性能和高效率。同時，我們也證明了所提出的 SC2 和 FS-TCD 是靈活的度量，可以與學習網路相結合以進一步提升其性能。

REFERENCES

參考文獻

[1] T. Bailey and H. Durrant-Whyte, “Simultaneous localization and mapping (SLAM): Part II,” IEEE Robot. Automat. Mag., vol. 13, no. 3, pp. 108–117, Sep. 2006.

[1] T. Bailey and H. Durrant-Whyte, “Simultaneous localization and mapping (SLAM): Part II,” IEEE Robot. Automat. Mag., vol. 13, no. 3, pp. 108–117, Sep. 2006.

[2] H. Durrant-Whyte and T. Bailey, “Simultaneous localization and mapping: Part I,” IEEE Robot. Automat. Mag., vol. 13, no. 2, pp. 99–110, Jun. 2006.

[2] H. Durrant-Whyte and T. Bailey, “Simultaneous localization and mapping: Part I,” IEEE Robot. Automat. Mag., vol. 13, no. 2, pp. 99–110, Jun. 2006.

[3] M. Montemerlo et al., “FastSLAM: A factored solution to the simultaneous localization and mapping problem,” in Proc. Conf. Assoc. Advance. Artif. Intell., 2002, pp. 593–598.

[3] M. Montemerlo et al., “FastSLAM: A factored solution to the simultaneous localization and mapping problem,” in Proc. Conf. Assoc. Advance. Artif. Intell., 2002, pp. 593–598.

[4] K. Sun and W. Tao, “A center-driven image set partition algorithm for efficient structure from motion,” Inf. Sci., vol. 479, pp. 101–115, 2019.

[4] K. Sun and W. Tao, “A center-driven image set partition algorithm for efficient structure from motion,” Inf. Sci., vol. 479, pp. 101–115, 2019.

[5] R. T. Azuma, “A survey of augmented reality,” Presence Teleoperators Virtual Environ., vol. 6, no. 4, pp. 355–385, 1997.

[5] R. T. Azuma, “A survey of augmented reality,” Presence Teleoperators Virtual Environ., vol. 6, no. 4, pp. 355–385, 1997.

[6] M. Billinghurst, A. Clark, and G. Lee, “A survey of augmented reality,” Foundations Trends Human–Comput. Interact., vol. 8, no. 2-3, pp. 73– 272, 2015.

[6] M. Billinghurst, A. Clark, and G. Lee, “A survey of augmented reality,” Foundations Trends Human–Comput. Interact., vol. 8, no. 2-3, pp. 73– 272, 2015.

[7] I. Kostavelis and A. Gasteratos, “Semantic mapping for mobile robotics tasks: A survey,” Robot. Auton. Syst., vol. 66, pp. 86–103, 2015.

[7] I. Kostavelis and A. Gasteratos, “Semantic mapping for mobile robotics tasks: A survey,” Robot. Auton. Syst., vol. 66, pp. 86–103, 2015.

[8] M. A. Fischler and R. C. Bolles, “Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography,” Commun. ACM, vol. 24, no. 6, pp. 381–395, 1981.

[8] M. A. Fischler and R. C. Bolles, “Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography,” Commun. ACM, vol. 24, no. 6, pp. 381–395, 1981.

[9] X. Bai et al., “PointDSC: Robust point cloud registration using deep spatial consistency,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 15 859–15 869.

[9] X. Bai et al., “PointDSC: Robust point cloud registration using deep spatial consistency,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 15 859–15 869.

[10] S. Quan and J. Yang, “Compatibility-guided sampling consensus for 3-D point cloud registration,” IEEE Trans. Geosci. Remote Sens., vol. 58, no. 10, pp. 7380–7392, Oct. 2020.

[10] S. Quan and J. Yang, “Compatibility-guided sampling consensus for 3-D point cloud registration,” IEEE Trans. Geosci. Remote Sens., vol. 58, no. 10, pp. 7380–7392, Oct. 2020.

[11] J. Lee, S. Kim, M. Cho, and J. Park, “Deep hough voting for robust global registration,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 15 994–16 003.

[11] J. Lee, S. Kim, M. Cho, and J. Park, “Deep hough voting for robust global registration,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 15 994–16 003.

[12] J. Yang, Z. Huang, S. Quan, Z. Qi, and Y. Zhang, “SAC-COT: Sample consensus by sampling compatibility triangles in graphs for 3-D point cloud registration,” IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–15, 2022.

[12] J. Yang, Z. Huang, S. Quan, Z. Qi, and Y. Zhang, “SAC-COT: Sample consensus by sampling compatibility triangles in graphs for 3-D point cloud registration,” IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–15, 2022.

[13] P. J. Besl and N. D. McKay, “Method for registration of 3-D shapes,” in Sensor Fusion IV: Control Paradigms and Data Structures, vol. 1611. Bellingham, WA, USA: SPIE, 1992, pp. 586–606.

[13] P. J. Besl and N. D. McKay, “Method for registration of 3-D shapes,” in Sensor Fusion IV: Control Paradigms and Data Structures, vol. 1611. Bellingham, WA, USA: SPIE, 1992, pp. 586–606.

[14] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry, “A papier-mâché approach to learning 3D surface generation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 216–224.

[14] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry, “A papier-mâché approach to learning 3D surface generation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 216–224.

[15] Y. Yang, C. Feng, Y. Shen, and D. Tian, “FoldingNet: Point cloud autoencoder via deep grid deformation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 206–215.

[15] Y. Yang, C. Feng, Y. Shen, and D. Tian, “FoldingNet: Point cloud autoencoder via deep grid deformation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 206–215.

[16] X. Huang, G. Mei, and J. Zhang, “Feature-metric registration: A fast semi-supervised approach for robust point cloud registration without correspondences,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 11 366–11 374.

[16] X. Huang, G. Mei, and J. Zhang, “Feature-metric registration: A fast semi-supervised approach for robust point cloud registration without correspondences,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 11 366–11 374.

[17] L. Cavalli, V. Larsson, M. R. Oswald, T. Sattler, and M. Pollefeys, “Handcrafted outlier detection revisited,” in Proc. Eur. Conf. Comput. Vis., Springer, 2020, pp. 770–787.

[17] L. Cavalli, V. Larsson, M. R. Oswald, T. Sattler, and M. Pollefeys, “Handcrafted outlier detection revisited,” in Proc. Eur. Conf. Comput. Vis., Springer, 2020, pp. 770–787.

[18] K. Sun, W. Tao, and Y. Qian, “Guide to match: Multi-layer feature matching with a hybrid gaussian mixture model,” IEEE Trans. Multimedia, vol. 22, no. 9, pp. 2246–2261, Sep. 2020.

[18] K. Sun, W. Tao, and Y. Qian, “Guide to match: Multi-layer feature matching with a hybrid gaussian mixture model,” IEEE Trans. Multimedia, vol. 22, no. 9, pp. 2246–2261, Sep. 2020.

[19] G. D. Pais, S. Ramalingam, V. M. Govindu, J. C. Nascimento, R. Chellappa, and P. Miraldo, “3DRegNet: A deep neural network for 3D point registration,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 7193–7203.

[19] G. D. Pais, S. Ramalingam, V. M. Govindu, J. C. Nascimento, R. Chellappa, and P. Miraldo, “3DRegNet: A deep neural network for 3D point registration,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 7193–7203.

[20] C. Choy, W. Dong, and V. Koltun, “Deep global registration,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 2514–2523.

[20] C. Choy, W. Dong, and V. Koltun, “Deep global registration,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 2514–2523.

[21] Z. Chen, K. Sun, F. Yang, and W. Tao, “SC2-PCR: A second order spatial compatibility for efficient and robust point cloud registration,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2022, pp. 13 221–13 231.

[21] Z. Chen, K. Sun, F. Yang, and W. Tao, “SC2-PCR: A second order spatial compatibility for efficient and robust point cloud registration,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2022, pp. 13 221–13 231.

[22] A. E. Johnson and M. Hebert, “Using spin images for efficient object recognition in cluttered 3D scenes,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 21, no. 5, pp. 433–449, May 1999.

[22] A. E. Johnson and M. Hebert, “Using spin images for efficient object recognition in cluttered 3D scenes,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 21, no. 5, pp. 433–449, May 1999.

[23] A. E. Johnson and M. Hebert, “Surface matching for object recognition in complex three-dimensional scenes,” Image Vis. Comput., vol. 16, no. 9/10, pp. 635–651, 1998.

[23] A. E. Johnson and M. Hebert, “Surface matching for object recognition in complex three-dimensional scenes,” Image Vis. Comput., vol. 16, no. 9/10, pp. 635–651, 1998.

[24] A. Frome, D. Huber, R. Kolluri, T. Bülow, and J. Malik, “Recognizing objects in range data using regional point descriptors,” in Proc. Eur. Conf. Comput. Vis., Springer, 2004, pp. 224–237.

[24] A. Frome, D. Huber, R. Kolluri, T. Bülow, and J. Malik, “Recognizing objects in range data using regional point descriptors,” in Proc. Eur. Conf. Comput. Vis., Springer, 2004, pp. 224–237.

[25] F. Tombari, S. Salti, and L. Di Stefano, “Unique shape context for 3D data description,” in Proc. ACM Workshop 3D Object Retrieval, 2010, pp. 57–62.

[25] F. Tombari, S. Salti, and L. Di Stefano, “Unique shape context for 3D data description,” in Proc. ACM Workshop 3D Object Retrieval, 2010, pp. 57–62.

[26] R. B. Rusu, N. Blodow, Z. C. Marton, and M. Beetz, “Aligning point cloud views using persistent feature histograms,” in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2008, pp. 3384–3391.

[26] R. B. Rusu, N. Blodow, Z. C. Marton, and M. Beetz, “Aligning point cloud views using persistent feature histograms,” in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2008, pp. 3384–3391.

[27] R. B. Rusu, N. Blodow, and M. Beetz, “Fast point feature histograms (FPFH) for 3D registration,” in Proc. IEEE Int. Conf. Robot. Automat., 2009, pp. 3212–3217.

[27] R. B. Rusu, N. Blodow, and M. Beetz, “Fast point feature histograms (FPFH) for 3D registration,” in Proc. IEEE Int. Conf. Robot. Automat., 2009, pp. 3212–3217.

[28] Y. Guo, M. Bennamoun, F. Sohel, M. Lu, J. Wan, and N. M. Kwok, “A comprehensive performance evaluation of 3D local feature descriptors,” Int. J. Comput. Vis., vol. 116, no. 1, pp. 66–89, 2016.

[28] Y. Guo, M. Bennamoun, F. Sohel, M. Lu, J. Wan, and N. M. Kwok, “A comprehensive performance evaluation of 3D local feature descriptors,” Int. J. Comput. Vis., vol. 116, no. 1, pp. 66–89, 2016.

[29] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, and T. Funkhouser, “3DMatch: Learning local geometric descriptors from rgb-d reconstructions,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 1802–1811.

[29] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, and T. Funkhouser, “3DMatch: Learning local geometric descriptors from rgb-d reconstructions,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 1802–1811.

[30] C. Choy, J. Park, and V. Koltun, “Fully convolutional geometric features,” in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 8958–8966.

[30] C. Choy, J. Park, and V. Koltun, “Fully convolutional geometric features,” in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 8958–8966.

[31] X. Bai, Z. Luo, L. Zhou, H. Fu, L. Quan, and C.-L. Tai, “D3Feat: Joint learning of dense detection and description of 3D local features,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 6359–6367.

[31] X. Bai, Z. Luo, L. Zhou, H. Fu, L. Quan, and C.-L. Tai, “D3Feat: Joint learning of dense detection and description of 3D local features,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 6359–6367.

[32] Y. Wang, C. Yan, Y. Feng, S. Du, Q. Dai, and Y. Gao, “STORM: Structurebased overlap matching for partial point cloud registration,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 1, pp. 1135–1149, Jan. 2023.

[32] Y. Wang, C. Yan, Y. Feng, S. Du, Q. Dai, and Y. Gao, “STORM: Structurebased overlap matching for partial point cloud registration,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 1, pp. 1135–1149, Jan. 2023.

[33] H. Deng, T. Birdal, and S. Ilic, “PPFNet: Global context aware local features for robust 3D point matching,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 195–205.

[33] H. Deng, T. Birdal, and S. Ilic, “PPFNet: Global context aware local features for robust 3D point matching,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 195–205.

[34] H. Deng, T. Birdal, and S. Ilic, “PPF-FoldNet: Unsupervised learning of rotation invariant 3D local descriptors,” in Proc. Eur. Conf. Comput. Vis., 2018, pp. 602–618.

[34] H. Deng, T. Birdal, and S. Ilic, “PPF-FoldNet: Unsupervised learning of rotation invariant 3D local descriptors,” in Proc. Eur. Conf. Comput. Vis., 2018, pp. 602–618.

[35] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “PointNet: Deep learning on point sets for 3D classification and segmentation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 652–660.

[35] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “PointNet: Deep learning on point sets for 3D classification and segmentation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 652–660.

[36] Z. Gojcic, C. Zhou, J. D. Wegner, and A. Wieser, “The perfect match: 3D point cloud matching with smoothed densities,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 5545–5554.

[36] Z. Gojcic, C. Zhou, J. D. Wegner, and A. Wieser, “The perfect match: 3D point cloud matching with smoothed densities,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 5545–5554.

[37] C. Choy, J. Gwak, and S. Savarese, “4D spatio-temporal convnets: Minkowski convolutional neural networks,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 3075–3084.

[37] C. Choy, J. Gwak, and S. Savarese, “4D spatio-temporal convnets: Minkowski convolutional neural networks,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 3075–3084.

[38] Z. J. Yew and G. H. Lee, “3DFeat-Net: Weakly supervised local 3D features for point cloud registration,” in Proc. Eur. Conf. Comput. Vis., 2018, pp. 607–623.

[38] Z. J. Yew and G. H. Lee, “3DFeat-Net: Weakly supervised local 3D features for point cloud registration,” in Proc. Eur. Conf. Comput. Vis., 2018, pp. 607–623.

[39] A. Tonioni, S. Salti, F. Tombari, R. Spezialetti, and L. D. Stefano, “Learning to detect good 3D keypoints,” Int. J. Comput. Vis., vol. 126, no. 1, pp. 1–20, 2018.

[39] A. Tonioni, S. Salti, F. Tombari, R. Spezialetti, and L. D. Stefano, “Learning to detect good 3D keypoints,” Int. J. Comput. Vis., vol. 126, no. 1, pp. 1–20, 2018.

[40] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 6000–6010.

[40] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 6000–6010.

[41] S. Huang, Z. Gojcic, M. Usvyatsov, A. Wieser, and K. Schindler, “PREDATOR: Registration of 3D point clouds with low overlap,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 4267–4276.

[41] S. Huang, Z. Gojcic, M. Usvyatsov, A. Wieser, and K. Schindler, “PREDATOR: Registration of 3D point clouds with low overlap,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 4267–4276.

[42] H. Yu, F. Li, M. Saleh, B. Busam, and S. Ilic, “CoFiNet: Reliable coarseto-fine correspondences for robust pointcloud registration,” in Proc. Adv. Neural Inf. Process. Syst., 2021, pp. 23872–23884.

[42] H. Yu, F. Li, M. Saleh, B. Busam, and S. Ilic, “CoFiNet: Reliable coarseto-fine correspondences for robust pointcloud registration,” in Proc. Adv. Neural Inf. Process. Syst., 2021, pp. 23872–23884.

[43] Y. Li and T. Harada, “Lepard: Learning partial point cloud matching in rigid and deformable scenes,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2022, pp. 5554–5564.

[43] Y. Li and T. Harada, “Lepard: Learning partial point cloud matching in rigid and deformable scenes,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2022, pp. 5554–5564.

[44] Z. Qin, H. Yu, C. Wang, Y. Guo, Y. Peng, and K. Xu, “Geometric transformer for fast and robust point cloud registration,” 2022, arXiv:2202.06688.

[44] Z. Qin, H. Yu, C. Wang, Y. Guo, Y. Peng, and K. Xu, “Geometric transformer for fast and robust point cloud registration,” 2022, arXiv:2202.06688.

[45] P. H. Torr, S. J. Nasuto, and J. M. Bishop, “NAPSAC: High noise, high dimensional robust estimation-it’s in the bag,” in Proc. Brit. Mach. Vis. Conf., 2002, pp. 44.1–44.10.

[45] P. H. Torr, S. J. Nasuto, and J. M. Bishop, “NAPSAC: High noise, high dimensional robust estimation-it’s in the bag,” in Proc. Brit. Mach. Vis. Conf., 2002, pp. 44.1–44.10.

[46] K. Ni, H. Jin, and F. Dellaert, “GroupSAC: Efficient consensus in the presence of groupings,” in Proc. IEEE 12th Int. Conf. Comput. Vis., 2009, pp. 2193–2200.

[46] K. Ni, H. Jin, and F. Dellaert, “GroupSAC: Efficient consensus in the presence of groupings,” in Proc. IEEE 12th Int. Conf. Comput. Vis., 2009, pp. 2193–2200.

[47] O. Chum and J. Matas, “Matching with prosac-progressive sample consensus,” in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., 2005, pp. 220–226.

[47] O. Chum and J. Matas, “Matching with prosac-progressive sample consensus,” in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., 2005, pp. 220–226.

[48] P. H. Torr and A. Zisserman, “MLESAC: A new robust estimator with application to estimating image geometry,” Comput. Vis. Image Understanding, vol. 78, no. 1, pp. 138–156, 2000.

[48] P. H. Torr and A. Zisserman, “MLESAC: A new robust estimator with application to estimating image geometry,” Comput. Vis. Image Understanding, vol. 78, no. 1, pp. 138–156, 2000.

[49] V. Fragoso, P. Sen, S. Rodriguez, and M. Turk, “EVSAC: Accelerating hypotheses generation by modeling matching scores with extreme value theory,” in Proc. IEEE Int. Conf. Comput. Vis., 2013, pp. 2472–2479.

[49] V. Fragoso, P. Sen, S. Rodriguez, and M. Turk, “EVSAC: Accelerating hypotheses generation by modeling matching scores with extreme value theory,” in Proc. IEEE Int. Conf. Comput. Vis., 2013, pp. 2472–2479.

[50] O. Chum, J. Matas, and J. Kittler, “Locally optimized RANSAC,” in Proc. Joint Pattern Recognit. Symp., Springer, 2003, pp. 236–243.

[50] O. Chum, J. Matas, and J. Kittler, “Locally optimized RANSAC,” in Proc. Joint Pattern Recognit. Symp., Springer, 2003, pp. 236–243.

[51] K. Lebeda, J. Matas, and O. Chum, “Fixing the locally optimized RANSAC–full experimental evaluation,” in Proc. Brit. Mach. Vis. Conf., 2012, pp. 1–11.

[51] K. Lebeda, J. Matas, and O. Chum, “Fixing the locally optimized RANSAC–full experimental evaluation,” in Proc. Brit. Mach. Vis. Conf., 2012, pp. 1–11.

[52] D. Barath and J. Matas, “Graph-cut RANSAC,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 6733–6741.

[52] D. Barath and J. Matas, “Graph-cut RANSAC,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 6733–6741.

[53] D. Barath and J. Matas, “Graph-cut RANSAC: Local optimization on spatially coherent structures,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 9, pp. 4961–4974, Sep. 2022.

[53] D. Barath and J. Matas, “Graph-cut RANSAC: Local optimization on spatially coherent structures,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 9, pp. 4961–4974, Sep. 2022.

[54] D. Barath, J. Matas, and J. Noskova, “MAGSAC: Marginalizing sample consensus,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 10 197–10 205.

[54] D. Barath, J. Matas, and J. Noskova, “MAGSAC: Marginalizing sample consensus,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 10 197–10 205.

[55] J. Ma, X. Jiang, A. Fan, J. Jiang, and J. Yan, “Image matching from handcrafted to deep features: A survey,” Int. J. Comput. Vis., vol. 129, no. 1, pp. 23–79, 2021.

[55] J. Ma, X. Jiang, A. Fan, J. Jiang, and J. Yan, “Image matching from handcrafted to deep features: A survey,” Int. J. Comput. Vis., vol. 129, no. 1, pp. 23–79, 2021.

[56] K. Moo Yi, E. Trulls, Y. Ono, V. Lepetit, M. Salzmann, and P. Fua, “Learning to find good correspondences,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 2666–2674.

[56] K. Moo Yi, E. Trulls, Y. Ono, V. Lepetit, M. Salzmann, and P. Fua, “Learning to find good correspondences,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 2666–2674.

[57] E. Brachmann and C. Rother, “Neural-guided RANSAC: Learning where to sample model hypotheses,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2019, pp. 4322–4331.

[57] E. Brachmann and C. Rother, “Neural-guided RANSAC: Learning where to sample model hypotheses,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2019, pp. 4322–4331.

[58] C. Zhao, Z. Cao, C. Li, X. Li, and J. Yang, “Nm-net: Mining reliable neighbors for robust feature correspondences,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 215–224.

[58] C. Zhao, Z. Cao, C. Li, X. Li, and J. Yang, “Nm-net: Mining reliable neighbors for robust feature correspondences,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 215–224.

[59] Y. Liu, L. Liu, C. Lin, Z. Dong, and W. Wang, “Learnable motion coherence for correspondence pruning,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 3237–3246.

[59] Y. Liu, L. Liu, C. Lin, Z. Dong, and W. Wang, “Learnable motion coherence for correspondence pruning,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 3237–3246.

[60] J. Zhang et al., “Learning two-view correspondences and geometry using order-aware network,” 2019, arXiv: 1908.04964.

[60] J. Zhang et al., “Learning two-view correspondences and geometry using order-aware network,” 2019, arXiv: 1908.04964.

[61] H. Chen et al., “Learning to match features with seeded graph matching network,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 6301–6310.

[61] H. Chen et al., “Learning to match features with seeded graph matching network,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 6301–6310.

[62] C. Zhao, Y. Ge, F. Zhu, R. Zhao, H. Li, and M. Salzmann, “Progressive correspondence pruning by consensus learning,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 6464–6473.

[62] C. Zhao, Y. Ge, F. Zhu, R. Zhao, H. Li, and M. Salzmann, “Progressive correspondence pruning by consensus learning,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 6464–6473.

[63] Z. Chen, F. Yang, and W. Tao, “Cascade network with guided loss and hybrid attention for finding good correspondences,” in Proc. AAAI Conf. Artif. Intell., 2021, pp. 1123–1131.

[63] Z. Chen, F. Yang, and W. Tao, “Cascade network with guided loss and hybrid attention for finding good correspondences,” in Proc. AAAI Conf. Artif. Intell., 2021, pp. 1123–1131.

[64] Z. Chen, F. Yang, and W. Tao, “DetarNet: Decoupling translation and rotation by siamese network for point cloud registration,” in Proc. AAAI Conf. Artif. Intell., 2022, pp. 401–409.

[64] Z. Chen, F. Yang, and W. Tao, “DetarNet: Decoupling translation and rotation by siamese network for point cloud registration,” in Proc. AAAI Conf. Artif. Intell., 2022, pp. 401–409.

[65] G. Mei, X. Huang, L. Yu, J. Zhang, and M. Bennamoun, “COTReg: Coupled optimal transport based point cloud registration,” 2021, arXiv:2112.14381.

[65] G. Mei, X. Huang, L. Yu, J. Zhang, and M. Bennamoun, “COTReg: Coupled optimal transport based point cloud registration,” 2021, arXiv:2112.14381.

[66] M. Leordeanu and M. Hebert, “A spectral technique for correspondence problems using pairwise constraints,” in Proc. IEEE 10th Int. Conf. Comput. Vis., vol. 2, 2005, pp. 1482–1489.

[66] M. Leordeanu and M. Hebert, “A spectral technique for correspondence problems using pairwise constraints,” in Proc. IEEE 10th Int. Conf. Comput. Vis., vol. 2, 2005, pp. 1482–1489.

[67] Q.-Y. Zhou, J. Park, and V. Koltun, “Fast global registration,” in Proc. Eur. Conf. Comput. Vis., Springer, 2016, pp. 766–782.

[67] Q.-Y. Zhou, J. Park, and V. Koltun, “Fast global registration,” in Proc. Eur. Conf. Comput. Vis., Springer, 2016, pp. 766–782.

[68] A. Glent Buch, Y. Yang, N. Kruger, and H. Gordon Petersen, “In search of inliers: 3D correspondence by local and global voting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2014, pp. 2067–2074.

[68] A. Glent Buch, Y. Yang, N. Kruger, and H. Gordon Petersen, “In search of inliers: 3D correspondence by local and global voting,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2014, pp. 2067–2074.

[69] H. Sahloul, S. Shirafuji, and J. Ota, “An accurate and efficient voting scheme for a maximally all-inlier 3D correspondence set,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 7, pp. 2287–2298, Jul. 2021.

[69] H. Sahloul, S. Shirafuji, and J. Ota, “An accurate and efficient voting scheme for a maximally all-inlier 3D correspondence set,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 7, pp. 2287–2298, Jul. 2021.

[70] L. Sun and L. Deng, “TriVoC: Efficient voting-based consensus maximization for robust point cloud registration with extreme outlier ratios,” IEEE Trans. Robot. Autom., vol. 7, no. 2, pp. 4654–4661, Apr. 2022.

[70] L. Sun and L. Deng, “TriVoC: Efficient voting-based consensus maximization for robust point cloud registration with extreme outlier ratios,” IEEE Trans. Robot. Autom., vol. 7, no. 2, pp. 4654–4661, Apr. 2022.

[71] W. Tang and D. Zou, “Multi-instance point cloud registration by efficient correspondence clustering,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2022, pp. 6667–6676.

[71] W. Tang and D. Zou, “Multi-instance point cloud registration by efficient correspondence clustering,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2022, pp. 6667–6676.

[72] Y. Chen and G. Medioni, “Object modelling by registration of multiple range images,” Image Vis. Comput., vol. 10, no. 3, pp. 145–155, 1992.

[72] Y. Chen and G. Medioni, “Object modelling by registration of multiple range images,” Image Vis. Comput., vol. 10, no. 3, pp. 145–155, 1992.

[73] A. Segal, D. Haehnel, and S. Thrun, “Generalized-ICP,” in Proc. Conf. Robot. Sci. Syst., 2009, Art. no. 435.

[73] A. Segal, D. Haehnel, and S. Thrun, “Generalized-ICP,” in Proc. Conf. Robot. Sci. Syst., 2009, Art. no. 435.

[74] J. Yang, H. Li, D. Campbell, and Y. Jia, “Go-ICP: A globally optimal solution to 3D ICP point-set registration,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 11, pp. 2241–2254, Nov. 2016.

[74] J. Yang, H. Li, D. Campbell, and Y. Jia, “Go-ICP: A globally optimal solution to 3D ICP point-set registration,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 11, pp. 2241–2254, Nov. 2016.

[75] D. Campbell and L. Petersson, “GOGMA: Globally-optimal Gaussian mixture alignment,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 5685–5694.

[75] D. Campbell and L. Petersson, “GOGMA: Globally-optimal Gaussian mixture alignment,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 5685–5694.

[76] D. Campbell, L. Petersson, L. Kneip, H. Li, and S. Gould, “The alignment of the spheres: Globally-optimal spherical mixture alignment for camera pose estimation,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 11 796–11 806.

[76] D. Campbell, L. Petersson, L. Kneip, H. Li, and S. Gould, “The alignment of the spheres: Globally-optimal spherical mixture alignment for camera pose estimation,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 11 796–11 806.

[77] Y. Aoki, H. Goforth, R. A. Srivatsan, and S. Lucey, “PointNetLK: Robust & efficient point cloud registration using PointNet,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 7163–7172.

[77] Y. Aoki, H. Goforth, R. A. Srivatsan, and S. Lucey, “PointNetLK: Robust & efficient point cloud registration using PointNet,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 7163–7172.

[78] Y. Wang and J. M. Solomon, “Deep closest point: Learning representations for point cloud registration,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2019, pp. 3523–3532.

[78] Y. Wang and J. M. Solomon, “Deep closest point: Learning representations for point cloud registration,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2019, pp. 3523–3532.

[79] Z. J. Yew and G. H. Lee, “RPM-Net: Robust point matching using learned features,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 11 824–11 833.

[79] Z. J. Yew and G. H. Lee, “RPM-Net: Robust point matching using learned features,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 11 824–11 833.

[80] K. Fu, S. Liu, X. Luo, and M. Wang, “Robust point cloud registration framework based on deep graph matching,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 8893–8902.

[80] K. Fu, S. Liu, X. Luo, and M. Wang, “Robust point cloud registration framework based on deep graph matching,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 8893–8902.

[81] W. Yuan, B. Eckart, K. Kim, V. Jampani, D. Fox, and J. Kautz, “DeepGMR: Learning latent gaussian mixture models for registration,” in Proc. 16th Eur. Conf. Comput. Vis., Glasgow, UK, Springer, Aug. 23–28, 2020, pp. 733–750.

[81] W. Yuan, B. Eckart, K. Kim, V. Jampani, D. Fox, and J. Kautz, “DeepGMR: Learning latent gaussian mixture models for registration,” in Proc. 16th Eur. Conf. Comput. Vis., Glasgow, UK, Springer, Aug. 23–28, 2020, pp. 733–750.

[82] Y. Wang and J. M. Solomon, “PRNet: Self-supervised learning for partialto-partial registration,” in Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 8814–8826.

[82] Y. Wang and J. M. Solomon, “PRNet: Self-supervised learning for partialto-partial registration,” in Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 8814–8826.

[83] H. Xu, S. Liu, G. Wang, G. Liu, and B. Zeng, “OMNet: Learning overlapping mask for partial-to-partial point cloud registration,” 2021, arXiv:2103.00937.

[83] H. Xu, S. Liu, G. Wang, G. Liu, and B. Zeng, “OMNet: Learning overlapping mask for partial-to-partial point cloud registration,” 2021, arXiv:2103.00937.

[84] Z. J. Yew and G. H. Lee, “REGTR: End-to-end point cloud correspondences with transformers,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2022, pp. 6677–6686.

[84] Z. J. Yew and G. H. Lee, “REGTR: End-to-end point cloud correspondences with transformers,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2022, pp. 6677–6686.

[85] J. O. Irwin, “The frequency distribution of the difference between two independent variates following the same poisson distribution,” J. Roy. Statist. Soc., vol. 100, no. 3, pp. 415–416, 1937.

[85] J. O. Irwin, “The frequency distribution of the difference between two independent variates following the same poisson distribution,” J. Roy. Statist. Soc., vol. 100, no. 3, pp. 415–416, 1937.

[86] D. Karlis and I. Ntzoufras, “Analysis of sports data by using bivariate poisson models,” J. Roy. Statist. Soc. Ser. D, vol. 52, no. 3, pp. 381–393, 2003.

[86] D. Karlis and I. Ntzoufras, “Analysis of sports data by using bivariate poisson models,” J. Roy. Statist. Soc. Ser. D, vol. 52, no. 3, pp. 381–393, 2003.

[87] D. Karlis and I. Ntzoufras, “Bayesian analysis of the differences of count data,” Statist. Med., vol. 25, no. 11, pp. 1885–1905, 2006.

[87] D. Karlis and I. Ntzoufras, “Bayesian analysis of the differences of count data,” Statist. Med., vol. 25, no. 11, pp. 1885–1905, 2006.

[88] G. E. Box et al., Statistics for Experimenters, vol. 664. Hoboken, NJ, USA: Wiley, 1978.

[88] G. E. Box et al., Statistics for Experimenters, vol. 664. Hoboken, NJ, USA: Wiley, 1978.

[89] P. Virtanen et al., “SciPy 1.0: Fundamental algorithms for scientific computing in Python” Nat. Methods, vol. 17, pp. 261–272, 2020, doi: 10.1038/s41592-019-0686-2.

[89] P. Virtanen et al., “SciPy 1.0: Fundamental algorithms for scientific computing in Python” Nat. Methods, vol. 17, pp. 261–272, 2020, doi: 10.1038/s41592-019-0686-2.

[90] R. Mises and H. Pollaczek-Geiringer, “Praktische verfahren der gleichungsauflösung,” ZAMM-J. Appl. Math. Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik, vol. 9, no. 1, pp. 58–77, 1929.

[90] R. Mises and H. Pollaczek-Geiringer, “Praktische verfahren der gleichungsauflösung,” ZAMM-J. Appl. Math. Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik, vol. 9, no. 1, pp. 58–77, 1929.

[91] K. S. Arun, T. S. Huang, and S. D. Blostein, “Least-squares fitting of two 3-D point sets,” IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-9, no. 5, pp. 698–700, Sep. 1987.

[91] K. S. Arun, T. S. Huang, and S. D. Blostein, “Least-squares fitting of two 3-D point sets,” IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-9, no. 5, pp. 698–700, Sep. 1987.

[92] H. Yang, J. Shi, and L. Carlone, “TEASER: Fast and certifiable point cloud registration,” IEEE Trans. Robot., vol. 37, no. 2, pp. 314–333, Apr. 2021.

[92] H. Yang, J. Shi, and L. Carlone, “TEASER: Fast and certifiable point cloud registration,” IEEE Trans. Robot., vol. 37, no. 2, pp. 314–333, Apr. 2021.

[93] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving? The kitti vision benchmark suite,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 3354–3361.

[93] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving? The kitti vision benchmark suite,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 3354–3361.

[94] S. Choi, Q.-Y. Zhou, and V. Koltun, “Robust reconstruction of indoor scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp. 5556–5565.

[94] S. Choi, Q.-Y. Zhou, and V. Koltun, “Robust reconstruction of indoor scenes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp. 5556–5565.

[95] A. Handa, T. Whelan, J. McDonald, and A. J. Davison, “A benchmark for RGB-D visual odometry, 3D reconstruction and slam,” in Proc. IEEE Int. Conf. Robot. Automat., 2014, pp. 1524–1531.

[95] A. Handa, T. Whelan, J. McDonald, and A. J. Davison, “A benchmark for RGB-D visual odometry, 3D reconstruction and slam,” in Proc. IEEE Int. Conf. Robot. Automat., 2014, pp. 1524–1531.

[96] Y. Ma, S. Soatto, J. Kosecka, and S. S. Sastry, An Invitation to 3-D Vision: From Images to Geometric Models, vol. 26. Berlin, Germany: Springer, 2012.

[96] Y. Ma, S. Soatto, J. Kosecka, and S. S. Sastry, An Invitation to 3-D Vision: From Images to Geometric Models, vol. 26. Berlin, Germany: Springer, 2012.

[97] R. Kümmerle, G. Grisetti, H. Strasdat, K. Konolige, and W. Burgard, “G 2 O: A general framework for graph optimization,” in Proc. IEEE Int. Conf. Robot. Automat., 2011, pp. 3607–3613.

[97] R. Kümmerle, G. Grisetti, H. Strasdat, K. Konolige, and W. Burgard, “G 2 O: A general framework for graph optimization,” in Proc. IEEE Int. Conf. Robot. Automat., 2011, pp. 3607–3613.

[98] Q.-Y. Zhou, J. Park, and V. Koltun, “Open3D: A modern library for 3D data processing,” 2018, arXiv: 1801.09847.

[98] Q.-Y. Zhou, J. Park, and V. Koltun, “Open3D: A modern library for 3D data processing,” 2018, arXiv: 1801.09847.

[99] T. Whelan, R. F. Salas-Moreno, B. Glocker, A. J. Davison, and S. Leutenegger, “ElasticFusion: Real-time dense SLAM and light source estimation,” Int. J. Robot. Res., vol. 35, no. 14, pp. 1697–1716, 2016.

[99] T. Whelan, R. F. Salas-Moreno, B. Glocker, A. J. Davison, and S. Leutenegger, “ElasticFusion: Real-time dense SLAM and light source estimation,” Int. J. Robot. Res., vol. 35, no. 14, pp. 1697–1716, 2016.

[100] O. Kähler, V. A. Prisacariu, and D. W. Murray, “Real-time large-scale dense 3D reconstruction with loop closure,” in Proc. Eur. Conf. Comput. Vis., Springer, 2016, pp. 500–516.

[100] O. Kähler, V. A. Prisacariu, and D. W. Murray, “Real-time large-scale dense 3D reconstruction with loop closure,” in Proc. Eur. Conf. Comput. Vis., Springer, 2016, pp. 500–516.

[101] T. Schops, T. Sattler, and M. Pollefeys, “BAD SLAM: Bundle adjusted direct RGB-D SLAM,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 134–144.

[101] T. Schops, T. Sattler, and M. Pollefeys, “BAD SLAM: Bundle adjusted direct RGB-D SLAM,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 134–144.
